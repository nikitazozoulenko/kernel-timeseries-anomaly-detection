{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "import sklearn.utils\n",
    "import sklearn.metrics\n",
    "import iisignature\n",
    "import torch\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Optional, Dict, Set, Callable\n",
    "from joblib import Memory, Parallel, delayed\n",
    "import tslearn\n",
    "import tslearn.metrics\n",
    "from tslearn.datasets import UCR_UEA_datasets\n",
    "import sigkernel\n",
    "import scipy\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from signature import streams_to_sigs, transform_stream\n",
    "from conformance import BaseclassConformanceScore, pairwise_kernel_gram, stream_to_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: ArticularyWordRecognition\n",
    "# Number of Classes: 25\n",
    "# Dimension of path: 9\n",
    "# Length: 144\n",
    "# Train Size, Test Size 275 300\n",
    "\n",
    "# Dataset: AtrialFibrillation\n",
    "# No dataset found\n",
    "\n",
    "# Dataset: BasicMotions\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 6\n",
    "# Length: 100\n",
    "# Train Size, Test Size 40 40\n",
    "\n",
    "# Dataset: CharacterTrajectories\n",
    "# No dataset found\n",
    "\n",
    "# Dataset: Cricket\n",
    "# Number of Classes: 12\n",
    "# Dimension of path: 6\n",
    "# Length: 1197\n",
    "# Train Size, Test Size 108 72\n",
    "\n",
    "# Dataset: DuckDuckGeese\n",
    "# No dataset found\n",
    "\n",
    "# Dataset: EigenWorms\n",
    "# Number of Classes: 5\n",
    "# Dimension of path: 6\n",
    "# Length: 17984\n",
    "# Train Size, Test Size 128 131\n",
    "\n",
    "# Dataset: Epilepsy\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 3\n",
    "# Length: 206\n",
    "# Train Size, Test Size 137 138\n",
    "\n",
    "# Dataset: EthanolConcentration\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 3\n",
    "# Length: 1751\n",
    "# Train Size, Test Size 261 263\n",
    "\n",
    "# Dataset: ERing\n",
    "# No dataset found\n",
    "\n",
    "# Dataset: FaceDetection\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 144\n",
    "# Length: 62\n",
    "# Train Size, Test Size 5890 3524\n",
    "\n",
    "# Dataset: FingerMovements\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 28\n",
    "# Length: 50\n",
    "# Train Size, Test Size 316 100\n",
    "\n",
    "# Dataset: HandMovementDirection\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 10\n",
    "# Length: 400\n",
    "# Train Size, Test Size 160 74\n",
    "\n",
    "# Dataset: Handwriting\n",
    "# Number of Classes: 26\n",
    "# Dimension of path: 3\n",
    "# Length: 152\n",
    "# Train Size, Test Size 150 850\n",
    "\n",
    "# Dataset: Heartbeat\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 61\n",
    "# Length: 405\n",
    "# Train Size, Test Size 204 205\n",
    "\n",
    "# Dataset: InsectWingbeat\n",
    "# Number of Classes: 10\n",
    "# Dimension of path: 200\n",
    "# Length: 22\n",
    "# Train Size, Test Size 25000 25000\n",
    "\n",
    "# Dataset: JapaneseVowels\n",
    "# No dataset found\n",
    "\n",
    "# Dataset: Libras\n",
    "# Number of Classes: 15\n",
    "# Dimension of path: 2\n",
    "# Length: 45\n",
    "# Train Size, Test Size 180 180\n",
    "\n",
    "# Dataset: LSST\n",
    "# Number of Classes: 14\n",
    "# Dimension of path: 6\n",
    "# Length: 36\n",
    "# Train Size, Test Size 2459 2466\n",
    "\n",
    "# Dataset: MotorImagery\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 64\n",
    "# Length: 3000\n",
    "# Train Size, Test Size 278 100\n",
    "\n",
    "# Dataset: NATOPS\n",
    "# Number of Classes: 6\n",
    "# Dimension of path: 24\n",
    "# Length: 51\n",
    "# Train Size, Test Size 180 180\n",
    "\n",
    "# Dataset: PenDigits\n",
    "# Number of Classes: 10\n",
    "# Dimension of path: 2\n",
    "# Length: 8\n",
    "# Train Size, Test Size 7494 3498\n",
    "\n",
    "# Dataset: PEMS-SF\n",
    "# Number of Classes: 7\n",
    "# Dimension of path: 963\n",
    "# Length: 144\n",
    "# Train Size, Test Size 267 173\n",
    "\n",
    "# Dataset: Phoneme\n",
    "# Number of Classes: 39\n",
    "# Dimension of path: 1\n",
    "# Length: 1024\n",
    "# Train Size, Test Size 214 1896\n",
    "\n",
    "# Dataset: RacketSports\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 6\n",
    "# Length: 30\n",
    "# Train Size, Test Size 151 152\n",
    "\n",
    "# Dataset: SelfRegulationSCP1\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 6\n",
    "# Length: 896\n",
    "# Train Size, Test Size 268 293\n",
    "\n",
    "# Dataset: SelfRegulationSCP2\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 7\n",
    "# Length: 1152\n",
    "# Train Size, Test Size 200 180\n",
    "\n",
    "# Dataset: SpokenArabicDigits\n",
    "# No dataset found\n",
    "\n",
    "# Dataset: StandWalkJump\n",
    "# Number of Classes: 3\n",
    "# Dimension of path: 4\n",
    "# Length: 2500\n",
    "# Train Size, Test Size 12 15\n",
    "\n",
    "# Dataset: UWaveGestureLibrary\n",
    "# Number of Classes: 8\n",
    "# Dimension of path: 3\n",
    "# Length: 315\n",
    "# Train Size, Test Size 120 320\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tslearn datasets (equal length)\n",
    "\n",
    "* equal length (in time) UCR_UEA multivariate time series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "######################## Static Kernels on R^d ###########################\n",
    "##########################################################################\n",
    "\n",
    "def _check_gram_dims(X:np.ndarray, \n",
    "                     Y:np.ndarray,\n",
    "                     diag:bool = False,):\n",
    "    \"\"\"Stacks the input into a Gram matrix shape (N1, N2, ..., d) or\n",
    "    into a diagonal Gram shape (N1, ..., d) if diag and N1==N2.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Shape (N1, ... , d).\n",
    "        Y (np.ndarray): Shape (N2, ... , d).\n",
    "        diag (bool): If True, use diagonal Gram shape.\n",
    "    \"\"\"\n",
    "    if len(X.shape)<2 or len(Y.shape)<2:\n",
    "        raise ValueError(\"X and Y must have at least 2 dimensions.\")\n",
    "    if X.shape[1:] != Y.shape[1:]:\n",
    "        raise ValueError(\"X and Y must have the same dimensions except for the first axis.\")\n",
    "\n",
    "    N1 = X.shape[0]\n",
    "    N2 = Y.shape[0]\n",
    "    if diag and N1!=N2:\n",
    "        raise ValueError(\"If 'diag' is True, X and Y must have the same number of samples.\")\n",
    "\n",
    "\n",
    "def linear_kernel_gram(X:np.ndarray, \n",
    "                       Y:np.ndarray,\n",
    "                       diag:bool = False,\n",
    "                       ):\n",
    "    \"\"\"Computes the Rd inner product matrix <x_i, y_j> or diagonal <x_i, y_i>.\n",
    "    The inputs dimensions can only differ in the first axis.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Shape (N1, ... , d).\n",
    "        Y (np.ndarray): Shape (N2, ... , d).\n",
    "        diag (bool): If True, computes the diagonal of the gram matrix.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (N1, N2, ...) or (N1, ...) if diag=True.\n",
    "    \"\"\"\n",
    "    _check_gram_dims(X, Y, diag)\n",
    "    if diag:\n",
    "        #out_i... = sum(X_i...k * Y_i...k)\n",
    "        return np.einsum('i...k,i...k -> i...', X, Y)\n",
    "    else:\n",
    "        #out_ij... = sum(X_i...k * Y_j...k)\n",
    "        return np.einsum('i...k,j...k -> ij...', X, Y)\n",
    "\n",
    "\n",
    "def rbf_kernel_gram(X:np.ndarray, \n",
    "                    Y:np.ndarray,\n",
    "                    gamma:float,\n",
    "                    diag:bool = False,\n",
    "                    ):\n",
    "    \"\"\"Computes the RBF gram matrix k(x_i, y_j) or diagonal k(x_i, y_i).\n",
    "    The inputs dimensions can only differ in the first axis.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Shape (N1, ... , d).\n",
    "        Y (np.ndarray): Shape (N2, ... , d).\n",
    "        gamma (float): RBF parameter\n",
    "        diag (bool): If True, computes the diagonal of the gram matrix.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (N1, N2, ...) or (N1, ...) if diag=True.\n",
    "    \"\"\"\n",
    "    if diag:\n",
    "        diff = X-Y\n",
    "        norms_squared = linear_kernel_gram(diff, diff, diag=True)\n",
    "    else:\n",
    "        xx = linear_kernel_gram(X, X, diag=True)\n",
    "        xy = linear_kernel_gram(X, Y, diag=False)\n",
    "        yy = linear_kernel_gram(Y, Y, diag=True)\n",
    "        norms_squared = xx[:, np.newaxis] -2*xy + yy[np.newaxis, :]\n",
    "\n",
    "    d= X.shape[-1]\n",
    "    return np.exp(-gamma * norms_squared/d)\n",
    "\n",
    "\n",
    "def poly_kernel_gram(X:np.ndarray, \n",
    "                     Y:np.ndarray,\n",
    "                     p:float, #eg 2 or 3\n",
    "                     diag:bool = False):\n",
    "    \"\"\"Computes the polynomial kernel (<x_i, y_j> + 1)^p.\n",
    "    The inputs dimensions can only differ in the first axis.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Shape (N1, ... , d).\n",
    "        Y (np.ndarray): Shape (N2, ... , d).\n",
    "        p (float): Polynomial degree.\n",
    "        diag (bool): If True, computes the diagonal of the gram matrix.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (N1, N2, ...) or (N1, ...) if diag=True.\n",
    "    \"\"\"\n",
    "    d = X.shape[-1]\n",
    "    xy = linear_kernel_gram(X, Y, diag)\n",
    "    return (xy + d)**p\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "################### time series Integral Kernel of static kernel ######################\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "def integral_kernel(static_diag_kernel:Callable,\n",
    "                    s1: np.ndarray,\n",
    "                    s2: np.ndarray\n",
    "                    )-> float:\n",
    "    \"\"\"Computes the integral kernel K(x, y) = \\int k(x_t, y_t) dt \n",
    "    given static kernel and two piecewise linear paths.\n",
    "\n",
    "    Args:\n",
    "        static_diag_kernel_gram (Callable): Takes in two arrays of shape (M, d) \n",
    "                        and outputs the diagonal Gram <x_m, y_m> of shape (M).\n",
    "        s1 (np.ndarray): A time series of shape (T1, d)\n",
    "        s2 (np.ndarray): A time series of shape (T2, d)\n",
    "    \"\"\"\n",
    "    #Find all breakpoints of the piecewise linear paths\n",
    "    T1, d = s1.shape\n",
    "    T2, d = s2.shape\n",
    "    times = np.concatenate([np.linspace(0, 1, T1), np.linspace(0, 1, T2)])\n",
    "    times = sorted(np.unique(times))\n",
    "\n",
    "    #Add the extra breakpoints to the paths\n",
    "    f1 = interp1d(np.linspace(0, 1, T1), s1, axis=0, assume_sorted=True)\n",
    "    f2 = interp1d(np.linspace(0, 1, T2), s2, axis=0, assume_sorted=True)\n",
    "    x = f1(times) #shape (len(times), d)\n",
    "    y = f2(times)\n",
    "\n",
    "    #calculate k(x_t, y_t) for each t\n",
    "    Kt = static_diag_kernel(x, y)\n",
    "\n",
    "    #return integral of k(x_t, y_t) dt\n",
    "    return np.trapz(Kt, times)\n",
    "\n",
    "\n",
    "def integral_kernel_gram(\n",
    "        static_kernel_gram:Callable, #either linear_kernel_gram or rbf_kernel_gram with \"diag\" argument\n",
    "        X:List[np.ndarray],\n",
    "        Y:List[np.ndarray],\n",
    "        variable_length:bool,\n",
    "    ):\n",
    "    \"\"\"Computes the Gram matrix K(X_i, Y_j) of the integral kernel \n",
    "    K(x, y) = \\int k(x_t, y_t) dt.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        static_kernel_gram (Callable): Gram kernel function taking in two ndarrays and\n",
    "                    one boolean \"diag\" argument, see e.g. 'linear_kernel_gram' or \n",
    "                    'rbf_kernel_gram'.\n",
    "        X (List[np.ndarray]): List of time series of shape (T_i, d).\n",
    "        Y (List[np.ndarray]): List of time series of shape (T_i, d).\n",
    "        variable_length (bool): If False, uses the optimized kernels for equal \n",
    "                                length time series.\n",
    "    \"\"\"\n",
    "    if not variable_length:\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "        ijKt = static_kernel_gram(X, Y, False) #diag=False\n",
    "\n",
    "        #return integral of k(x_t, y_t) dt for each pair x and y\n",
    "        N, T, d = X.shape\n",
    "        return np.trapz(ijKt, dx=1/T, axis=-1)\n",
    "    else:\n",
    "        # static kernel with diag=True\n",
    "        static_ker = lambda a,b : static_kernel_gram(a,b, True) #diag=True\n",
    "        pairwise_int_ker = lambda s1, s2 : integral_kernel(static_ker, s1, s2)\n",
    "        return pairwise_kernel_gram(X,\n",
    "                                    Y,\n",
    "                                    pairwise_int_ker,\n",
    "                                    sym=True)\n",
    "\n",
    "###########################################################################\n",
    "########################## signature kernels ##############################\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_stats(num_classes, d, T, N_train, N_test):\n",
    "    print(\"Number of Classes:\", num_classes)\n",
    "    print(\"Dimension of path:\", d)\n",
    "    print(\"Length:\", T)\n",
    "    print(\"Train:\", N_train)\n",
    "    print(\"Test:\", N_test)\n",
    "\n",
    "\n",
    "def case_static(train:np.ndarray, \n",
    "                test:np.ndarray,\n",
    "                static_kernel_gram:Callable):\n",
    "    \"\"\"Calculates the gram matrices of equal length time series for \n",
    "    a static kernel on R^d. Train and test are of shape (N1, T, d) \n",
    "    and (N2, T, d). Static kernel should take in two arrays of shape \n",
    "    (M, T*d) and return the Gram matrix.\"\"\"\n",
    "    N, T = train.shape[:2]\n",
    "    train = train.reshape(N, -1)\n",
    "    test = test.reshape(N, -1)\n",
    "    vv_gram = static_kernel_gram(train, train)/T\n",
    "    uv_gram = static_kernel_gram(test, train)/T\n",
    "    return vv_gram, uv_gram\n",
    "\n",
    "\n",
    "def case_linear(train:np.ndarray, \n",
    "                test:np.ndarray):\n",
    "    \"\"\"Calculates the gram matrices for the euclidean inner product.\n",
    "    Train and test are of shape (N1, T, d) and (N2, T, d).\"\"\"\n",
    "    return case_static(train, test, linear_kernel_gram)\n",
    "\n",
    "\n",
    "def case_rbf(train:np.ndarray, \n",
    "                   test:np.ndarray,\n",
    "                   gamma:float):\n",
    "    \"\"\"Calculates the gram matrices for the rbf kernel.\n",
    "    Train and test are of shape (N1, T, d) and (N2, T, d).\"\"\"\n",
    "    rbf_ker = lambda X, Y : rbf_kernel_gram(X, Y, gamma)\n",
    "    return case_static(train, test, rbf_ker)\n",
    "\n",
    "\n",
    "def case_poly(train:np.ndarray, \n",
    "            test:np.ndarray,\n",
    "            p:float):\n",
    "    \"\"\"Calculates the gram matrices for the rbf kernel.\n",
    "    Train and test are of shape (N1, T, d) and (N2, T, d).\"\"\"\n",
    "    poly_ker = lambda X, Y : poly_kernel_gram(X, Y, p)\n",
    "    return case_static(train, test, poly_ker)\n",
    "\n",
    "\n",
    "def case_gak(train:List[np.ndarray], \n",
    "                   test:List[np.ndarray], \n",
    "                   variable_length:bool,\n",
    "                   sigma:float = 1.0,):\n",
    "    \"\"\"Calculates the gram matrices for the gak kernel.\n",
    "    Train and test are lists of possibly variable length multidimension \n",
    "    time series of shape (T_i, d)\"\"\"\n",
    "    #pick sigma parameter according to GAK paper\n",
    "    if not variable_length:\n",
    "        sigma = tslearn.metrics.sigma_gak(np.array(train))\n",
    "\n",
    "    #compute gram matrices\n",
    "    kernel = lambda s1, s2 : tslearn.metrics.gak(s1, s2, sigma)\n",
    "    vv_gram = pairwise_kernel_gram(train, train, kernel, sym=True, disable_tqdm=False)\n",
    "    uv_gram = pairwise_kernel_gram(test, train, kernel, sym=False, disable_tqdm=False)\n",
    "    return vv_gram, uv_gram\n",
    "\n",
    "\n",
    "\n",
    "#TODO TODO create new truncsig with custom kernel\n",
    "def choose_trunc_order(d:int, trunc_sig_dim_bound:int=1000):\n",
    "    \"\"\"Chooses the order N such that the truncated signature has dimensionality\n",
    "    less than some bound. The dimensionality is d**(N+1) - d\"\"\"\n",
    "    N = -1 + np.log(d+trunc_sig_dim_bound)/np.log(d)\n",
    "    N = max(2, np.floor(N))\n",
    "    return int(N)\n",
    "def case_trunc_sig(train:List[np.ndarray], \n",
    "                   test:List[np.ndarray], \n",
    "                   variable_length:bool,\n",
    "                   trunc_sig_dim_bound:int,\n",
    "                   ):\n",
    "    \"\"\"Calculates the gram matrices for the truncated signature.\n",
    "    Train and test are lists of possibly variable length multidimension \n",
    "    time series of shape (T_i, d)\"\"\"\n",
    "    d = train[0].shape[1]\n",
    "    order = choose_trunc_order(d, trunc_sig_dim_bound)\n",
    "    if variable_length:\n",
    "        train = streams_to_sigs(train, order, disable_tqdm=False)\n",
    "        test = streams_to_sigs(test, order, disable_tqdm=False)\n",
    "    else: \n",
    "        train = iisignature.sig(np.array(train), order)\n",
    "        test = iisignature.sig(np.array(test), order)\n",
    "    return case_linear(train, test)\n",
    "\n",
    "\n",
    "def case_sig_pde(train:List[np.ndarray], \n",
    "                 test:List[np.ndarray], \n",
    "                 dyadic_order:int = 3,\n",
    "                 static_kernel = sigkernel.LinearKernel(),\n",
    "                ):\n",
    "    \"\"\"Calculates the signature kernel gram matrices of the train and test.\n",
    "    Train and test are lists of possibly variable length multidimension \n",
    "    time series of shape (T_i, d)\"\"\"\n",
    "    sig_kernel = sigkernel.SigKernel(static_kernel, dyadic_order)\n",
    "    kernel = lambda s1, s2 : sig_kernel.compute_kernel(\n",
    "                                stream_to_torch(s1), \n",
    "                                stream_to_torch(s2)).numpy()[0]\n",
    "    vv_gram = pairwise_kernel_gram(train, train, kernel, sym=True, disable_tqdm=False)\n",
    "    uv_gram = pairwise_kernel_gram(test, train, kernel, sym=False, disable_tqdm=False)\n",
    "    return vv_gram, uv_gram\n",
    "\n",
    "\n",
    "def calc_grams(train:List[np.ndarray], \n",
    "               test:List[np.ndarray],\n",
    "               kernel_name:str, \n",
    "               variable_length:bool, \n",
    "               dyadic_order:int,        #for signature pde\n",
    "               trunc_sig_dim_bound:int,  #for truncated signature\n",
    "               gamma:float = 2,       #for rbf\n",
    "               p:float = 2,             #for polynomial\n",
    "               ):   \n",
    "    \"\"\"Calculates gram matrices <train, train>, <test, train> given a kernel.\n",
    "    Train and test are lists of possibly variable length multidimension time \n",
    "    series of shape (T_i, d)\"\"\"\n",
    "\n",
    "    #Transform to array if possible\n",
    "    if not variable_length:\n",
    "        train = np.array(train)\n",
    "        test = np.array(test)\n",
    "    \n",
    "    #choose method based on kernel name\n",
    "    if kernel_name == \"linear\":\n",
    "        return case_linear(train, test)\n",
    "    \n",
    "    elif kernel_name == \"rbf\":\n",
    "        return case_rbf(train, test, gamma)\n",
    "    \n",
    "    elif kernel_name == \"poly\":\n",
    "        return case_poly(train, test, p)\n",
    "\n",
    "    elif kernel_name == \"gak\":\n",
    "        return case_gak(train, test, variable_length)\n",
    "\n",
    "    elif kernel_name == \"truncated signature\":\n",
    "        return case_trunc_sig(train, test, variable_length,\n",
    "                                    trunc_sig_dim_bound)\n",
    "    \n",
    "    elif kernel_name == \"signature pde\":\n",
    "        return case_sig_pde(train, \n",
    "                        test, \n",
    "                        variable_length,\n",
    "                        dyadic_order=dyadic_order, \n",
    "                        static_kernel=sigkernel.LinearKernel(),)\n",
    "    \n",
    "    elif kernel_name == \"signature pde RBF\":\n",
    "        return case_sig_pde(train, \n",
    "                        test, \n",
    "                        variable_length,\n",
    "                        dyadic_order=dyadic_order, \n",
    "                        static_kernel=sigkernel.RBFKernel(sigma=0.5),)\n",
    "    \n",
    "    elif kernel_name == \"integral linear\":\n",
    "        vv_gram = integral_kernel_gram(linear_kernel_gram, train, train, variable_length)\n",
    "        uv_gram = integral_kernel_gram(linear_kernel_gram, test, train, variable_length)\n",
    "        return vv_gram, uv_gram\n",
    "\n",
    "    elif kernel_name == \"integral rbf\":\n",
    "        ker = lambda X, Y, diag: rbf_kernel_gram(X, Y, gamma, diag)\n",
    "        vv_gram = integral_kernel_gram(ker, train, train, variable_length)\n",
    "        uv_gram = integral_kernel_gram(ker, test, train, variable_length)\n",
    "        return vv_gram, uv_gram\n",
    "\n",
    "    elif kernel_name == \"integral poly\":\n",
    "        ker = lambda X, Y, diag : poly_kernel_gram(X, Y, p, diag)\n",
    "        vv_gram = integral_kernel_gram(ker, train, train, variable_length)\n",
    "        uv_gram = integral_kernel_gram(ker, test, train, variable_length)\n",
    "        return vv_gram, uv_gram\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid kernel name:\", kernel_name)\n",
    "\n",
    "\n",
    "def normalize_streams(train:np.ndarray, \n",
    "                      test:np.ndarray,\n",
    "                      ):\n",
    "    \"\"\"Inputs are 3D arrays of shape (N, T, d) where N is the number of time series, \n",
    "    T is the length of each time series, and d is the dimension of each time series.\"\"\"\n",
    "    # Normalize data by training set mean and std\n",
    "    mean = np.mean(train, axis=0, keepdims=True)\n",
    "    std = np.std(train, axis=0, keepdims=True)\n",
    "    train = (train - mean) / std\n",
    "    test = (test - mean) / std\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def run_single_kernel(X_train:List[np.ndarray], \n",
    "                    y_train:np.array, \n",
    "                    X_test:List[np.ndarray], \n",
    "                    y_test:np.array, \n",
    "                    labels:np.array, \n",
    "                    kernel_name:str,\n",
    "                    variable_length:bool,\n",
    "                    normalize:bool,\n",
    "                    dyadic_order:int = 3,   #for signature pde\n",
    "                    trunc_sig_dim_bound:int = 1000, #for truncated signature\n",
    "                    SVD_threshold:float = 0.01,\n",
    "                    SVD_max_rank:Optional[int] = None,\n",
    "                    gamma:float = 2,       #for rbf\n",
    "                    ):\n",
    "    \"\"\"Computes the AUC scores (weighted one vs rest) for a single kernel,\n",
    "    using kernelized nearest neighbour variance adjusted distances.\n",
    "\n",
    "    Args:\n",
    "        X_train (List[np.ndarray]): List of time series of shape (T_i, d).\n",
    "        y_train (np.array): 1-dim array of class labels.\n",
    "        X_test (List[np.ndarray]): List of time series of shape (T_i, d).\n",
    "        y_test (np.array): 1-dim array of class labels.\n",
    "        labels (np.array): Array of unique class labels.\n",
    "        kernel_name (str): Name of the kernel to use.\n",
    "        variable_length (bool): If False, uses the optimized kernels for equal \n",
    "                                length time series.\n",
    "        normalize (bool): If True, normalizes train and test by the training set\n",
    "                          mean and std.\n",
    "        dyadic_order (int): Dyadic order for PDE solver \n",
    "                            (int > 0, higher = more accurate but slower).\n",
    "        max_batch (int): Batch size in sig kernel computations.\n",
    "        trunc_sig_dim_bound (int): Upper bound on the dimensionality of the \n",
    "                                  truncated signature.\n",
    "        SVD_threshold (float): Sets all eigenvalues below this threshold to be 0.\n",
    "        SVD_max_rank (int): Sets all SVD eigenvalues to be 0 beyond 'SVD_max_rank'.\n",
    "    \"\"\"\n",
    "    # 2 methods (conf, mahal), 2 metrics (roc_auc, pr_auc), C classes\n",
    "    C = len(labels)\n",
    "    aucs = np.zeros( (2, 2, C) ) \n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        # Get all samples of the current class\n",
    "        idxs = np.where(y_train == label)[0]\n",
    "        corpus = [X_train[k] for k in idxs]\n",
    "        test = X_test\n",
    "        if normalize and not variable_length:\n",
    "            corpus, test = normalize_streams(np.array(corpus), test)\n",
    "\n",
    "        # Calculate amomaly distancce scores for all test samples\n",
    "        vv_gram, uv_gram = calc_grams(corpus, test, kernel_name, \n",
    "                            variable_length, dyadic_order, \n",
    "                            trunc_sig_dim_bound, gamma=gamma)\n",
    "        scorer = BaseclassConformanceScore(vv_gram, SVD_threshold, print_rank=True, \n",
    "                                           SVD_max_rank=SVD_max_rank)\n",
    "        dists = np.array([scorer._anomaly_distance(sample, method=\"both\") \n",
    "                          for sample in uv_gram]).T\n",
    "        distances_conf, distances_mahal = dists\n",
    "\n",
    "        # Calculate one vs rest AUC, weighted by size of class\n",
    "        for idx_conf_mahal, distances in enumerate([distances_conf, distances_mahal]):\n",
    "            ovr_labels = y_test != label\n",
    "            average=\"weighted\" #average = \"macro\"\n",
    "            roc_auc = sklearn.metrics.roc_auc_score(ovr_labels, distances, average=average)\n",
    "            pr_auc = sklearn.metrics.average_precision_score(ovr_labels, distances, average=average)\n",
    "            aucs[idx_conf_mahal, 0, i] = roc_auc\n",
    "            aucs[idx_conf_mahal, 1, i] = pr_auc\n",
    "    \n",
    "    return aucs\n",
    "\n",
    "\n",
    "def run_tslearn_experiments(dataset_names:List[str], \n",
    "                            kernel_names:List[str],\n",
    "                            gamma:float = 2,\n",
    "                            ):\n",
    "    \"\"\"Runs a series of time series anomaly detection experiments on the specified \n",
    "    tslearn datasets using kernel conformance scores.\"\"\"\n",
    "    experiments = {}\n",
    "    for dataset_name in dataset_names:\n",
    "        # Load dataset\n",
    "        X_train, y_train, X_test, y_test = UCR_UEA_datasets().load_dataset(dataset_name)\n",
    "\n",
    "        # stats\n",
    "        labels = np.unique(y_train)\n",
    "        num_classes = len(labels)\n",
    "        N_train, T, d = X_train.shape\n",
    "        N_test, _, _  = X_test.shape\n",
    "        print_dataset_stats(num_classes, d, T, N_train, N_test)\n",
    "\n",
    "        # Run each kernel\n",
    "        kernel_results = {}\n",
    "        for kernel_name in kernel_names:\n",
    "            print(\"Kernel:\", kernel_name)\n",
    "            scores = run_single_kernel(X_train, y_train, X_test, y_test, labels, \n",
    "                            kernel_name, SVD_threshold=0.001, SVD_max_rank=100,\n",
    "                            variable_length=False, normalize=True, gamma=gamma)\n",
    "            kernel_results[kernel_name] = scores\n",
    "        \n",
    "        #log dataset experiment\n",
    "        experiments[dataset_name] = {\"results\": kernel_results, \n",
    "                                     \"num_classes\": num_classes, \n",
    "                                     \"dim\":d,\n",
    "                                     \"ts_length\":T, \n",
    "                                     \"N_train\":N_train, \n",
    "                                     \"N_test\":N_test}\n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes: 15\n",
      "Dimension of path: 2\n",
      "Length: 45\n",
      "Train: 180\n",
      "Test: 180\n",
      "Kernel: rbf\n",
      "Covariance operator numerical rank = 9\n",
      "Covariance operator numerical rank = 8\n",
      "Covariance operator numerical rank = 9\n",
      "Covariance operator numerical rank = 10\n",
      "Covariance operator numerical rank = 8\n",
      "Covariance operator numerical rank = 6\n",
      "Covariance operator numerical rank = 6\n",
      "Covariance operator numerical rank = 9\n",
      "Covariance operator numerical rank = 9\n",
      "Covariance operator numerical rank = 7\n",
      "Covariance operator numerical rank = 6\n",
      "Covariance operator numerical rank = 8\n",
      "Covariance operator numerical rank = 5\n",
      "Covariance operator numerical rank = 6\n",
      "Covariance operator numerical rank = 4\n",
      "Kernel: integral rbf\n",
      "Covariance operator numerical rank = 1\n",
      "Covariance operator numerical rank = 1\n",
      "Covariance operator numerical rank = 2\n",
      "Covariance operator numerical rank = 1\n",
      "Covariance operator numerical rank = 1\n",
      "Covariance operator numerical rank = 2\n",
      "Covariance operator numerical rank = 2\n",
      "Covariance operator numerical rank = 1\n",
      "Covariance operator numerical rank = 1\n",
      "Covariance operator numerical rank = 2\n",
      "Covariance operator numerical rank = 2\n",
      "Covariance operator numerical rank = 2\n",
      "Covariance operator numerical rank = 1\n",
      "Covariance operator numerical rank = 2\n",
      "Covariance operator numerical rank = 1\n",
      "Dataset: Libras\n",
      "Number of Classes: 15\n",
      "Dimension of path: 2\n",
      "Length: 45\n",
      "Train: 180\n",
      "Test: 180\n",
      "\n",
      "Kernel: rbf\n",
      "Conformance AUC: 0.89005\n",
      "Mahalanobis AUC: 0.80321\n",
      "Conformance PR AUC: 0.98823\n",
      "Mahalanobis PR AUC: 0.97926\n",
      "\n",
      "Kernel: integral rbf\n",
      "Conformance AUC: 0.63413\n",
      "Mahalanobis AUC: 0.43753\n",
      "Conformance PR AUC: 0.95531\n",
      "Mahalanobis PR AUC: 0.91955\n",
      "\n",
      "End Dataset\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiments = run_tslearn_experiments(\n",
    "    dataset_names = [\n",
    "        #'ArticularyWordRecognition', \n",
    "        #'BasicMotions', \n",
    "        #'Cricket',\n",
    "         ##########'ERing', #cant find dataset\n",
    "        'Libras', \n",
    "        #'NATOPS', \n",
    "        #'RacketSports',     \n",
    "        #'FingerMovements',\n",
    "        #'Heartbeat',\n",
    "        #'SelfRegulationSCP1', \n",
    "        #'UWaveGestureLibrary'\n",
    "        ],\n",
    "    kernel_names = [\n",
    "        #\"linear\", \n",
    "        \"rbf\",\n",
    "        #\"poly\",\n",
    "        # \"gak\",\n",
    "        # \"truncated signature\",\n",
    "        # \"signature pde\",\n",
    "        # \"signature pde RBF\",\n",
    "        #\"integral linear\",\n",
    "        \"integral rbf\",\n",
    "        #\"integral poly\",\n",
    "        ],\n",
    "        gamma=0.001)\n",
    "\n",
    "\n",
    "def print_experiment_results(experiments, round_digits=5):\n",
    "    for dataset_name, results in experiments.items():\n",
    "        #Dataset:\n",
    "        print(\"Dataset:\", dataset_name)\n",
    "        print_dataset_stats(results[\"num_classes\"], results[\"dim\"], \n",
    "                            results[\"ts_length\"], results[\"N_train\"], \n",
    "                            results[\"N_test\"])\n",
    "\n",
    "\n",
    "        #Results for each kernel:\n",
    "        for kernel_name, scores in results[\"results\"].items():\n",
    "            print(\"\\nKernel:\", kernel_name)\n",
    "            scores = np.mean(scores, axis=2)\n",
    "            print(\"Conformance AUC:\", round(scores[0, 0], round_digits))\n",
    "            print(\"Mahalanobis AUC:\", round(scores[1, 0], round_digits))\n",
    "            print(\"Conformance PR AUC:\", round(scores[0, 1], round_digits))\n",
    "            print(\"Mahalanobis PR AUC:\", round(scores[1, 1], round_digits))\n",
    "\n",
    "        print(\"\\nEnd Dataset\\n\\n\\n\")\n",
    "        \n",
    "print_experiment_results(experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PenDigits dataset (Variable Length) \n",
    "\n",
    "* Can't use ts-learn since it interpolated and homogenized the length of all time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/pendigits-orig.tra'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m         dataframes\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(dataframes)\n\u001b[0;32m---> 47\u001b[0m data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mread_pendigits_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mData/pendigits-orig.tra\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m: read_pendigits_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData/pendigits-orig.tes\u001b[39m\u001b[38;5;124m\"\u001b[39m)}\n\u001b[1;32m     50\u001b[0m df_pendigits_raw \u001b[38;5;241m=\u001b[39m create_pendigits_dataframe(data)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_pendigits_entry\u001b[39m(sample):\n",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36mread_pendigits_dataset\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_pendigits_dataset\u001b[39m(filename):\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      9\u001b[0m         data_lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m     11\u001b[0m     data \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Code/kernel-timeseries-anomaly-detection/.conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/pendigits-orig.tra'"
     ]
    }
   ],
   "source": [
    "#################################################################################################################\n",
    "## Loading code taken from https://github.com/pafoster/conformance_distance_experiments_cochrane_et_al_2020    ##\n",
    "## DATASET_URLS = ['https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits-orig.tes.Z', ##\n",
    "##                 'https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits-orig.tra.Z'] ##\n",
    "#################################################################################################################\n",
    "\n",
    "def read_pendigits_dataset(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data_lines = f.readlines()\n",
    "\n",
    "    data = []\n",
    "    data_labels = []\n",
    "    current_digit = None\n",
    "    for line in data_lines:\n",
    "        if line == \"\\n\":\n",
    "            continue\n",
    "\n",
    "        if line[0] == \".\":\n",
    "            if \"SEGMENT DIGIT\" in line[1:]:\n",
    "                if current_digit is not None:\n",
    "                    data.append(np.array(current_digit))\n",
    "                    data_labels.append(digit_label)\n",
    "\n",
    "                current_digit = []\n",
    "                digit_label = int(line.split('\"')[1])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            x, y = map(float, line.split())\n",
    "            current_digit.append([x, y])\n",
    "            \n",
    "    data.append(np.array(current_digit))\n",
    "    data_labels.append(digit_label)\n",
    "    return data, np.array(data_labels)\n",
    "\n",
    "\n",
    "def create_pendigits_dataframe(data):\n",
    "    dataframes = []\n",
    "    for subset, data in data.items():\n",
    "        df = pd.DataFrame(data).T\n",
    "        df.columns = ['data', 'label']\n",
    "        df['subset'] = subset\n",
    "        dataframes.append(df)\n",
    "    return pd.concat(dataframes)\n",
    "\n",
    "data = {'train': read_pendigits_dataset(\"Data/pendigits-orig.tra\"),\n",
    "        'test': read_pendigits_dataset(\"Data/pendigits-orig.tes\")}\n",
    "\n",
    "df_pendigits_raw = create_pendigits_dataframe(data)\n",
    "\n",
    "def plot_pendigits_entry(sample):\n",
    "    df = pd.DataFrame(sample[\"data\"], columns=[\"x\", \"y\"])\n",
    "    fig = px.line(df, x=\"x\", y=\"y\", text=df.index, width=500, height=500)\n",
    "    fig.update_traces(textposition=\"bottom right\")\n",
    "    fig.show()\n",
    "\n",
    "plot_pendigits_entry(df_pendigits_raw.iloc[20])\n",
    "print(df_pendigits_raw.head()) \n",
    "# Each data point is a timeseries of the form ([x_t1, y_t1], ... , [x_tn, y_tn]) \n",
    "# of variable length, that is an array of shape (N_i, 2) for each i in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m############################################################################################## |\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m################################### PenDigits experiments #################################### |\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m############################################################################################## \\/\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_pendigits_experiments\u001b[39m(df:\u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame, \n\u001b[1;32m      6\u001b[0m                               kernel_names:List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m      7\u001b[0m                               stream_transforms \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_enhance\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_max_normalize\u001b[39m\u001b[38;5;124m\"\u001b[39m],):\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calculates AUCs for each kernel on the PenDigits dataset.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    df has columns [\"data\", \"label\", \"subset\"]. Each data point \u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    is a timeseries of shape (T_i, d) of variable length.\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#transform streams\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "############################################################################################## |\n",
    "################################### PenDigits experiments #################################### |\n",
    "############################################################################################## \\/\n",
    "\n",
    "def run_pendigits_experiments(df:pd.DataFrame, \n",
    "                              kernel_names:List[str],\n",
    "                              stream_transforms = [\"time_enhance\", \"min_max_normalize\"],):\n",
    "    \"\"\"Calculates AUCs for each kernel on the PenDigits dataset.\n",
    "    df has columns [\"data\", \"label\", \"subset\"]. Each data point \n",
    "    is a timeseries of shape (T_i, d) of variable length.\"\"\"\n",
    "    #transform streams\n",
    "    df[\"data\"] = df[\"data\"].apply(lambda x : transform_stream(x, stream_transforms))\n",
    "\n",
    "    #Gather dataset info\n",
    "    X_train = df[df[\"subset\"]==\"train\"][\"data\"].values\n",
    "    y_train = np.array(df[df[\"subset\"]==\"train\"][\"label\"].values)\n",
    "    X_test = df[df[\"subset\"]==\"test\"][\"data\"].values\n",
    "    y_test = np.array(df[df[\"subset\"]==\"test\"][\"label\"].values)\n",
    "    labels = sorted(df[\"label\"].unique())\n",
    "    num_classes = len(labels)\n",
    "    d = X_train[0].shape[1]\n",
    "    T = \"variable length\"\n",
    "    N_train = len(X_train)\n",
    "    N_test = len(X_test)\n",
    "    print_dataset_stats(num_classes, d, T, N_train, N_test)\n",
    "\n",
    "    # Run each kernel\n",
    "    kernel_results = {}\n",
    "    for kernel_name in kernel_names:\n",
    "        print(kernel_name)\n",
    "        scores = run_single_kernel(X_train, y_train, X_test, y_test, labels, \n",
    "                        kernel_name, variable_length=True, normalize=False,\n",
    "                        trunc_sig_dim_bound=200, SVD_max_rank=None)\n",
    "        kernel_results[kernel_name] = scores\n",
    "\n",
    "    #log results\n",
    "    pendigits_results = {\"results\": kernel_results, \n",
    "                         \"num_classes\": num_classes,\n",
    "                         \"dim\": d,\n",
    "                         \"ts_length\":T, \n",
    "                         \"N_train\":N_train, \n",
    "                         \"N_test\":N_test}\n",
    "    return pendigits_results\n",
    "\n",
    "# pendigits_results = run_pendigits_experiments(\n",
    "#     df_pendigits_raw, \n",
    "#     kernel_names=[\n",
    "#         #\"gak\",\n",
    "#         \"truncated signature\", \n",
    "#         #\"signature pde\", \n",
    "#         #\"signature pde RBF\"\n",
    "#         ],\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: Libras\n",
    "# Number of Classes: 15\n",
    "# Dimension of path: 2\n",
    "# Length: 45\n",
    "# Train: 180\n",
    "# Test: 180\n",
    "\n",
    "# Kernel: linear\n",
    "# Conformance AUC: 0.9471891534391536\n",
    "# Mahalanobis AUC: 0.9460978835978835\n",
    "# Conformance PR AUC: 0.9952856063472626\n",
    "# Mahalanobis PR AUC: 0.9953123143532209\n",
    "\n",
    "# Kernel: rbf\n",
    "# Conformance AUC: 0.6121858465608465\n",
    "# Mahalanobis AUC: 0.235896164021164\n",
    "# Conformance PR AUC: 0.9543784983385764\n",
    "# Mahalanobis PR AUC: 0.9073331474510873\n",
    "\n",
    "# Kernel: gak\n",
    "# Conformance AUC: 0.8303240740740742\n",
    "# Mahalanobis AUC: 0.056779100529100526\n",
    "# Conformance PR AUC: 0.9720553520740843\n",
    "# Mahalanobis PR AUC: 0.8271237492596946\n",
    "\n",
    "# Kernel: truncated signature\n",
    "# Conformance AUC: 0.8772486772486773\n",
    "# Mahalanobis AUC: 0.8711970899470899\n",
    "# Conformance PR AUC: 0.9884586491342986\n",
    "# Mahalanobis PR AUC: 0.9880448383038575\n",
    "\n",
    "# Kernel: signature pde\n",
    "# Conformance AUC: 0.8616071428571429\n",
    "# Mahalanobis AUC: 0.8559854497354498\n",
    "# Conformance PR AUC: 0.9860276339146576\n",
    "# Mahalanobis PR AUC: 0.9856341293618142\n",
    "\n",
    "# Kernel: signature pde RBF\n",
    "# Conformance AUC: 0.48647486772486775\n",
    "# Mahalanobis AUC: 0.5124669312169311\n",
    "# Conformance PR AUC: 0.9148945413486355\n",
    "# Mahalanobis PR AUC: 0.9149847211152167\n",
    "\n",
    "# End Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_experiment_results({\"PenDigits\": pendigits_results})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "################ Playing around with Ksig ####################\n",
    "##############################################################\n",
    "\n",
    "import numpy as np\n",
    "import ksig\n",
    "\n",
    "# Number of signature levels to use.\n",
    "n_levels = 5 \n",
    "\n",
    "# Use the RBF kernel for vector-valued data as static (base) kernel.\n",
    "static_kernel = ksig.static.kernels.RBFKernel() \n",
    "\n",
    "# Instantiate the signature kernel, which takes as input the static kernel.\n",
    "sig_kernel = ksig.kernels.SignatureKernel(n_levels, static_kernel=static_kernel)\n",
    "\n",
    "# Generate 10 sequences of length 50 with 5 channels.\n",
    "n_seq, l_seq, n_feat = 2, 50, 5 \n",
    "X = np.random.randn(n_seq, l_seq, n_feat)\n",
    "\n",
    "# Sequence kernels take as input an array of sequences of ndim == 3,\n",
    "# and work as a callable for computing the kernel matrix. \n",
    "K_XX = sig_kernel(X)  # K_XX has shape (10, 10).\n",
    "\n",
    "# The diagonal kernel entries can also be computed.\n",
    "K_X = sig_kernel(X, diag=True)  # K_X has shape (10,).\n",
    "\n",
    "# Generate another array of 8 sequences of length 20 and 5 features.\n",
    "n_seq2, l_seq2 = 8, 20\n",
    "Y = np.random.randn(n_seq2, l_seq2, n_feat)\n",
    "\n",
    "# Compute the kernel matrix between arrays X and Y.\n",
    "K_XY = sig_kernel(X, Y)  # K_XY has shape (10, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sig_X [-2.4729541   0.30670244]\n",
      "sig_Y [0.88073519 0.08890687]\n",
      "sig1 -1.150749752128363\n",
      "sig2 0.012491415965406483\n",
      "sig1-sig2 -1.1632411680937693\n",
      "sig_X [ 0.51075959 -0.45718983  0.13043768  4.90733483 -5.14084892  0.10451127]\n",
      "sig_Y [-0.24567046  1.11724594  0.03017699  2.79655671 -3.07103103  0.62411924]\n",
      "sig1 29.94423839293617\n",
      "sig2 0.3518627096527344\n",
      "sig1-sig2 29.592375683283436\n",
      "comparison [0.36816186 0.08089722]\n",
      "\n",
      "iisig [0.00147939 0.00049615]\n",
      "\n",
      "ksig [0.00401831 0.00613308]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def calc_iisig_kernel(X, Y, order):\n",
    "    sig_X, sig_Y = streams_to_sigs([X,Y], order, disable_tqdm=True)\n",
    "    print(\"sig_X\", sig_X)\n",
    "    print(\"sig_Y\", sig_Y)\n",
    "    dot = 1 + np.dot(sig_X, sig_Y)\n",
    "    return dot\n",
    "\n",
    "\n",
    "def calc_ksig_kernel(X, Y, order):\n",
    "    static_kernel = ksig.static.kernels.LinearKernel() \n",
    "    sig_kernel = ksig.kernels.SignatureKernel(order, static_kernel=static_kernel)\n",
    "    dot = sig_kernel(np.array([X,X]),Y[None,:])[0,0]\n",
    "    return dot\n",
    "\n",
    "\n",
    "d = 2\n",
    "MAX_ORDER = 2\n",
    "times_iisig = np.zeros( (MAX_ORDER) )\n",
    "times_ksig  = np.zeros( (MAX_ORDER) )\n",
    "for order in range(1, MAX_ORDER+1):\n",
    "    X, Y = np.random.randn(2, 50, d)\n",
    "    t0= time.time()\n",
    "    sig1=calc_iisig_kernel(X, Y, order)\n",
    "    t1 = time.time()\n",
    "    sig2=calc_ksig_kernel(X, Y, order)\n",
    "    t2 = time.time()\n",
    "    times_iisig[order-1] = t1-t0\n",
    "    times_ksig[order-1] = t2-t1\n",
    "    print(\"sig1\", sig1)\n",
    "    print(\"sig2\", sig2)\n",
    "    print(\"sig1-sig2\", sig1-sig2)\n",
    "\n",
    "\n",
    "print(\"comparison\", times_iisig/times_ksig)\n",
    "print(\"\\niisig\", times_iisig)\n",
    "print(\"\\nksig\", times_ksig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order 1\n",
      "sig1 1.408125434785367\n",
      "sig2 1.4081254347853687\n",
      "order 2\n",
      "sig1 0.21177077951716372\n",
      "sig2 -10.735498526929668\n",
      "order 3\n",
      "sig1 -0.07772962768761205\n",
      "sig2 -17.705741617783374\n",
      "order 4\n",
      "sig1 -0.10346728913889391\n",
      "sig2 -19.81525818644514\n",
      "order 5\n",
      "sig1 0.2574217454957398\n",
      "sig2 -19.81525818644513\n",
      "order 6\n",
      "sig1 0.3589555583778956\n",
      "sig2 -19.815258186445142\n",
      "order 7\n",
      "sig1 0.33802976809676\n",
      "sig2 -19.815258186445135\n",
      "order 8\n",
      "sig1 0.3387920971912588\n",
      "sig2 -19.815258186445142\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import shift\n",
    "\n",
    "\n",
    "\n",
    "def calc_iisig_kernel(X, Y, order):\n",
    "    sig_X, sig_Y = streams_to_sigs([X,Y], order, disable_tqdm=True)\n",
    "    # print(\"sig_X\", sig_X)\n",
    "    # print(\"sig_Y\", sig_Y)\n",
    "    dot = 1 + np.dot(sig_X, sig_Y)\n",
    "    return dot\n",
    "\n",
    "\n",
    "# def old_calc_ksig_kernel(s1:np.ndarray, \n",
    "#                      s2:np.ndarray, \n",
    "#                      order:int):\n",
    "#     \"\"\"s1 and s2 are time series of shape (T_i, d)\"\"\"\n",
    "#     K = linear_kernel_gram(s1, s2) #TODO add as argument\n",
    "#     nabla = K[1:, 1:] + K[:-1, :-1] - K[1:, :-1] - K[:-1, 1:]\n",
    "#     A = np.zeros((order, *nabla.shape))\n",
    "#     A[0] = nabla\n",
    "#     print(\"nabla\", nabla)\n",
    "#     for m in range(1, order):\n",
    "#         #Reverse cumsums\n",
    "\n",
    "#         Q = A[m-1]\n",
    "#         # Q = np.cumsum(Q[::-1], axis=0)[::-1]\n",
    "#         # Q = np.cumsum(Q.T[::-1], axis=0)[::-1].T\n",
    "#         Q = np.cumsum(Q, axis=0)\n",
    "#         Q = np.cumsum(Q, axis=1)\n",
    "#         print(\"Q\", Q)\n",
    "\n",
    "#         shifted = (1+shift(Q, (0,0), cval=0))\n",
    "#         A[m] = nabla * shifted\n",
    "#         print(\"shifted\", shifted)\n",
    "#         # print(\"A[m]\", A[m])\n",
    "#         # print(\"nabla*shift\", nabla * shifted)\n",
    "#         # print(\"nabla again\", nabla)\n",
    "#         # print(\"matmul\", nabla @ shifted)\n",
    "\n",
    "#     return 1+np.sum(np.sum(A[order-1], axis=-1), axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "def calc_ksig_kernel(s1:np.ndarray, \n",
    "                     s2:np.ndarray, \n",
    "                     order:int):\n",
    "    \"\"\"s1 and s2 are time series of shape (T_i, d)\"\"\"\n",
    "    K = linear_kernel_gram(s1, s2) #TODO add as argument\n",
    "    nabla = K[1:, 1:] + K[:-1, :-1] - K[1:, :-1] - K[:-1, 1:]\n",
    "    A = np.ones((order, *nabla.shape))\n",
    "    for m in range(1, order):\n",
    "        #obtain summands\n",
    "        Q = nabla * A[m-1]\n",
    "\n",
    "        # #Reverse cumsums\n",
    "        # Q = np.cumsum(Q[::-1], axis=0)[::-1]\n",
    "        # Q = np.cumsum(Q.T[::-1], axis=0)[::-1].T\n",
    "\n",
    "        # #shift\n",
    "        # A[m] = 1 + shift(Q, (-1,-1), cval=0)\n",
    "\n",
    "        #cumsums\n",
    "        Q = np.cumsum(Q, axis=0)\n",
    "        Q = np.cumsum(Q, axis=1)\n",
    "\n",
    "        #shift\n",
    "        A[m] = 1 + shift(Q, (1,1), cval=0)\n",
    "\n",
    "    return 1 + np.sum(nabla*A[-1])\n",
    "\n",
    "\n",
    "d = 2\n",
    "MAX_ORDER = 8\n",
    "times_iisig = np.zeros( (MAX_ORDER) )\n",
    "times_ksig  = np.zeros( (MAX_ORDER) )\n",
    "for order in range(1, MAX_ORDER+1):\n",
    "    print(\"order\", order)\n",
    "    np.random.seed(99)\n",
    "    X, Y = np.random.randn(2, 5, d)\n",
    "    t0= time.time()\n",
    "    sig1=calc_iisig_kernel(X, Y, order)\n",
    "    t1 = time.time()\n",
    "    sig2=calc_ksig_kernel(X, Y, order)\n",
    "    t2 = time.time()\n",
    "    times_iisig[order-1] = t1-t0\n",
    "    times_ksig[order-1] = t2-t1\n",
    "    print(\"sig1\", sig1)\n",
    "    print(\"sig2\", sig2)\n",
    "    #print(\"sig1-sig2\", sig1-sig2)\n",
    "\n",
    "#TODO implement the other, less efficient PRODUCT FORMULA\n",
    "\n",
    "# print(\"\\ncomparison\", times_iisig/times_ksig)\n",
    "# print(\"\\niisig\", times_iisig)\n",
    "# print(\"\\nksig\", times_ksig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n",
      "[[ 9 12 15]\n",
      " [ 9 11 13]\n",
      " [ 6  7  8]] test1\n",
      "[[36 27 15]\n",
      " [33 24 13]\n",
      " [21 15  8]] test2\n",
      "[[ 8 15 21]\n",
      " [13 24 33]\n",
      " [15 27 36]] flip\n",
      "[[24 13  0]\n",
      " [15  8  0]\n",
      " [ 0  0  0]] shift\n"
     ]
    }
   ],
   "source": [
    "test = np.arange(3*3).reshape(3,3)\n",
    "print(test)\n",
    "test1 = np.cumsum(test[::-1], axis=0)[::-1]\n",
    "print(test1, \"test1\")\n",
    "test2 = np.cumsum(test1.T[::-1], axis=0)[::-1].T\n",
    "print(test2, \"test2\")\n",
    "print(np.flip(test2, axis=(-1, -2)), \"flip\")\n",
    "\n",
    "print(shift(test2, (-1,-1), cval=0), \"shift\")\n",
    "\n",
    "# [[0 1 2]\n",
    "#  [3 4 5]\n",
    "#  [6 7 8]]\n",
    "# [[ 9 12 15]\n",
    "#  [ 9 11 13]\n",
    "#  [ 6  7  8]]\n",
    "# [[36 27 15]\n",
    "#  [33 24 13]\n",
    "#  [21 15  8]]\n",
    "# [[ 8 15 21]\n",
    "#  [13 24 33]\n",
    "#  [15 27 36]] flip\n",
    "# [[24 13  0]\n",
    "#  [15  8  0]\n",
    "#  [ 0  0  0]] shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n",
      "[[36 27 15]\n",
      " [33 24 13]\n",
      " [21 15  8]] test2\n",
      "[[24 13  0]\n",
      " [15  8  0]\n",
      " [ 0  0  0]] shift\n"
     ]
    }
   ],
   "source": [
    "test = np.arange(3*3).reshape(3,3)\n",
    "print(test)\n",
    "test1 = np.cumsum(test[::-1], axis=0)[::-1]\n",
    "test2 = np.cumsum(test1.T[::-1], axis=0)[::-1].T\n",
    "print(test2,\"test2\")\n",
    "\n",
    "print(shift(test2, (-1,-1), cval=0), \"shift\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ArticularyWordRecognition\n",
      "Number of Classes: 25\n",
      "Dimension of path: 9\n",
      "Length: 144\n",
      "Train Size, Test Size 275 300\n",
      "\n",
      "Dataset: BasicMotions\n",
      "Number of Classes: 4\n",
      "Dimension of path: 6\n",
      "Length: 100\n",
      "Train Size, Test Size 40 40\n",
      "\n",
      "Dataset: Cricket\n",
      "Number of Classes: 12\n",
      "Dimension of path: 6\n",
      "Length: 1197\n",
      "Train Size, Test Size 108 72\n",
      "\n",
      "Dataset: Libras\n",
      "Number of Classes: 15\n",
      "Dimension of path: 2\n",
      "Length: 45\n",
      "Train Size, Test Size 180 180\n",
      "\n",
      "Dataset: NATOPS\n",
      "Number of Classes: 6\n",
      "Dimension of path: 24\n",
      "Length: 51\n",
      "Train Size, Test Size 180 180\n",
      "\n",
      "Dataset: RacketSports\n",
      "Number of Classes: 4\n",
      "Dimension of path: 6\n",
      "Length: 30\n",
      "Train Size, Test Size 151 152\n",
      "\n",
      "Dataset: FingerMovements\n",
      "Number of Classes: 2\n",
      "Dimension of path: 28\n",
      "Length: 50\n",
      "Train Size, Test Size 316 100\n",
      "\n",
      "Dataset: Heartbeat\n",
      "Number of Classes: 2\n",
      "Dimension of path: 61\n",
      "Length: 405\n",
      "Train Size, Test Size 204 205\n",
      "\n",
      "Dataset: SelfRegulationSCP1\n",
      "Number of Classes: 2\n",
      "Dimension of path: 6\n",
      "Length: 896\n",
      "Train Size, Test Size 268 293\n",
      "\n",
      "Dataset: UWaveGestureLibrary\n",
      "Number of Classes: 8\n",
      "Dimension of path: 3\n",
      "Length: 315\n",
      "Train Size, Test Size 120 320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tslearn\n",
    "\n",
    "_datasets = [\n",
    "            'ArticularyWordRecognition', \n",
    "            'BasicMotions', \n",
    "            'Cricket',\n",
    "            #'ERing',\n",
    "            'Libras', \n",
    "            'NATOPS', \n",
    "            'RacketSports',     \n",
    "            'FingerMovements',\n",
    "            'Heartbeat',\n",
    "            'SelfRegulationSCP1', \n",
    "            'UWaveGestureLibrary'\n",
    "            ]\n",
    "\n",
    "\n",
    "#for dataset_name in ucr_datasets.list_multivariate_datasets():\n",
    "for dataset_name in _datasets:\n",
    "    print(\"Dataset:\", dataset_name)\n",
    "    dataset = tslearn.datasets.UCR_UEA_datasets().load_dataset(dataset_name)\n",
    "    if dataset[0] is not None:\n",
    "        X_train, y_train, X_test, y_test = dataset\n",
    "        num_classes = len(np.unique(y_train))\n",
    "        N_train, T, d = X_train.shape\n",
    "        N_test, _, _  = X_test.shape\n",
    "\n",
    "        # is_irregular = dataset[\"is_irregular\"]\n",
    "        \n",
    "        print(\"Number of Classes:\", num_classes)\n",
    "        print(\"Dimension of path:\", d)\n",
    "        print(\"Length:\", T)\n",
    "        print(\"Train Size, Test Size\", N_train, N_test)\n",
    "        print()\n",
    "    else:\n",
    "        print(\"No dataset found\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: ArticularyWordRecognition\n",
    "# Number of Classes: 25\n",
    "# Dimension of path: 9\n",
    "# Length: 144\n",
    "# Train Size, Test Size 275 300\n",
    "\n",
    "# Dataset: BasicMotions\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 6\n",
    "# Length: 100\n",
    "# Train Size, Test Size 40 40\n",
    "\n",
    "# Dataset: Cricket\n",
    "# Number of Classes: 12\n",
    "# Dimension of path: 6\n",
    "# Length: 1197\n",
    "# Train Size, Test Size 108 72\n",
    "\n",
    "# Dataset: Libras\n",
    "# Number of Classes: 15\n",
    "# Dimension of path: 2\n",
    "# Length: 45\n",
    "# Train Size, Test Size 180 180\n",
    "\n",
    "# Dataset: NATOPS\n",
    "# Number of Classes: 6\n",
    "# Dimension of path: 24\n",
    "# Length: 51\n",
    "# Train Size, Test Size 180 180\n",
    "\n",
    "# Dataset: RacketSports\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 6\n",
    "# Length: 30\n",
    "# Train Size, Test Size 151 152\n",
    "\n",
    "# Dataset: FingerMovements\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 28\n",
    "# Length: 50\n",
    "# Train Size, Test Size 316 100\n",
    "\n",
    "# Dataset: Heartbeat\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 61\n",
    "# Length: 405\n",
    "# Train Size, Test Size 204 205\n",
    "\n",
    "# Dataset: SelfRegulationSCP1\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 6\n",
    "# Length: 896\n",
    "# Train Size, Test Size 268 293\n",
    "\n",
    "# Dataset: UWaveGestureLibrary\n",
    "# Number of Classes: 8\n",
    "# Dimension of path: 3\n",
    "# Length: 315\n",
    "# Train Size, Test Size 120 320"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
