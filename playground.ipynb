{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "import sklearn.utils\n",
    "import sklearn.metrics\n",
    "import iisignature\n",
    "import torch\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Optional, Dict, Set, Callable, Any\n",
    "from joblib import Memory, Parallel, delayed\n",
    "import tslearn\n",
    "import tslearn.metrics\n",
    "from tslearn.datasets import UCR_UEA_datasets\n",
    "import sigkernel\n",
    "import scipy\n",
    "from scipy.interpolate import interp1d\n",
    "from numba import njit\n",
    "\n",
    "from signature import streams_to_sigs, transform_stream\n",
    "from conformance import BaseclassConformanceScore, pairwise_kernel_gram, stream_to_torch\n",
    "from kernels import linear_kernel_gram, rbf_kernel_gram, poly_kernel_gram\n",
    "from kernels import integral_kernel_gram, sig_kernel_gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tslearn datasets (equal length)\n",
    "\n",
    "* equal length (in time) UCR_UEA multivariate time series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment code\n",
    "\n",
    "def print_dataset_stats(num_classes, d, T, N_train, N_test):\n",
    "    print(\"Number of Classes:\", num_classes)\n",
    "    print(\"Dimension of path:\", d)\n",
    "    print(\"Length:\", T)\n",
    "    print(\"Train:\", N_train)\n",
    "    print(\"Test:\", N_test)\n",
    "\n",
    "\n",
    "def case_static(train:np.ndarray, \n",
    "                test:np.ndarray,\n",
    "                static_kernel_gram:Callable,):\n",
    "    \"\"\"Calculates the gram matrices of equal length time series for \n",
    "    a static kernel on R^d. Train and test are of shape (N1, T, d) \n",
    "    and (N2, T, d). Static kernel should take in two arrays of shape \n",
    "    (M, T*d) and return the Gram matrix.\"\"\"\n",
    "    N1, T, d = train.shape\n",
    "    N2, _, _ = test.shape\n",
    "    train = train.reshape(N1, -1)\n",
    "    test = test.reshape(N2, -1)\n",
    "    vv_gram = static_kernel_gram(train, train)\n",
    "    uv_gram = static_kernel_gram(test, train)\n",
    "    return vv_gram, uv_gram\n",
    "\n",
    "\n",
    "def case_linear(train:np.ndarray, \n",
    "                test:np.ndarray):\n",
    "    \"\"\"Calculates the gram matrices for the euclidean inner product.\n",
    "    Train and test are of shape (N1, T, d) and (N2, T, d).\"\"\"\n",
    "    return case_static(train, test, linear_kernel_gram)\n",
    "\n",
    "\n",
    "def case_rbf(train:np.ndarray, \n",
    "             test:np.ndarray,\n",
    "             sigma:float):\n",
    "    \"\"\"Calculates the gram matrices for the rbf kernel.\n",
    "    Train and test are of shape (N1, T, d) and (N2, T, d).\"\"\"\n",
    "    rbf_ker = lambda X, Y : rbf_kernel_gram(X, Y, sigma)\n",
    "    return case_static(train, test, rbf_ker)\n",
    "\n",
    "\n",
    "def case_poly(train:np.ndarray, \n",
    "              test:np.ndarray,\n",
    "              p:float):\n",
    "    \"\"\"Calculates the gram matrices for the rbf kernel.\n",
    "    Train and test are of shape (N1, T, d) and (N2, T, d).\"\"\"\n",
    "    poly_ker = lambda X, Y : poly_kernel_gram(X, Y, p)\n",
    "    return case_static(train, test, poly_ker)\n",
    "\n",
    "\n",
    "def case_gak(train:List[np.ndarray], \n",
    "                   test:List[np.ndarray], \n",
    "                   fixed_length:bool,\n",
    "                   sigma:float = 1.0,):\n",
    "    \"\"\"Calculates the gram matrices for the gak kernel.\n",
    "    Train and test are lists of possibly variable length multidimension \n",
    "    time series of shape (T_i, d)\"\"\"\n",
    "    #pick sigma parameter according to GAK paper\n",
    "    if fixed_length:\n",
    "        sigma = tslearn.metrics.sigma_gak(np.array(train))\n",
    "\n",
    "    #compute gram matrices\n",
    "    kernel = lambda s1, s2 : tslearn.metrics.gak(s1, s2, sigma)\n",
    "    vv_gram = pairwise_kernel_gram(train, train, kernel, sym=True, disable_tqdm=False)\n",
    "    uv_gram = pairwise_kernel_gram(test, train, kernel, sym=False, disable_tqdm=False)\n",
    "    return vv_gram, uv_gram\n",
    "\n",
    "\n",
    "# Solely to be used in sigkernel library. See e.g. sigkernel.LinearKernel.\n",
    "# Had to reimplement it since the original class is missing the scalar in \n",
    "# the Gram method\n",
    "class LinearKernel():\n",
    "    def __init__(self, scale=1.0):\n",
    "        self.scale = scale\n",
    "        \n",
    "    def batch_kernel(self, X, Y):\n",
    "        return self.scale*torch.bmm(X, Y.permute(0,2,1))\n",
    "\n",
    "    def Gram_matrix(self, X, Y):\n",
    "        return self.scale * torch.einsum('ipk,jqk->ijpq', X, Y)\n",
    "    \n",
    "class PolyKernel():\n",
    "    def __init__(self, scale=1.0, p=2):\n",
    "        self.scale = scale\n",
    "        self.p = p\n",
    "        \n",
    "    def batch_kernel(self, X, Y):\n",
    "        return self.scale * (1+torch.bmm(X, Y.permute(0,2,1)))**self.p\n",
    "\n",
    "    def Gram_matrix(self, X, Y):\n",
    "        return self.scale * (1+torch.einsum('ipk,jqk->ijpq', X, Y))**self.p\n",
    "\n",
    " \n",
    "def case_sig_pde(train:List[np.ndarray], \n",
    "                 test:List[np.ndarray], \n",
    "                 dyadic_order:int = 3,\n",
    "                 static_kernel = sigkernel.LinearKernel(),\n",
    "                ):\n",
    "    \"\"\"Calculates the signature kernel gram matrices of the train and test.\n",
    "    Train and test are lists of possibly variable length multidimension \n",
    "    time series of shape (T_i, d)\"\"\"\n",
    "    sig_kernel = sigkernel.SigKernel(static_kernel, dyadic_order)\n",
    "    kernel = lambda s1, s2 : sig_kernel.compute_kernel(\n",
    "                                stream_to_torch(s1), \n",
    "                                stream_to_torch(s2)).numpy()[0]\n",
    "    vv_gram = pairwise_kernel_gram(train, train, kernel, sym=True, disable_tqdm=False)\n",
    "    uv_gram = pairwise_kernel_gram(test, train, kernel, sym=False, disable_tqdm=False)\n",
    "    return vv_gram, uv_gram\n",
    "\n",
    "\n",
    "def calc_grams(train:List[np.ndarray], \n",
    "               test:List[np.ndarray],\n",
    "               param_dict:Dict[str, Any], # name : value\n",
    "               fixed_length:bool, \n",
    "               ):   \n",
    "    \"\"\"Calculates gram matrices <train, train>, <test, train> given a kernel.\n",
    "    Train and test are lists of possibly variable length multidimension time \n",
    "    series of shape (T_i, d)\"\"\"\n",
    "\n",
    "    #Transform to array if possible\n",
    "    if fixed_length:\n",
    "        train = np.array(train)\n",
    "        test = np.array(test)\n",
    "    \n",
    "    #choose method based on kernel name\n",
    "    kernel_name = param_dict[\"kernel_name\"]\n",
    "    if kernel_name == \"linear\":\n",
    "        return case_linear(train, test)\n",
    "    \n",
    "    elif kernel_name == \"rbf\":\n",
    "        return case_rbf(train, test, param_dict[\"sigma\"])\n",
    "    \n",
    "    elif kernel_name == \"poly\":\n",
    "        return case_poly(train, test, param_dict[\"p\"])\n",
    "\n",
    "    elif kernel_name == \"gak\":\n",
    "        return case_gak(train, test, fixed_length)\n",
    "\n",
    "    elif kernel_name == \"truncated sig\":\n",
    "        vv_gram = sig_kernel_gram(train, train, param_dict[\"order\"], linear_kernel_gram, sym=True)\n",
    "        uv_gram = sig_kernel_gram(test, train, param_dict[\"order\"], linear_kernel_gram)\n",
    "        return vv_gram, uv_gram\n",
    "    \n",
    "    elif kernel_name == \"truncated sig rbf\":\n",
    "        ker = lambda X, Y: rbf_kernel_gram(X, Y, param_dict[\"sigma\"])\n",
    "        vv_gram = sig_kernel_gram(train, train, param_dict[\"order\"], ker, sym=True)\n",
    "        uv_gram = sig_kernel_gram(test, train, param_dict[\"order\"], ker)\n",
    "        return vv_gram, uv_gram\n",
    "    \n",
    "    elif kernel_name == \"truncated sig poly\":\n",
    "        ker = lambda X, Y : poly_kernel_gram(X, Y, param_dict[\"p\"])\n",
    "        vv_gram = sig_kernel_gram(train, train, param_dict[\"order\"], ker, sym=True)\n",
    "        uv_gram = sig_kernel_gram(test, train, param_dict[\"order\"], ker)\n",
    "        return vv_gram, uv_gram\n",
    "    \n",
    "    elif kernel_name == \"signature pde\":\n",
    "        return case_sig_pde(train, \n",
    "                        test,\n",
    "                        static_kernel=LinearKernel(1/train[0].shape[-1]),)\n",
    "    \n",
    "    elif kernel_name == \"signature pde rbf\":\n",
    "        return case_sig_pde(train, \n",
    "                        test,\n",
    "                        static_kernel=sigkernel.RBFKernel(\n",
    "                            param_dict[\"sigma\"] * train[0].shape[-1]),)\n",
    "\n",
    "    elif kernel_name == \"signature pde poly\":\n",
    "        return case_sig_pde(train, \n",
    "                        test,\n",
    "                        static_kernel=PolyKernel(\n",
    "                            1/train[0].shape[-1], param_dict[\"p\"]),)\n",
    "    \n",
    "    elif kernel_name == \"integral linear\":\n",
    "        vv_gram = integral_kernel_gram(train, train, linear_kernel_gram, fixed_length, sym=True)\n",
    "        uv_gram = integral_kernel_gram(test, train, linear_kernel_gram, fixed_length)\n",
    "        return vv_gram, uv_gram\n",
    "\n",
    "    elif kernel_name == \"integral rbf\":\n",
    "        ker = lambda X, Y, diag: rbf_kernel_gram(X, Y, param_dict[\"sigma\"], diag)\n",
    "        vv_gram = integral_kernel_gram(train, train, ker, fixed_length, sym=True)\n",
    "        uv_gram = integral_kernel_gram(test, train, ker, fixed_length)\n",
    "        return vv_gram, uv_gram\n",
    "\n",
    "    elif kernel_name == \"integral poly\":\n",
    "        ker = lambda X, Y, diag : poly_kernel_gram(X, Y, param_dict[\"p\"], diag)\n",
    "        vv_gram = integral_kernel_gram(train, train, ker, fixed_length, sym=True)\n",
    "        uv_gram = integral_kernel_gram(test, train, ker, fixed_length)\n",
    "        return vv_gram, uv_gram\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid kernel name:\", kernel_name)\n",
    "\n",
    "\n",
    "def normalize_streams(train:np.ndarray, \n",
    "                      test:np.ndarray,\n",
    "                      ):\n",
    "    \"\"\"Inputs are 3D arrays of shape (N, T, d) where N is the number of time series, \n",
    "    T is the length of each time series, and d is the dimension of each time series.\"\"\"\n",
    "    # Normalize data by training set mean and std\n",
    "    mean = np.mean(train, axis=0, keepdims=True)\n",
    "    std = np.std(train, axis=0, keepdims=True)\n",
    "    train = (train - mean) / std\n",
    "    test = (test - mean) / std\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def compute_aucs(distances_conf:np.ndarray,     #size N\n",
    "                 distances_mahal:np.ndarray,    #size Nint\n",
    "                 y_test:np.array,               #size N\n",
    "                 class_to_test,):\n",
    "    # 2 methods (conf, mahal), 2 metrics (roc_auc, pr_auc)\n",
    "    aucs = np.zeros( (2, 2) ) \n",
    "\n",
    "    # Calculate one vs rest AUC, weighted by size of class\n",
    "    for idx_conf_mahal, distances in enumerate([distances_conf, distances_mahal]):\n",
    "        ovr_labels = y_test != class_to_test\n",
    "        average=\"macro\" #average = \"macro\" or \"eeighted\"\n",
    "        roc_auc = sklearn.metrics.roc_auc_score(ovr_labels, distances, average=average)\n",
    "        pr_auc = sklearn.metrics.average_precision_score(ovr_labels, distances, average=average)\n",
    "        aucs[idx_conf_mahal, 0] = roc_auc\n",
    "        aucs[idx_conf_mahal, 1] = pr_auc\n",
    "    \n",
    "    return aucs\n",
    "\n",
    "\n",
    "def run_single_kernel_single_label(\n",
    "        X_train:List[np.ndarray],\n",
    "        y_train:np.ndarray,\n",
    "        X_test:List[np.ndarray], \n",
    "        y_test:np.ndarray,\n",
    "        class_to_test,\n",
    "        param_dict:Dict[str, Any], # name : value\n",
    "        fixed_length:bool,\n",
    "        SVD_threshold:float = 10e-14,\n",
    "        SVD_max_rank:Optional[int] = None,\n",
    "        verbose:bool = False,\n",
    "        vv_gram=None,\n",
    "        uv_gram=None,\n",
    "        return_all_levels:bool = False,\n",
    "\n",
    "        ):\n",
    "    \"\"\"Computes the AUC scores (weighted one vs rest) for a single kernel,\n",
    "    using kernelized nearest neighbour variance adjusted distances.\n",
    "\n",
    "    Args:\n",
    "        X_train (List[np.ndarray]): List of time series of shape (T_i, d).\n",
    "        y_train (np.array): 1-dim array of class labels.\n",
    "        X_test (List[np.ndarray]): List of time series of shape (T_i, d).\n",
    "        y_test (np.array): 1-dim array of class labels.\n",
    "        unique_labels (np.array): Array of unique class labels.\n",
    "        kernel_name (str): Name of the kernel to use.\n",
    "        fixed_length (bool): If True, uses the optimized kernels for fixed \n",
    "                             length time series.\n",
    "        normalize (bool): If True, normalizes train and test by the training set\n",
    "                          mean and std.\n",
    "        dyadic_order (int): Dyadic order for PDE solver \n",
    "                            (int > 0, higher = more accurate but slower).\n",
    "        max_batch (int): Batch size in sig kernel computations.\n",
    "        trunc_sig_dim_bound (int): Upper bound on the dimensionality of the \n",
    "                                  truncated signature.\n",
    "        SVD_threshold (float): Sets all eigenvalues below this threshold to be 0.\n",
    "        SVD_max_rank (int): Sets all SVD eigenvalues to be 0 beyond 'SVD_max_rank'.\n",
    "    \"\"\"\n",
    "    # Get all samples of the current class\n",
    "    idxs = np.where(y_train == class_to_test)[0]\n",
    "    corpus = [X_train[k] for k in idxs]\n",
    "    test = X_test\n",
    "    if fixed_length:\n",
    "        corpus, test = normalize_streams(np.array(corpus), test)\n",
    "    \n",
    "    # Calculate amomaly distancce scores for all test samples\n",
    "    if (vv_gram is None) and (uv_gram is None):\n",
    "        vv_gram, uv_gram = calc_grams(corpus, test, param_dict, fixed_length)\n",
    "    scorer = BaseclassConformanceScore(vv_gram, SVD_threshold, verbose=verbose, \n",
    "                                    SVD_max_rank=SVD_max_rank)\n",
    "        \n",
    "    # only return accs for highest allowed threshold\n",
    "    if not return_all_levels:\n",
    "        conf, mahal = scorer._anomaly_distance(uv_gram, method=\"both\")\n",
    "        aucs = compute_aucs(conf, mahal, y_test, class_to_test)\n",
    "    else:\n",
    "        conf, mahal = scorer._anomaly_distance(uv_gram, method=\"both\",\n",
    "                                                return_all_levels=True)\n",
    "        aucs = np.array([compute_aucs(c, m, y_test, class_to_test)\n",
    "                         for c,m in zip(conf.T, mahal.T)])\n",
    "\n",
    "    return aucs\n",
    "\n",
    "\n",
    "def run_all_kernels(X_train:List[np.ndarray], \n",
    "                    y_train:np.array, \n",
    "                    X_test:List[np.ndarray], \n",
    "                    y_test:np.array, \n",
    "                    unique_labels:np.array, \n",
    "                    kernel_names:List[str],\n",
    "                    fixed_length:bool,\n",
    "                    verbose:bool = True,\n",
    "                    ):\n",
    "    kernel_results = {}\n",
    "    for kernel_name in kernel_names:\n",
    "        # 2 methods (conf, mahal), 2 metrics (roc_auc, pr_auc), C classes\n",
    "        aucs = np.zeros( (2, 2, len(unique_labels)) ) \n",
    "        for i, label in enumerate(unique_labels):\n",
    "            #run model\n",
    "            scores = run_single_kernel_single_label(X_train, y_train, \n",
    "                                    X_test, y_test,label, kernel_name, #TODO no more kernel name\n",
    "                                    fixed_length, verbose=verbose)\n",
    "            aucs[:,:, i] = scores\n",
    "        \n",
    "        #update kernel results\n",
    "        kernel_results[kernel_name] = aucs\n",
    "    return kernel_results\n",
    "\n",
    "\n",
    "def run_tslearn_experiments(dataset_names:List[str], \n",
    "                            kernel_names:List[str],\n",
    "                            verbose:bool=True,\n",
    "                            ):\n",
    "    \"\"\"Runs a series of time series anomaly detection experiments on the specified \n",
    "    tslearn datasets using kernel conformance scores.\"\"\"\n",
    "    experiments = {}\n",
    "    for dataset_name in dataset_names:\n",
    "        # Load dataset\n",
    "        X_train, y_train, X_test, y_test = UCR_UEA_datasets().load_dataset(dataset_name)\n",
    "\n",
    "        # stats\n",
    "        unique_labels = np.unique(y_train)\n",
    "        num_classes = len(unique_labels)\n",
    "        N_train, T, d = X_train.shape\n",
    "        N_test, _, _  = X_test.shape\n",
    "        print_dataset_stats(num_classes, d, T, N_train, N_test)\n",
    "\n",
    "        # Run each kernel\n",
    "        kernel_results = run_all_kernels(X_train, y_train, X_test, y_test, \n",
    "                                         unique_labels, kernel_names,\n",
    "                                         fixed_length=True,\n",
    "                                         verbose=verbose)\n",
    "        \n",
    "        #log dataset experiment\n",
    "        experiments[dataset_name] = {\"results\": kernel_results, \n",
    "                                     \"num_classes\": num_classes, \n",
    "                                     \"dim\":d,\n",
    "                                     \"ts_length\":T, \n",
    "                                     \"N_train\":N_train, \n",
    "                                     \"N_test\":N_test}\n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #run experiments\n",
    "\n",
    "# experiments = run_tslearn_experiments(\n",
    "#     dataset_names = [\n",
    "#         #'ArticularyWordRecognition', \n",
    "#         #'BasicMotions', \n",
    "#         #'Cricket',\n",
    "#          ##########'ERing', #cant find dataset\n",
    "#         'Libras', \n",
    "#         #'NATOPS', \n",
    "#         #'RacketSports',     \n",
    "#         #'FingerMovements',\n",
    "#         #'Heartbeat',\n",
    "#         #'SelfRegulationSCP1', \n",
    "#         #'UWaveGestureLibrary'\n",
    "#         ],\n",
    "#     kernel_names = [\n",
    "#         \"linear\",\n",
    "#         #\"rbf\",\n",
    "#         #\"poly\",\n",
    "#         #\"gak\",\n",
    "#         #\"truncated sig\",\n",
    "#         #\"truncated sig rbf\",\n",
    "#         #\"truncated sig poly\",\n",
    "#         #\"signature pde\",\n",
    "#         #\"signature pde rbf\",\n",
    "#         #\"signature pde poly\",\n",
    "#         #\"integral linear\",\n",
    "#         #\"integral rbf\",\n",
    "#         #\"integral poly\",\n",
    "#         ],\n",
    "#         verbose=True\n",
    "#         )\n",
    "\n",
    "\n",
    "# def print_experiment_results(experiments, round_digits=5):\n",
    "#     for dataset_name, results in experiments.items():\n",
    "#         #Dataset:\n",
    "#         print(\"\\nStart Dataset {dataset_name} results:\", dataset_name)\n",
    "#         print_dataset_stats(results[\"num_classes\"], results[\"dim\"], \n",
    "#                             results[\"ts_length\"], results[\"N_train\"], \n",
    "#                             results[\"N_test\"])\n",
    "\n",
    "#         #Results for each kernel:\n",
    "#         for kernel_name, scores in results[\"results\"].items():\n",
    "#             print(\"\\nKernel:\", kernel_name)\n",
    "#             scores = np.mean(scores, axis=2)\n",
    "#             print(\"Conformance AUC:\", round(scores[0, 0], round_digits))\n",
    "#             print(\"Mahalanobis AUC:\", round(scores[1, 0], round_digits))\n",
    "#             print(\"Conformance PR AUC:\", round(scores[0, 1], round_digits))\n",
    "#             print(\"Mahalanobis PR AUC:\", round(scores[1, 1], round_digits))\n",
    "\n",
    "#         print(\"\\nEnd Dataset {dataset_name} results\\n\\n\\n\")\n",
    "        \n",
    "# print_experiment_results(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comments\n",
    "\n",
    "# Kernel: truncated sig\n",
    "# Conformance AUC: 0.90757\n",
    "# Mahalanobis AUC: 0.90728\n",
    "# Conformance PR AUC: 0.99236\n",
    "# Mahalanobis PR AUC: 0.99235\n",
    "\n",
    "# Kernel: linear\n",
    "# Conformance AUC: 0.94646\n",
    "# Mahalanobis AUC: 0.93737\n",
    "# Conformance PR AUC: 0.99506\n",
    "# Mahalanobis PR AUC: 0.99445\n",
    "\n",
    "# Kernel: rbf\n",
    "# Conformance AUC: 0.7671\n",
    "# Mahalanobis AUC: 0.08571\n",
    "# Conformance PR AUC: 0.95905\n",
    "# Mahalanobis PR AUC: 0.84199\n",
    "\n",
    "# Kernel: integral linear\n",
    "# Conformance AUC: 0.94686\n",
    "# Mahalanobis AUC: 0.93849\n",
    "# Conformance PR AUC: 0.99512\n",
    "# Mahalanobis PR AUC: 0.99451\n",
    "\n",
    "# End Dataset {dataset_name} results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all tslearn datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _datasets = [\n",
    "#             'ArticularyWordRecognition', \n",
    "#             'BasicMotions', \n",
    "#             'Cricket',\n",
    "#             #'ERing',\n",
    "#             'Libras', \n",
    "#             'NATOPS', \n",
    "#             'RacketSports',     \n",
    "#             'FingerMovements',\n",
    "#             'Heartbeat',\n",
    "#             'SelfRegulationSCP1', \n",
    "#             'UWaveGestureLibrary'\n",
    "#             ]\n",
    "\n",
    "# import tslearn\n",
    "# UCR_UEA_datasets = tslearn.datasets.UCR_UEA_datasets()\n",
    "\n",
    "# for dataset_name in UCR_UEA_datasets.list_multivariate_datasets():\n",
    "# #for dataset_name in _datasets:\n",
    "#     print(\"Dataset:\", dataset_name)\n",
    "#     dataset = UCR_UEA_datasets.load_dataset(dataset_name)\n",
    "#     if dataset[0] is not None:\n",
    "#         X_train, y_train, X_test, y_test = dataset\n",
    "#         num_classes = len(np.unique(y_train))\n",
    "#         N_train, T, d = X_train.shape\n",
    "#         N_test, _, _  = X_test.shape\n",
    "        \n",
    "#         print(\"Number of Classes:\", num_classes)\n",
    "#         print(\"Dimension of path:\", d)\n",
    "#         print(\"Length:\", T)\n",
    "#         print(\"Train Size, Test Size\", N_train, N_test)\n",
    "#         print()\n",
    "#     else:\n",
    "#         print(\"No dataset found\")\n",
    "#         print()\n",
    "\n",
    "#yes\n",
    "# Dataset: ArticularyWordRecognition\n",
    "# Number of Classes: 25\n",
    "# Dimension of path: 9\n",
    "# Length: 144\n",
    "# Train Size, Test Size 275 300\n",
    "\n",
    "# Dataset: AtrialFibrillation\n",
    "# No dataset found\n",
    "\n",
    "#yes\n",
    "# Dataset: BasicMotions\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 6\n",
    "# Length: 100\n",
    "# Train Size, Test Size 40 40\n",
    "\n",
    "# Dataset: CharacterTrajectories\n",
    "# No dataset found\n",
    "\n",
    "#yes\n",
    "# Dataset: Cricket\n",
    "# Number of Classes: 12\n",
    "# Dimension of path: 6\n",
    "# Length: 1197\n",
    "# Train Size, Test Size 108 72\n",
    "\n",
    "# Dataset: DuckDuckGeese\n",
    "# No dataset found\n",
    "\n",
    "# Dataset: EigenWorms\n",
    "# Number of Classes: 5\n",
    "# Dimension of path: 6\n",
    "# Length: 17984\n",
    "# Train Size, Test Size 128 131\n",
    "\n",
    "#why not\n",
    "# Dataset: Epilepsy\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 3\n",
    "# Length: 206\n",
    "# Train Size, Test Size 137 138\n",
    "\n",
    "#longLength\n",
    "# Dataset: EthanolConcentration\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 3\n",
    "# Length: 1751\n",
    "# Train Size, Test Size 261 263\n",
    "\n",
    "# Dataset: ERing\n",
    "# No dataset found\n",
    "\n",
    "#big\n",
    "# Dataset: FaceDetection\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 144\n",
    "# Length: 62\n",
    "# Train Size, Test Size 5890 3524\n",
    "\n",
    "#yes\n",
    "# Dataset: FingerMovements\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 28\n",
    "# Length: 50\n",
    "# Train Size, Test Size 316 100\n",
    "\n",
    "#why not, maybe big length\n",
    "# Dataset: HandMovementDirection\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 10\n",
    "# Length: 400\n",
    "# Train Size, Test Size 160 74\n",
    "\n",
    "#smallTrain\n",
    "# Dataset: Handwriting\n",
    "# Number of Classes: 26\n",
    "# Dimension of path: 3\n",
    "# Length: 152\n",
    "# Train Size, Test Size 150 850\n",
    "\n",
    "#yes\n",
    "# Dataset: Heartbeat\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 61\n",
    "# Length: 405\n",
    "# Train Size, Test Size 204 205\n",
    "\n",
    "#big\n",
    "# Dataset: InsectWingbeat\n",
    "# Number of Classes: 10\n",
    "# Dimension of path: 200\n",
    "# Length: 22\n",
    "# Train Size, Test Size 25000 25000\n",
    "\n",
    "# Dataset: JapaneseVowels\n",
    "# No dataset found\n",
    "\n",
    "#yes\n",
    "# Dataset: Libras\n",
    "# Number of Classes: 15\n",
    "# Dimension of path: 2\n",
    "# Length: 45\n",
    "# Train Size, Test Size 180 180\n",
    "\n",
    "#TODO I SHOULD INCLUDE\n",
    "# Dataset: LSST\n",
    "# Number of Classes: 14\n",
    "# Dimension of path: 6\n",
    "# Length: 36\n",
    "# Train Size, Test Size 2459 2466\n",
    "\n",
    "#length\n",
    "# Dataset: MotorImagery\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 64\n",
    "# Length: 3000\n",
    "# Train Size, Test Size 278 100\n",
    "\n",
    "#yes\n",
    "# Dataset: NATOPS\n",
    "# Number of Classes: 6\n",
    "# Dimension of path: 24\n",
    "# Length: 51\n",
    "# Train Size, Test Size 180 180\n",
    "\n",
    "#TODO NOT TSLEARN. LENGTH WRONG\n",
    "# Dataset: PenDigits\n",
    "# Number of Classes: 10\n",
    "# Dimension of path: 2\n",
    "# Length: 8\n",
    "# Train Size, Test Size 7494 3498\n",
    "\n",
    "#highDim\n",
    "# Dataset: PEMS-SF\n",
    "# Number of Classes: 7\n",
    "# Dimension of path: 963\n",
    "# Length: 144\n",
    "# Train Size, Test Size 267 173\n",
    "\n",
    "#dim=1, big length\n",
    "# Dataset: Phoneme\n",
    "# Number of Classes: 39\n",
    "# Dimension of path: 1\n",
    "# Length: 1024\n",
    "# Train Size, Test Size 214 1896\n",
    "\n",
    "#yes\n",
    "# Dataset: RacketSports\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 6\n",
    "# Length: 30\n",
    "# Train Size, Test Size 151 152\n",
    "\n",
    "#yes\n",
    "# Dataset: SelfRegulationSCP1\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 6\n",
    "# Length: 896\n",
    "# Train Size, Test Size 268 293\n",
    "\n",
    "# Dataset: SelfRegulationSCP2\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 7\n",
    "# Length: 1152\n",
    "# Train Size, Test Size 200 180\n",
    "\n",
    "# Dataset: SpokenArabicDigits\n",
    "# No dataset found\n",
    "\n",
    "#long, also very small set\n",
    "# Dataset: StandWalkJump\n",
    "# Number of Classes: 3\n",
    "# Dimension of path: 4\n",
    "# Length: 2500\n",
    "# Train Size, Test Size 12 15\n",
    "\n",
    "#yes\n",
    "# Dataset: UWaveGestureLibrary\n",
    "# Number of Classes: 8\n",
    "# Dimension of path: 3\n",
    "# Length: 315\n",
    "# Train Size, Test Size 120 320\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation code (for anomaly detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repeated_scores (2, 0, 9)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 180\u001b[0m\n\u001b[1;32m    173\u001b[0m     unique_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_train)\n\u001b[1;32m    175\u001b[0m     cv_given_dataset(X_train, \n\u001b[1;32m    176\u001b[0m                      y_train, \n\u001b[1;32m    177\u001b[0m                      unique_labels, \n\u001b[1;32m    178\u001b[0m                      [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    179\u001b[0m                      fixed_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 180\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[53], line 175\u001b[0m, in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m X_train, y_train, X_test, y_test \u001b[38;5;241m=\u001b[39m UCR_UEA_datasets()\u001b[38;5;241m.\u001b[39mload_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLibras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    173\u001b[0m unique_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_train)\n\u001b[0;32m--> 175\u001b[0m \u001b[43mcv_given_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m                 \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m                 \u001b[49m\u001b[43munique_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m                 \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mfixed_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[53], line 164\u001b[0m, in \u001b[0;36mcv_given_dataset\u001b[0;34m(X, y, unique_labels, kernel_names, fixed_length, k, n_repeats)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepeated_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m, repeated_scores\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    163\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(repeated_scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m#average over repeats\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#max over params\u001b[39;00m\n\u001b[1;32m    165\u001b[0m best \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(scores)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m, scores\u001b[38;5;241m.\u001b[39mshape, scores)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Code/kernel-timeseries-anomaly-detection/.conda/lib/python3.11/site-packages/numpy/core/fromnumeric.py:2820\u001b[0m, in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2703\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_amax_dispatcher)\n\u001b[1;32m   2704\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mamax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2705\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   2706\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2707\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[1;32m   2708\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2818\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[1;32m   2819\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2820\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2821\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/kernel-timeseries-anomaly-detection/.conda/lib/python3.11/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "def repeat_k_folds(X:List,    #dataset\n",
    "                y:np.ndarray, #class labels\n",
    "                k:int = 5,\n",
    "                n_repeats:int = 2,):\n",
    "    repeats = [create_k_folds(X, y, k) for _ in range(n_repeats)]\n",
    "    return repeats\n",
    "\n",
    "\n",
    "def create_k_folds(X:List,    #dataset\n",
    "                y:np.ndarray, #class labels\n",
    "                k:int = 5,):\n",
    "    \"\"\"Generates balanced k-folds for cross-validation, where each fold\n",
    "    is balanced the same as the original dataset.\"\"\"\n",
    "    #is X numpy array?\n",
    "    is_numpy=True if isinstance(X, np.ndarray) else False\n",
    "\n",
    "    #shuffle data\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    X = [X[i] for i in indices]\n",
    "    y = np.array([y[i] for i in indices])\n",
    "    unique_labels = np.unique(y)\n",
    "\n",
    "    #split into classes\n",
    "    classwise = {label:[] for label in unique_labels}\n",
    "    for x, label in zip(X, y):\n",
    "        classwise[label].append(x)\n",
    "\n",
    "    #split into k-folds\n",
    "    classwise_folds = {}\n",
    "    for label, dataclass in classwise.items():\n",
    "        classwise_folds[label] = np.array_split(dataclass, k)\n",
    "    \n",
    "    #create folds\n",
    "    folds=[]\n",
    "    for i in range(k):\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        X_val = []\n",
    "        y_val = []\n",
    "        for label, dataclass in classwise_folds.items():\n",
    "            for j in range(k):\n",
    "                if j!=i:\n",
    "                    X_train.extend(dataclass[j])\n",
    "                    y_train.extend([label]*len(dataclass[j]))\n",
    "                else:\n",
    "                    X_val.extend(dataclass[j])\n",
    "                    y_val.extend([label]*len(dataclass[j]))\n",
    "\n",
    "        #convert to numpy if possible\n",
    "        if is_numpy:\n",
    "            X_train = np.array(X_train)\n",
    "            X_val = np.array(X_val)\n",
    "        y_train = np.array(y_train)\n",
    "        y_val = np.array(y_val)\n",
    "        folds.append([X_train, y_train, X_val, y_val])\n",
    "\n",
    "    return folds\n",
    "\n",
    "\n",
    "def get_hyperparam_combinations(kernel_name:str):\n",
    "    \"\"\"Returns a dict of hyperparameter ranges and a list of all \n",
    "    possible combinations of hyperparameters for the specified kernel\"\"\"\n",
    "    ranges = get_hyperparam_ranges(kernel_name)\n",
    "    values = ranges.values()\n",
    "    keys = ranges.keys()\n",
    "\n",
    "    if ranges:\n",
    "        #create array of all combinations\n",
    "        meshgrid = np.meshgrid(*values)\n",
    "        combinations = np.vstack([x.flatten() for x in meshgrid]).T\n",
    "\n",
    "        #convert to dict\n",
    "        dict_combinations = [dict(zip(keys, vals)) for vals in combinations]\n",
    "\n",
    "        return dict_combinations, ranges\n",
    "    else:\n",
    "        return [], {}\n",
    "\n",
    "\n",
    "def get_hyperparam_ranges(kernel_name:str):\n",
    "    \"\"\" Returns a dict of hyperparameter ranges for the specified kernel.\"\"\"\n",
    "    n_static_params = 7\n",
    "    ranges = {}\n",
    "\n",
    "    #static kernel params. Note that sig and integral kernels also use this\n",
    "    if \"rbf\" in kernel_name:\n",
    "        ranges[\"sigma\"] = np.exp(np.linspace(-5, 0, n_static_params))\n",
    "    elif \"poly\" in kernel_name:\n",
    "        ranges[\"p\"] = np.linspace(1.5, 3.5, n_static_params)\n",
    "\n",
    "    return ranges\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate_folds(folds:List[tuple],\n",
    "                class_to_test,\n",
    "                min_fold_size:int,\n",
    "                fixed_length:bool,\n",
    "                param_dict:Dict[str, Any], # name : value\n",
    "                # ranges:Dict[str, np.array], # name : param range\n",
    "                ):\n",
    "    \"\"\"\"We permform anomaly detection using 'class_to_test' as the normal class. \n",
    "    We then calculate the AUC scores for the given hyperparameters.\"\"\"\n",
    "    scores = np.zeros((len(folds), min_fold_size))\n",
    "    for i, fold in enumerate(folds):\n",
    "        #for now does not support trunc sig. just add Max_trunc to param_dict\n",
    "        X_train, y_train, X_val, y_val = fold\n",
    "        aucs = run_single_kernel_single_label(X_train, y_train, X_val, y_val,\n",
    "                                class_to_test, param_dict,\n",
    "                                fixed_length, verbose=False,\n",
    "                                return_all_levels=True,\n",
    "                                SVD_threshold=0,\n",
    "                                SVD_max_rank=min_fold_size, #TODO maybe change\n",
    "                                )\n",
    "        score = np.max(aucs, axis=1)# max of conf and mahal\n",
    "        score = score[..., 0]       # only interested in roc (and not pr)\n",
    "        #score = np.mean(score, axis=-1)# mean of roc and pr\n",
    "        scores[i, :] = score[:min_fold_size]\n",
    "    return np.mean(scores, axis=0) #average across folds\n",
    "\n",
    "\n",
    "\n",
    "def cv_given_dataset(X:List,                #Training Dataset\n",
    "                    y:np.array,            #Training class labels\n",
    "                    unique_labels:np.array, #Unique class labels\n",
    "                    kernel_names:List[str],\n",
    "                    fixed_length:bool,\n",
    "                    k:int = 5,          #k-fold cross validation\n",
    "                    n_repeats:int = 2,  #repeats of k-fold CV\n",
    "                    ):\n",
    "    model_params_CV = {}\n",
    "    # anomaly detection model is specified by a \n",
    "    # pair (normal class label, param_dict)\n",
    "    # or rather (label, kernel_name, *params)\n",
    "\n",
    "    # model_params_CV[pair] = CV_scores\n",
    "    repeats_and_folds = repeat_k_folds(X, y, k, n_repeats)\n",
    "    for kernel_name in kernel_names:\n",
    "        #get hyperparams\n",
    "        hyperparams, ranges = get_hyperparam_combinations(kernel_name)\n",
    "\n",
    "        #loop over normal class\n",
    "        for label in unique_labels:\n",
    "\n",
    "            #calc minimum size of the label class\n",
    "            min_fold_size = min([len(np.where(y_train==label)[0]) \n",
    "                                 for (_, y_train, _, _) in repeats_and_folds[0]])\n",
    "            repeated_scores = np.zeros((n_repeats, len(hyperparams), min_fold_size))\n",
    "\n",
    "            #loop over repeats and params\n",
    "            for repeat_idx, folds in enumerate(repeats_and_folds):\n",
    "                for i, param_dict in enumerate(hyperparams):\n",
    "                    param_dict[\"kernel_name\"] = kernel_name\n",
    "                    param_dict[\"normal_class_label\"] = label\n",
    "                    scores = evaluate_folds(folds, label,\n",
    "                                            min_fold_size, fixed_length,\n",
    "                                            param_dict)\n",
    "                    repeated_scores[repeat_idx, i] = scores\n",
    "            \n",
    "            #average scores\n",
    "            print(\"repeated_scores\", repeated_scores.shape)\n",
    "            scores = np.mean(repeated_scores, axis=0) #average over repeats\n",
    "            scores = np.max(scores, axis=0) #max over params\n",
    "            best = np.argmax(scores)\n",
    "            print(\"scores\", scores.shape, scores)\n",
    "            print(\"best\", best)\n",
    "            assert False\n",
    "\n",
    "\n",
    "def test():\n",
    "    X_train, y_train, X_test, y_test = UCR_UEA_datasets().load_dataset(\"Libras\")\n",
    "    unique_labels = np.unique(y_train)\n",
    "\n",
    "    cv_given_dataset(X_train, \n",
    "                     y_train, \n",
    "                     unique_labels, \n",
    "                     [\"linear\"],\n",
    "                     fixed_length=True)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparam_combinations(kernel_name:str):\n",
    "    \"\"\"Returns a dict of hyperparameter ranges and a list of all \n",
    "    possible combinations of hyperparameters for the specified kernel\"\"\"\n",
    "    ranges = get_hyperparam_ranges(kernel_name)\n",
    "    values = ranges.values()\n",
    "    meshgrid = np.meshgrid(*values)\n",
    "    combinations = np.vstack([x.flatten() for x in meshgrid]).T\n",
    "    return combinations, ranges\n",
    "\n",
    "\n",
    "def get_hyperparam_ranges(kernel_name:str):\n",
    "    \"\"\" Returns a dict of hyperparameter ranges for the specified kernel.\"\"\"\n",
    "    n_static_params = 9\n",
    "    max_trunc_order = 15\n",
    "    ranges = {}\n",
    "\n",
    "    #static kernel params\n",
    "    if \"rbf\" in kernel_name:\n",
    "        ranges[\"sigma\"] = np.exp(np.linspace(-5, 0, n_static_params))\n",
    "    elif \"poly\" in kernel_name:\n",
    "        ranges[\"p\"] = np.linspace(1.5, 3.5, n_static_params)\n",
    "    \n",
    "    #trunc sig params\n",
    "    if \"truncated sig\" in kernel_name:\n",
    "        ranges[\"order\"] = np.arange(1, max_trunc_order+1)\n",
    "    return ranges\n",
    "\n",
    "hyperparams, ranges = get_hyperparam_combinations(\"truncated sig rbf\")\n",
    "\n",
    "#print(hyperparams)\n",
    "\n",
    "keys = ranges.keys()\n",
    "\n",
    "better = [dict(zip(keys, vals)) for vals in hyperparams]\n",
    "print(better)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = ranges.items()\n",
    "print(x, \"x\\n\")\n",
    "print(y, \"y\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
