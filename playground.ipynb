{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "import sklearn.utils\n",
    "import sklearn.metrics\n",
    "import iisignature\n",
    "import torch\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Optional, Dict, Set, Callable\n",
    "from joblib import Memory, Parallel, delayed\n",
    "import tslearn\n",
    "import tslearn.metrics\n",
    "from tslearn.datasets import UCR_UEA_datasets\n",
    "import sigkernel\n",
    "import scipy\n",
    "from scipy.interpolate import interp1d\n",
    "from numba import njit\n",
    "\n",
    "from signature import streams_to_sigs, transform_stream\n",
    "from conformance import BaseclassConformanceScore, pairwise_kernel_gram, stream_to_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: ArticularyWordRecognition\n",
    "# Number of Classes: 25\n",
    "# Dimension of path: 9\n",
    "# Length: 144\n",
    "# Train Size, Test Size 275 300\n",
    "\n",
    "# Dataset: AtrialFibrillation\n",
    "# No dataset found\n",
    "\n",
    "# Dataset: BasicMotions\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 6\n",
    "# Length: 100\n",
    "# Train Size, Test Size 40 40\n",
    "\n",
    "# Dataset: CharacterTrajectories\n",
    "# No dataset found\n",
    "\n",
    "# Dataset: Cricket\n",
    "# Number of Classes: 12\n",
    "# Dimension of path: 6\n",
    "# Length: 1197\n",
    "# Train Size, Test Size 108 72\n",
    "\n",
    "# Dataset: DuckDuckGeese\n",
    "# No dataset found\n",
    "\n",
    "# Dataset: EigenWorms\n",
    "# Number of Classes: 5\n",
    "# Dimension of path: 6\n",
    "# Length: 17984\n",
    "# Train Size, Test Size 128 131\n",
    "\n",
    "# Dataset: Epilepsy\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 3\n",
    "# Length: 206\n",
    "# Train Size, Test Size 137 138\n",
    "\n",
    "# Dataset: EthanolConcentration\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 3\n",
    "# Length: 1751\n",
    "# Train Size, Test Size 261 263\n",
    "\n",
    "# Dataset: ERing\n",
    "# No dataset found\n",
    "\n",
    "# Dataset: FaceDetection\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 144\n",
    "# Length: 62\n",
    "# Train Size, Test Size 5890 3524\n",
    "\n",
    "# Dataset: FingerMovements\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 28\n",
    "# Length: 50\n",
    "# Train Size, Test Size 316 100\n",
    "\n",
    "# Dataset: HandMovementDirection\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 10\n",
    "# Length: 400\n",
    "# Train Size, Test Size 160 74\n",
    "\n",
    "# Dataset: Handwriting\n",
    "# Number of Classes: 26\n",
    "# Dimension of path: 3\n",
    "# Length: 152\n",
    "# Train Size, Test Size 150 850\n",
    "\n",
    "# Dataset: Heartbeat\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 61\n",
    "# Length: 405\n",
    "# Train Size, Test Size 204 205\n",
    "\n",
    "# Dataset: InsectWingbeat\n",
    "# Number of Classes: 10\n",
    "# Dimension of path: 200\n",
    "# Length: 22\n",
    "# Train Size, Test Size 25000 25000\n",
    "\n",
    "# Dataset: JapaneseVowels\n",
    "# No dataset found\n",
    "\n",
    "# Dataset: Libras\n",
    "# Number of Classes: 15\n",
    "# Dimension of path: 2\n",
    "# Length: 45\n",
    "# Train Size, Test Size 180 180\n",
    "\n",
    "# Dataset: LSST\n",
    "# Number of Classes: 14\n",
    "# Dimension of path: 6\n",
    "# Length: 36\n",
    "# Train Size, Test Size 2459 2466\n",
    "\n",
    "# Dataset: MotorImagery\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 64\n",
    "# Length: 3000\n",
    "# Train Size, Test Size 278 100\n",
    "\n",
    "# Dataset: NATOPS\n",
    "# Number of Classes: 6\n",
    "# Dimension of path: 24\n",
    "# Length: 51\n",
    "# Train Size, Test Size 180 180\n",
    "\n",
    "# Dataset: PenDigits\n",
    "# Number of Classes: 10\n",
    "# Dimension of path: 2\n",
    "# Length: 8\n",
    "# Train Size, Test Size 7494 3498\n",
    "\n",
    "# Dataset: PEMS-SF\n",
    "# Number of Classes: 7\n",
    "# Dimension of path: 963\n",
    "# Length: 144\n",
    "# Train Size, Test Size 267 173\n",
    "\n",
    "# Dataset: Phoneme\n",
    "# Number of Classes: 39\n",
    "# Dimension of path: 1\n",
    "# Length: 1024\n",
    "# Train Size, Test Size 214 1896\n",
    "\n",
    "# Dataset: RacketSports\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 6\n",
    "# Length: 30\n",
    "# Train Size, Test Size 151 152\n",
    "\n",
    "# Dataset: SelfRegulationSCP1\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 6\n",
    "# Length: 896\n",
    "# Train Size, Test Size 268 293\n",
    "\n",
    "# Dataset: SelfRegulationSCP2\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 7\n",
    "# Length: 1152\n",
    "# Train Size, Test Size 200 180\n",
    "\n",
    "# Dataset: SpokenArabicDigits\n",
    "# No dataset found\n",
    "\n",
    "# Dataset: StandWalkJump\n",
    "# Number of Classes: 3\n",
    "# Dimension of path: 4\n",
    "# Length: 2500\n",
    "# Train Size, Test Size 12 15\n",
    "\n",
    "# Dataset: UWaveGestureLibrary\n",
    "# Number of Classes: 8\n",
    "# Dimension of path: 3\n",
    "# Length: 315\n",
    "# Train Size, Test Size 120 320\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tslearn datasets (equal length)\n",
    "\n",
    "* equal length (in time) UCR_UEA multivariate time series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#static kernels#########################################################################\n",
    "######################## Static Kernels on R^d ###########################\n",
    "##########################################################################\n",
    "\n",
    "def _check_gram_dims(X:np.ndarray, \n",
    "                     Y:np.ndarray,\n",
    "                     diag:bool = False,):\n",
    "    \"\"\"Stacks the input into a Gram matrix shape (N1, N2, ..., d) or\n",
    "    into a diagonal Gram shape (N1, ..., d) if diag and N1==N2.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Shape (N1, ... , d).\n",
    "        Y (np.ndarray): Shape (N2, ... , d).\n",
    "        diag (bool): If True, use diagonal Gram shape.\n",
    "    \"\"\"\n",
    "    len1 = len(X.shape)\n",
    "    len2 = len(Y.shape)\n",
    "    if (len1<2) or (len2<2):\n",
    "        raise ValueError(\"X and Y must have at least 2 dimensions, found {len1} and {len2}.\")\n",
    "    if X.shape[1:] != Y.shape[1:]:\n",
    "        raise ValueError(\"X and Y must have the same dimensions except for the first axis.\")\n",
    "\n",
    "    N1 = X.shape[0]\n",
    "    N2 = Y.shape[0]\n",
    "    if diag and N1!=N2:\n",
    "        raise ValueError(\"If 'diag' is True, X and Y must have the same number of samples.\")\n",
    "\n",
    "\n",
    "def linear_kernel_gram(X:np.ndarray, \n",
    "                       Y:np.ndarray,\n",
    "                       diag:bool = False,\n",
    "                       divide_by_dims:bool = True,\n",
    "                       ):\n",
    "    \"\"\"Computes the Rd inner product matrix <x_i, y_j> or diagonal <x_i, y_i>.\n",
    "    The inputs dimensions can only differ in the first axis.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Shape (N1, ... , d).\n",
    "        Y (np.ndarray): Shape (N2, ... , d).\n",
    "        diag (bool): If True, computes the diagonal of the gram matrix.\n",
    "        divide_by_dims (bool): If True, divides the result by the dimension d.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (N1, N2, ...) or (N1, ...) if diag=True.\n",
    "    \"\"\"\n",
    "    _check_gram_dims(X, Y, diag)\n",
    "    if diag:\n",
    "        #out_i... = sum(X_i...k * Y_i...k)\n",
    "        out = np.einsum('i...k,i...k -> i...', X, Y)\n",
    "    else:\n",
    "        #out_ij... = sum(X_i...k * Y_j...k)\n",
    "        out = np.einsum('i...k,j...k -> ij...', X, Y)\n",
    "    \n",
    "    if divide_by_dims:\n",
    "        d = X.shape[-1]\n",
    "        out = out/d\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def rbf_kernel_gram(X:np.ndarray, \n",
    "                    Y:np.ndarray,\n",
    "                    sigma:float,\n",
    "                    diag:bool = False,\n",
    "                    divide_by_dims:bool = True,\n",
    "                    ):\n",
    "    \"\"\"Computes the RBF gram matrix k(x_i, y_j) or diagonal k(x_i, y_i).\n",
    "    The inputs dimensions can only differ in the first axis.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Shape (N1, ... , d).\n",
    "        Y (np.ndarray): Shape (N2, ... , d).\n",
    "        sigma (float): RBF parameter\n",
    "        diag (bool): If True, computes the diagonal of the gram matrix.\n",
    "        divide_by_dims (bool): If True, normalizes the norm by the dimension d.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (N1, N2, ...) or (N1, ...) if diag=True.\n",
    "    \"\"\"\n",
    "    if diag:\n",
    "        diff = X-Y\n",
    "        norms_squared = linear_kernel_gram(diff, diff, diag=True, \n",
    "                                           divide_by_dims=divide_by_dims)\n",
    "    else:\n",
    "        xx = linear_kernel_gram(X, X, diag=True, divide_by_dims=divide_by_dims)\n",
    "        xy = linear_kernel_gram(X, Y, diag=False, divide_by_dims=divide_by_dims)\n",
    "        yy = linear_kernel_gram(Y, Y, diag=True, divide_by_dims=divide_by_dims)\n",
    "        norms_squared = xx[:, np.newaxis] + yy[np.newaxis, :] - 2*xy\n",
    "\n",
    "    d= X.shape[-1]\n",
    "    return np.exp(-sigma * norms_squared)\n",
    "\n",
    "\n",
    "def poly_kernel_gram(X:np.ndarray, \n",
    "                     Y:np.ndarray,\n",
    "                     p:float, #eg 2 or 3\n",
    "                     diag:bool = False,\n",
    "                     divide_by_dims:bool = True,):\n",
    "    \"\"\"Computes the polynomial kernel (<x_i, y_j> + 1)^p.\n",
    "    The inputs dimensions can only differ in the first axis.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Shape (N1, ... , d).\n",
    "        Y (np.ndarray): Shape (N2, ... , d).\n",
    "        p (float): Polynomial degree.\n",
    "        diag (bool): If True, computes the diagonal of the gram matrix.\n",
    "        divide_by_dims (bool): If True, normalizes the norm by the dimension d.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (N1, N2, ...) or (N1, ...) if diag=True.\n",
    "    \"\"\"\n",
    "    d = X.shape[-1]\n",
    "    xy = linear_kernel_gram(X, Y, diag, divide_by_dims)\n",
    "    return (xy + 1)**p\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "################### time series Integral Kernel of static kernel ######################\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "def integral_kernel(s1: np.ndarray,\n",
    "                    s2: np.ndarray,\n",
    "                    static_diag_kernel:Callable,\n",
    "                    )-> float:\n",
    "    \"\"\"Computes the integral kernel K(x, y) = \\int k(x_t, y_t) dt \n",
    "    given static kernel and two piecewise linear paths.\n",
    "\n",
    "    Args:\n",
    "        s1 (np.ndarray): A time series of shape (T1, d).\n",
    "        s2 (np.ndarray): A time series of shape (T2, d).\n",
    "        static_diag_kernel_gram (Callable): Takes in two arrays of shape (M, d) \n",
    "                        and outputs the diagonal Gram <x_m, y_m> of shape (M).\n",
    "    \"\"\"\n",
    "    #Find all breakpoints of the piecewise linear paths\n",
    "    T1, d = s1.shape\n",
    "    T2, d = s2.shape\n",
    "    times = np.concatenate([np.linspace(0, 1, T1), np.linspace(0, 1, T2)])\n",
    "    times = sorted(np.unique(times))\n",
    "\n",
    "    #Add the extra breakpoints to the paths\n",
    "    f1 = interp1d(np.linspace(0, 1, T1), s1, axis=0, assume_sorted=True)\n",
    "    f2 = interp1d(np.linspace(0, 1, T2), s2, axis=0, assume_sorted=True)\n",
    "    x = f1(times) #shape (len(times), d)\n",
    "    y = f2(times)\n",
    "\n",
    "    #calculate k(x_t, y_t) for each t\n",
    "    Kt = static_diag_kernel(x, y)\n",
    "\n",
    "    #return integral of k(x_t, y_t) dt\n",
    "    return np.trapz(Kt, times)\n",
    "\n",
    "\n",
    "def integral_kernel_gram(\n",
    "        X:List[np.ndarray],\n",
    "        Y:List[np.ndarray],\n",
    "        static_kernel_gram:Callable, #either linear_kernel_gram or rbf_kernel_gram with \"diag\" argument\n",
    "        variable_length:bool,\n",
    "        sym:bool = False,\n",
    "    ):\n",
    "    \"\"\"Computes the Gram matrix K(X_i, Y_j) of the integral kernel \n",
    "    K(x, y) = \\int k(x_t, y_t) dt.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        static_kernel_gram (Callable): Gram kernel function taking in two ndarrays and\n",
    "                    one boolean \"diag\" argument, see e.g. 'linear_kernel_gram' or \n",
    "                    'rbf_kernel_gram'.\n",
    "        X (List[np.ndarray]): List of time series of shape (T_i, d).\n",
    "        Y (List[np.ndarray]): List of time series of shape (T_j, d).\n",
    "        variable_length (bool): If False, uses the optimized kernels for equal \n",
    "                                length time series.\n",
    "        sym (bool): If True, computes the symmetric Gram matrix.\n",
    "    \"\"\"\n",
    "    if not variable_length:\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "        ijKt = static_kernel_gram(X, Y, False) #diag=False\n",
    "\n",
    "        #return integral of k(x_t, y_t) dt for each pair x and y\n",
    "        N, T, d = X.shape\n",
    "        return np.trapz(ijKt, dx=1/(T-1), axis=-1)\n",
    "    else:\n",
    "        static_ker = lambda a,b : static_kernel_gram(a,b, True) #diag=True\n",
    "        pairwise_int_ker = lambda s1, s2 : integral_kernel(s1, s2, static_ker)\n",
    "        return pairwise_kernel_gram(X,\n",
    "                                    Y,\n",
    "                                    pairwise_int_ker,\n",
    "                                    sym)\n",
    "\n",
    "############################################################################\n",
    "################# signature kernels of static kernels ######################\n",
    "############################################################################\n",
    "\n",
    "\n",
    "def sig_kernel(s1:np.ndarray, \n",
    "               s2:np.ndarray, \n",
    "               order:int,\n",
    "               static_kernel_gram:Callable = linear_kernel_gram,\n",
    "               only_last:bool = True):\n",
    "    \"\"\"s1 and s2 are time series of shape (T_i, d)\"\"\"\n",
    "    K = static_kernel_gram(s1, s2)\n",
    "    nabla = K[1:, 1:] + K[:-1, :-1] - K[1:, :-1] - K[:-1, 1:]\n",
    "    sig_kers = jitted_trunc_sig_kernel(nabla, order)\n",
    "    if only_last:\n",
    "        return sig_kers[-1]\n",
    "    else:\n",
    "        return sig_kers\n",
    "\n",
    "\n",
    "@njit\n",
    "def reverse_cumsum(arr:np.ndarray, axis:int): #ndim=2\n",
    "    \"\"\"JITed reverse cumulative sum along the specified axis.\n",
    "    (np.cumsum with axis is not natively supported by Numba)\"\"\"\n",
    "    A = arr.copy()\n",
    "    if axis==0:\n",
    "        for i in np.arange(A.shape[0]-2, -1, -1):\n",
    "            A[i, :] += A[i+1, :]\n",
    "    else: #axis==1\n",
    "        for i in np.arange(A.shape[1]-2, -1, -1):\n",
    "            A[:,i] += A[:,i+1]\n",
    "    return A\n",
    "\n",
    "\n",
    "@njit\n",
    "def jitted_trunc_sig_kernel(nabla:np.ndarray, # gram matrix (T_1, T_2)\n",
    "                            order:int,\n",
    "                            ):\n",
    "    \"\"\"Given difference matrix nabla_ij = K[i+1, j+1] + K[i, j] - K[i+1, j] - K[i, j+1],\n",
    "    computes the truncated signature kernel of all orders up to 'order'.\"\"\"\n",
    "    B = np.ones((order+1, order+1, order+1, *nabla.shape))\n",
    "    for d in np.arange(order):\n",
    "        for n in np.arange(order-d):\n",
    "            for m in np.arange(order-d):\n",
    "                B[d+1,n,m] = 1 + nabla/(n+1)/(m+1)*B[d, n+1, m+1]\n",
    "                r1 = reverse_cumsum(nabla * B[d, n+1, 1] / (n+1), axis=0)\n",
    "                B[d+1,n,m, :-1, :] += r1[1:, :]\n",
    "                r2 = reverse_cumsum(nabla * B[d, 1, m+1] / (m+1), axis=1)\n",
    "                B[d+1,n,m, :, :-1] += r2[:, 1:]\n",
    "                rr = reverse_cumsum(nabla * B[d, 1, 1], axis=0)\n",
    "                rr = reverse_cumsum(rr, axis=1)\n",
    "                B[d+1,n,m, :-1, :-1] += rr[1:, 1:]\n",
    "\n",
    "    return B[:,0,0,0,0]\n",
    "\n",
    "\n",
    "def sig_kernel_gram(\n",
    "        X:List[np.ndarray],\n",
    "        Y:List[np.ndarray],\n",
    "        order:int,\n",
    "        static_kernel_gram:Callable,\n",
    "        only_last:bool = True,\n",
    "        sym:bool = False,\n",
    "    ):\n",
    "    \"\"\"Computes the Gram matrix k_sig(X_i, Y_j) of the signature kernel,\n",
    "    given the static kernel k(x, y) and the truncation order.\n",
    "\n",
    "    Args:\n",
    "        X (List[np.ndarray]): List of time series of shape (T_i, d).\n",
    "        Y (List[np.ndarray]): List of time series of shape (T_j, d).\n",
    "        static_kernel_gram (Callable): Gram kernel function taking in two ndarrays,\n",
    "                            see e.g. 'linear_kernel_gram' or 'rbf_kernel_gram'.\n",
    "        order (int): Truncation level of the signature kernel.\n",
    "        only_last (bool): If False, returns results of all truncation levels up to 'order'.\n",
    "        sym (bool): If True, computes the symmetric Gram matrix.\n",
    "    \"\"\"\n",
    "    pairwise_ker = lambda s1, s2 : sig_kernel(s1, s2, order, static_kernel_gram, only_last)\n",
    "    return pairwise_kernel_gram(X,\n",
    "                                Y,\n",
    "                                pairwise_ker,\n",
    "                                sym)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#experiment code\n",
    "\n",
    "\n",
    "def print_dataset_stats(num_classes, d, T, N_train, N_test):\n",
    "    print(\"Number of Classes:\", num_classes)\n",
    "    print(\"Dimension of path:\", d)\n",
    "    print(\"Length:\", T)\n",
    "    print(\"Train:\", N_train)\n",
    "    print(\"Test:\", N_test)\n",
    "\n",
    "\n",
    "def case_static(train:np.ndarray, \n",
    "                test:np.ndarray,\n",
    "                static_kernel_gram:Callable,):\n",
    "    \"\"\"Calculates the gram matrices of equal length time series for \n",
    "    a static kernel on R^d. Train and test are of shape (N1, T, d) \n",
    "    and (N2, T, d). Static kernel should take in two arrays of shape \n",
    "    (M, T*d) and return the Gram matrix.\"\"\"\n",
    "    N1, T, d = train.shape\n",
    "    N2, _, _ = test.shape\n",
    "    train = train.reshape(N1, -1)\n",
    "    test = test.reshape(N2, -1)\n",
    "    vv_gram = static_kernel_gram(train, train)\n",
    "    uv_gram = static_kernel_gram(test, train)\n",
    "    return vv_gram, uv_gram\n",
    "\n",
    "\n",
    "def case_linear(train:np.ndarray, \n",
    "                test:np.ndarray):\n",
    "    \"\"\"Calculates the gram matrices for the euclidean inner product.\n",
    "    Train and test are of shape (N1, T, d) and (N2, T, d).\"\"\"\n",
    "    return case_static(train, test, linear_kernel_gram)\n",
    "\n",
    "\n",
    "def case_rbf(train:np.ndarray, \n",
    "             test:np.ndarray,\n",
    "             sigma:float):\n",
    "    \"\"\"Calculates the gram matrices for the rbf kernel.\n",
    "    Train and test are of shape (N1, T, d) and (N2, T, d).\"\"\"\n",
    "    rbf_ker = lambda X, Y : rbf_kernel_gram(X, Y, sigma)\n",
    "    return case_static(train, test, rbf_ker)\n",
    "\n",
    "\n",
    "def case_poly(train:np.ndarray, \n",
    "              test:np.ndarray,\n",
    "              p:float):\n",
    "    \"\"\"Calculates the gram matrices for the rbf kernel.\n",
    "    Train and test are of shape (N1, T, d) and (N2, T, d).\"\"\"\n",
    "    poly_ker = lambda X, Y : poly_kernel_gram(X, Y, p)\n",
    "    return case_static(train, test, poly_ker)\n",
    "\n",
    "\n",
    "def case_gak(train:List[np.ndarray], \n",
    "                   test:List[np.ndarray], \n",
    "                   variable_length:bool,\n",
    "                   sigma:float = 1.0,):\n",
    "    \"\"\"Calculates the gram matrices for the gak kernel.\n",
    "    Train and test are lists of possibly variable length multidimension \n",
    "    time series of shape (T_i, d)\"\"\"\n",
    "    #pick sigma parameter according to GAK paper\n",
    "    if not variable_length:\n",
    "        sigma = tslearn.metrics.sigma_gak(np.array(train))\n",
    "\n",
    "    #compute gram matrices\n",
    "    kernel = lambda s1, s2 : tslearn.metrics.gak(s1, s2, sigma)\n",
    "    vv_gram = pairwise_kernel_gram(train, train, kernel, sym=True, disable_tqdm=False)\n",
    "    uv_gram = pairwise_kernel_gram(test, train, kernel, sym=False, disable_tqdm=False)\n",
    "    return vv_gram, uv_gram\n",
    "\n",
    "\n",
    "# Solely to be used in sigkernel library. See e.g. sigkernel.LinearKernel.\n",
    "# Had to reimplement it since the original class is missing the scalar in \n",
    "# the Gram method\n",
    "class LinearKernel():\n",
    "    def __init__(self, scale=1.0):\n",
    "        self.scale = scale\n",
    "        \n",
    "    def batch_kernel(self, X, Y):\n",
    "        return self.scale*torch.bmm(X, Y.permute(0,2,1))\n",
    "\n",
    "    def Gram_matrix(self, X, Y):\n",
    "        return self.scale * torch.einsum('ipk,jqk->ijpq', X, Y)\n",
    "    \n",
    "class PolyKernel():\n",
    "    def __init__(self, scale=1.0, p=2):\n",
    "        self.scale = scale\n",
    "        self.p = p\n",
    "        \n",
    "    def batch_kernel(self, X, Y):\n",
    "        return self.scale * (1+torch.bmm(X, Y.permute(0,2,1)))**self.p\n",
    "\n",
    "    def Gram_matrix(self, X, Y):\n",
    "        return self.scale * (1+torch.einsum('ipk,jqk->ijpq', X, Y))**self.p\n",
    "\n",
    " \n",
    "def case_sig_pde(train:List[np.ndarray], \n",
    "                 test:List[np.ndarray], \n",
    "                 dyadic_order:int = 3,\n",
    "                 static_kernel = sigkernel.LinearKernel(),\n",
    "                ):\n",
    "    \"\"\"Calculates the signature kernel gram matrices of the train and test.\n",
    "    Train and test are lists of possibly variable length multidimension \n",
    "    time series of shape (T_i, d)\"\"\"\n",
    "    sig_kernel = sigkernel.SigKernel(static_kernel, dyadic_order)\n",
    "    kernel = lambda s1, s2 : sig_kernel.compute_kernel(\n",
    "                                stream_to_torch(s1), \n",
    "                                stream_to_torch(s2)).numpy()[0]\n",
    "    vv_gram = pairwise_kernel_gram(train, train, kernel, sym=True, disable_tqdm=False)\n",
    "    uv_gram = pairwise_kernel_gram(test, train, kernel, sym=False, disable_tqdm=False)\n",
    "    return vv_gram, uv_gram\n",
    "\n",
    "\n",
    "def calc_grams(train:List[np.ndarray], \n",
    "               test:List[np.ndarray],\n",
    "               kernel_name:str, \n",
    "               variable_length:bool, \n",
    "               dyadic_order:int,        #for signature pde\n",
    "               order:int,               #for truncated signature\n",
    "               sigma:float = 2,         #for rbf\n",
    "               p:float = 2,             #for polynomial\n",
    "               ):   \n",
    "    \"\"\"Calculates gram matrices <train, train>, <test, train> given a kernel.\n",
    "    Train and test are lists of possibly variable length multidimension time \n",
    "    series of shape (T_i, d)\"\"\"\n",
    "\n",
    "    #Transform to array if possible\n",
    "    if not variable_length:\n",
    "        train = np.array(train)\n",
    "        test = np.array(test)\n",
    "    \n",
    "    #choose method based on kernel name\n",
    "    if kernel_name == \"linear\":\n",
    "        return case_linear(train, test)\n",
    "    \n",
    "    elif kernel_name == \"rbf\":\n",
    "        return case_rbf(train, test, sigma)\n",
    "    \n",
    "    elif kernel_name == \"poly\":\n",
    "        return case_poly(train, test, p)\n",
    "\n",
    "    elif kernel_name == \"gak\":\n",
    "        return case_gak(train, test, variable_length)\n",
    "\n",
    "    elif kernel_name == \"truncated sig\":\n",
    "        vv_gram = sig_kernel_gram(train, train, order, linear_kernel_gram, sym=True)\n",
    "        uv_gram = sig_kernel_gram(test, train, order, linear_kernel_gram)\n",
    "        return vv_gram, uv_gram\n",
    "    \n",
    "    elif kernel_name == \"truncated sig rbf\":\n",
    "        ker = lambda X, Y: rbf_kernel_gram(X, Y, sigma)\n",
    "        vv_gram = sig_kernel_gram(train, train, order, ker, sym=True)\n",
    "        uv_gram = sig_kernel_gram(test, train, order, ker)\n",
    "        return vv_gram, uv_gram\n",
    "    \n",
    "    elif kernel_name == \"truncated sig poly\":\n",
    "        ker = lambda X, Y : poly_kernel_gram(X, Y, p)\n",
    "        vv_gram = sig_kernel_gram(train, train, order, ker, sym=True)\n",
    "        uv_gram = sig_kernel_gram(test, train, order, ker)\n",
    "        return vv_gram, uv_gram\n",
    "    \n",
    "    elif kernel_name == \"signature pde\":\n",
    "        return case_sig_pde(train, \n",
    "                        test, \n",
    "                        dyadic_order=dyadic_order, \n",
    "                        static_kernel=LinearKernel(1/train[0].shape[-1]),)\n",
    "    \n",
    "    elif kernel_name == \"signature pde rbf\":\n",
    "        return case_sig_pde(train, \n",
    "                        test, \n",
    "                        dyadic_order=dyadic_order, \n",
    "                        static_kernel=sigkernel.RBFKernel(sigma * train[0].shape[-1]),)\n",
    "\n",
    "    elif kernel_name == \"signature pde poly\":\n",
    "        return case_sig_pde(train, \n",
    "                        test, \n",
    "                        dyadic_order=dyadic_order, \n",
    "                        static_kernel=PolyKernel(1/train[0].shape[-1], p),)\n",
    "    \n",
    "    elif kernel_name == \"integral linear\":\n",
    "        vv_gram = integral_kernel_gram(train, train, linear_kernel_gram, variable_length, sym=True)\n",
    "        uv_gram = integral_kernel_gram(test, train, linear_kernel_gram, variable_length)\n",
    "        return vv_gram, uv_gram\n",
    "\n",
    "    elif kernel_name == \"integral rbf\":\n",
    "        ker = lambda X, Y, diag: rbf_kernel_gram(X, Y, sigma, diag)\n",
    "        vv_gram = integral_kernel_gram(train, train, ker, variable_length, sym=True)\n",
    "        uv_gram = integral_kernel_gram(test, train, ker, variable_length)\n",
    "        return vv_gram, uv_gram\n",
    "\n",
    "    elif kernel_name == \"integral poly\":\n",
    "        ker = lambda X, Y, diag : poly_kernel_gram(X, Y, p, diag)\n",
    "        vv_gram = integral_kernel_gram(train, train, ker, variable_length, sym=True)\n",
    "        uv_gram = integral_kernel_gram(test, train, ker, variable_length)\n",
    "        return vv_gram, uv_gram\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid kernel name:\", kernel_name)\n",
    "\n",
    "\n",
    "def normalize_streams(train:np.ndarray, \n",
    "                      test:np.ndarray,\n",
    "                      ):\n",
    "    \"\"\"Inputs are 3D arrays of shape (N, T, d) where N is the number of time series, \n",
    "    T is the length of each time series, and d is the dimension of each time series.\"\"\"\n",
    "    # Normalize data by training set mean and std\n",
    "    mean = np.mean(train, axis=0, keepdims=True)\n",
    "    std = np.std(train, axis=0, keepdims=True)\n",
    "    train = (train - mean) / std\n",
    "    test = (test - mean) / std\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def run_single_kernel_single_label(\n",
    "        corpus:List[np.ndarray], \n",
    "        X_test:List[np.ndarray], \n",
    "        y_test:np.array, #one vs rest labels\n",
    "        class_to_test,\n",
    "        kernel_name:str,\n",
    "        variable_length:bool,\n",
    "        dyadic_order:int = 5,   #for signature pde\n",
    "        order:int = 10, #for truncated signature\n",
    "        SVD_threshold:float = 0.01,\n",
    "        SVD_max_rank:Optional[int] = None,\n",
    "        sigma:float = 2,       #for rbf\n",
    "        p:float=2,             #for polynomial\n",
    "        verbose:bool = False,\n",
    "        vv_gram=None,\n",
    "        uv_gram=None,\n",
    "        ):\n",
    "    \"\"\"Computes the AUC scores (weighted one vs rest) for a single kernel,\n",
    "    using kernelized nearest neighbour variance adjusted distances.\n",
    "\n",
    "    Args:\n",
    "        X_train (List[np.ndarray]): List of time series of shape (T_i, d).\n",
    "        y_train (np.array): 1-dim array of class labels.\n",
    "        X_test (List[np.ndarray]): List of time series of shape (T_i, d).\n",
    "        y_test (np.array): 1-dim array of class labels.\n",
    "        unique_labels (np.array): Array of unique class labels.\n",
    "        kernel_name (str): Name of the kernel to use.\n",
    "        variable_length (bool): If False, uses the optimized kernels for equal \n",
    "                                length time series.\n",
    "        normalize (bool): If True, normalizes train and test by the training set\n",
    "                          mean and std.\n",
    "        dyadic_order (int): Dyadic order for PDE solver \n",
    "                            (int > 0, higher = more accurate but slower).\n",
    "        max_batch (int): Batch size in sig kernel computations.\n",
    "        trunc_sig_dim_bound (int): Upper bound on the dimensionality of the \n",
    "                                  truncated signature.\n",
    "        SVD_threshold (float): Sets all eigenvalues below this threshold to be 0.\n",
    "        SVD_max_rank (int): Sets all SVD eigenvalues to be 0 beyond 'SVD_max_rank'.\n",
    "    \"\"\"\n",
    "    # 2 methods (conf, mahal), 2 metrics (roc_auc, pr_auc)\n",
    "    aucs = np.zeros( (2, 2) ) \n",
    "\n",
    "    # Calculate amomaly distancce scores for all test samples\n",
    "    if (vv_gram is None) and (uv_gram is None):\n",
    "        vv_gram, uv_gram = calc_grams(corpus, X_test, kernel_name, \n",
    "                                variable_length, dyadic_order,\n",
    "                                order, sigma=sigma, p=p)\n",
    "    scorer = BaseclassConformanceScore(vv_gram, SVD_threshold, print_rank=verbose, \n",
    "                                        SVD_max_rank=SVD_max_rank)\n",
    "    dists = np.array([scorer._anomaly_distance(sample, method=\"both\") \n",
    "                        for sample in uv_gram]).T\n",
    "    distances_conf, distances_mahal = dists\n",
    "\n",
    "    # Calculate one vs rest AUC, weighted by size of class\n",
    "    for idx_conf_mahal, distances in enumerate([distances_conf, distances_mahal]):\n",
    "        ovr_labels = y_test != class_to_test\n",
    "        average=\"weighted\" #average = \"macro\"\n",
    "        roc_auc = sklearn.metrics.roc_auc_score(ovr_labels, distances, average=average)\n",
    "        pr_auc = sklearn.metrics.average_precision_score(ovr_labels, distances, average=average)\n",
    "        aucs[idx_conf_mahal, 0] = roc_auc\n",
    "        aucs[idx_conf_mahal, 1] = pr_auc\n",
    "    \n",
    "    return aucs\n",
    "\n",
    "\n",
    "def run_all_kernels(X_train:List[np.ndarray], \n",
    "                    y_train:np.array, \n",
    "                    X_test:List[np.ndarray], \n",
    "                    y_test:np.array, \n",
    "                    unique_labels:np.array, \n",
    "                    kernel_names:List[str],\n",
    "                    variable_length:bool,\n",
    "                    verbose:bool = False,\n",
    "                    ):\n",
    "    kernel_results = {}\n",
    "    for kernel_name in kernel_names:\n",
    "        # 2 methods (conf, mahal), 2 metrics (roc_auc, pr_auc), C classes\n",
    "        aucs = np.zeros( (2, 2, len(unique_labels)) ) \n",
    "        for i, label in enumerate(unique_labels):\n",
    "            # Get all samples of the current class\n",
    "            idxs = np.where(y_train == label)[0]\n",
    "            corpus = [X_train[k] for k in idxs]\n",
    "            if not variable_length:\n",
    "                corpus, X_test = normalize_streams(np.array(corpus), X_test)\n",
    "\n",
    "            #run model\n",
    "            scores = run_single_kernel_single_label(corpus, X_test, y_test,\n",
    "                                    label, kernel_name, variable_length, \n",
    "                                    verbose=verbose)\n",
    "            aucs[:,:, i] = scores\n",
    "        \n",
    "        #update kernel results\n",
    "        kernel_results[kernel_name] = aucs\n",
    "    return kernel_results\n",
    "\n",
    "\n",
    "def run_tslearn_experiments(dataset_names:List[str], \n",
    "                            kernel_names:List[str],\n",
    "                            verbose:bool=False,\n",
    "                            ):\n",
    "    \"\"\"Runs a series of time series anomaly detection experiments on the specified \n",
    "    tslearn datasets using kernel conformance scores.\"\"\"\n",
    "    experiments = {}\n",
    "    for dataset_name in dataset_names:\n",
    "        # Load dataset\n",
    "        X_train, y_train, X_test, y_test = UCR_UEA_datasets().load_dataset(dataset_name)\n",
    "\n",
    "        # stats\n",
    "        unique_labels = np.unique(y_train)\n",
    "        num_classes = len(unique_labels)\n",
    "        N_train, T, d = X_train.shape\n",
    "        N_test, _, _  = X_test.shape\n",
    "        print_dataset_stats(num_classes, d, T, N_train, N_test)\n",
    "\n",
    "        # Run each kernel\n",
    "        kernel_results = run_all_kernels(X_train, y_train, X_test, y_test, \n",
    "                                         unique_labels, kernel_names,\n",
    "                                         variable_length=False,\n",
    "                                         verbose=verbose)\n",
    "        \n",
    "        #log dataset experiment\n",
    "        experiments[dataset_name] = {\"results\": kernel_results, \n",
    "                                     \"num_classes\": num_classes, \n",
    "                                     \"dim\":d,\n",
    "                                     \"ts_length\":T, \n",
    "                                     \"N_train\":N_train, \n",
    "                                     \"N_test\":N_test}\n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes: 15\n",
      "Dimension of path: 2\n",
      "Length: 45\n",
      "Train: 180\n",
      "Test: 180\n",
      "\n",
      "Start Dataset {dataset_name} results: Libras\n",
      "Number of Classes: 15\n",
      "Dimension of path: 2\n",
      "Length: 45\n",
      "Train: 180\n",
      "Test: 180\n",
      "\n",
      "Kernel: integral linear\n",
      "Conformance AUC: 0.44114\n",
      "Mahalanobis AUC: 0.44104\n",
      "Conformance PR AUC: 0.92761\n",
      "Mahalanobis PR AUC: 0.92762\n",
      "\n",
      "End Dataset {dataset_name} results\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#run experiments\n",
    "\n",
    "experiments = run_tslearn_experiments(\n",
    "    dataset_names = [\n",
    "        #'ArticularyWordRecognition', \n",
    "        #'BasicMotions', \n",
    "        #'Cricket',\n",
    "         ##########'ERing', #cant find dataset\n",
    "        'Libras', \n",
    "        #'NATOPS', \n",
    "        #'RacketSports',     \n",
    "        #'FingerMovements',\n",
    "        #'Heartbeat',\n",
    "        #'SelfRegulationSCP1', \n",
    "        #'UWaveGestureLibrary'\n",
    "        ],\n",
    "    kernel_names = [\n",
    "        #\"linear\",\n",
    "        #\"rbf\",\n",
    "        #\"poly\",\n",
    "        #\"gak\",\n",
    "        #\"truncated sig\",\n",
    "        #\"truncated sig rbf\",\n",
    "        #\"truncated sig poly\",\n",
    "        #\"signature pde\",\n",
    "        #\"signature pde rbf\",\n",
    "        #\"signature pde poly\",\n",
    "        #\"integral linear\",\n",
    "        #\"integral rbf\",\n",
    "        #\"integral poly\",\n",
    "        ]\n",
    "        )\n",
    "\n",
    "\n",
    "def print_experiment_results(experiments, round_digits=5):\n",
    "    for dataset_name, results in experiments.items():\n",
    "        #Dataset:\n",
    "        print(\"\\nStart Dataset {dataset_name} results:\", dataset_name)\n",
    "        print_dataset_stats(results[\"num_classes\"], results[\"dim\"], \n",
    "                            results[\"ts_length\"], results[\"N_train\"], \n",
    "                            results[\"N_test\"])\n",
    "\n",
    "        #Results for each kernel:\n",
    "        for kernel_name, scores in results[\"results\"].items():\n",
    "            print(\"\\nKernel:\", kernel_name)\n",
    "            scores = np.mean(scores, axis=2)\n",
    "            print(\"Conformance AUC:\", round(scores[0, 0], round_digits))\n",
    "            print(\"Mahalanobis AUC:\", round(scores[1, 0], round_digits))\n",
    "            print(\"Conformance PR AUC:\", round(scores[0, 1], round_digits))\n",
    "            print(\"Mahalanobis PR AUC:\", round(scores[1, 1], round_digits))\n",
    "\n",
    "        print(\"\\nEnd Dataset {dataset_name} results\\n\\n\\n\")\n",
    "        \n",
    "print_experiment_results(experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PenDigits dataset (Variable Length) \n",
    "\n",
    "* Can't use ts-learn since it interpolated and homogenized the length of all time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################################\n",
    "## Loading code taken from https://github.com/pafoster/conformance_distance_experiments_cochrane_et_al_2020    ##\n",
    "## DATASET_URLS = ['https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits-orig.tes.Z', ##\n",
    "##                 'https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits-orig.tra.Z'] ##\n",
    "#################################################################################################################\n",
    "\n",
    "def read_pendigits_dataset(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data_lines = f.readlines()\n",
    "\n",
    "    data = []\n",
    "    data_labels = []\n",
    "    current_digit = None\n",
    "    for line in data_lines:\n",
    "        if line == \"\\n\":\n",
    "            continue\n",
    "\n",
    "        if line[0] == \".\":\n",
    "            if \"SEGMENT DIGIT\" in line[1:]:\n",
    "                if current_digit is not None:\n",
    "                    data.append(np.array(current_digit))\n",
    "                    data_labels.append(digit_label)\n",
    "\n",
    "                current_digit = []\n",
    "                digit_label = int(line.split('\"')[1])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            x, y = map(float, line.split())\n",
    "            current_digit.append([x, y])\n",
    "            \n",
    "    data.append(np.array(current_digit))\n",
    "    data_labels.append(digit_label)\n",
    "    return data, np.array(data_labels)\n",
    "\n",
    "\n",
    "def create_pendigits_dataframe(data):\n",
    "    dataframes = []\n",
    "    for subset, data in data.items():\n",
    "        df = pd.DataFrame(data).T\n",
    "        df.columns = ['data', 'label']\n",
    "        df['subset'] = subset\n",
    "        dataframes.append(df)\n",
    "    return pd.concat(dataframes)\n",
    "\n",
    "data = {'train': read_pendigits_dataset(\"Data/pendigits-orig.tra\"),\n",
    "        'test': read_pendigits_dataset(\"Data/pendigits-orig.tes\")}\n",
    "\n",
    "df_pendigits_raw = create_pendigits_dataframe(data)\n",
    "\n",
    "def plot_pendigits_entry(sample):\n",
    "    df = pd.DataFrame(sample[\"data\"], columns=[\"x\", \"y\"])\n",
    "    fig = px.line(df, x=\"x\", y=\"y\", text=df.index, width=500, height=500)\n",
    "    fig.update_traces(textposition=\"bottom right\")\n",
    "    fig.show()\n",
    "\n",
    "plot_pendigits_entry(df_pendigits_raw.iloc[20])\n",
    "print(df_pendigits_raw.head()) \n",
    "# Each data point is a timeseries of the form ([x_t1, y_t1], ... , [x_tn, y_tn]) \n",
    "# of variable length, that is an array of shape (N_i, 2) for each i in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################## |\n",
    "################################### PenDigits experiments #################################### |\n",
    "############################################################################################## \\/\n",
    "\n",
    "def run_pendigits_experiments(df:pd.DataFrame, \n",
    "                              kernel_names:List[str],\n",
    "                              stream_transforms = [\"time_enhance\", \"min_max_normalize\"],):\n",
    "    \"\"\"Calculates AUCs for each kernel on the PenDigits dataset.\n",
    "    df has columns [\"data\", \"label\", \"subset\"]. Each data point \n",
    "    is a timeseries of shape (T_i, d) of variable length.\"\"\"\n",
    "    #transform streams\n",
    "    df[\"data\"] = df[\"data\"].apply(lambda x : transform_stream(x, stream_transforms))\n",
    "\n",
    "    #Gather dataset info\n",
    "    X_train = df[df[\"subset\"]==\"train\"][\"data\"].values\n",
    "    y_train = np.array(df[df[\"subset\"]==\"train\"][\"label\"].values)\n",
    "    X_test = df[df[\"subset\"]==\"test\"][\"data\"].values\n",
    "    y_test = np.array(df[df[\"subset\"]==\"test\"][\"label\"].values)\n",
    "    labels = sorted(df[\"label\"].unique())\n",
    "    num_classes = len(labels)\n",
    "    d = X_train[0].shape[1]\n",
    "    T = \"variable length\"\n",
    "    N_train = len(X_train)\n",
    "    N_test = len(X_test)\n",
    "    print_dataset_stats(num_classes, d, T, N_train, N_test)\n",
    "\n",
    "    # Run each kernel\n",
    "    kernel_results = {}\n",
    "    for kernel_name in kernel_names:\n",
    "        print(kernel_name)\n",
    "        scores = run_single_kernel(X_train, y_train, X_test, y_test, labels, \n",
    "                        kernel_name, variable_length=True, normalize=False,\n",
    "                        trunc_sig_dim_bound=200, SVD_max_rank=None)\n",
    "        kernel_results[kernel_name] = scores\n",
    "\n",
    "    #log results\n",
    "    pendigits_results = {\"results\": kernel_results, \n",
    "                         \"num_classes\": num_classes,\n",
    "                         \"dim\": d,\n",
    "                         \"ts_length\":T, \n",
    "                         \"N_train\":N_train, \n",
    "                         \"N_test\":N_test}\n",
    "    return pendigits_results\n",
    "\n",
    "# pendigits_results = run_pendigits_experiments(\n",
    "#     df_pendigits_raw, \n",
    "#     kernel_names=[\n",
    "#         #\"gak\",\n",
    "#         \"truncated signature\", \n",
    "#         #\"signature pde\", \n",
    "#         #\"signature pde RBF\"\n",
    "#         ],\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: Libras\n",
    "# Number of Classes: 15\n",
    "# Dimension of path: 2\n",
    "# Length: 45\n",
    "# Train: 180\n",
    "# Test: 180\n",
    "\n",
    "# Kernel: linear\n",
    "# Conformance AUC: 0.9471891534391536\n",
    "# Mahalanobis AUC: 0.9460978835978835\n",
    "# Conformance PR AUC: 0.9952856063472626\n",
    "# Mahalanobis PR AUC: 0.9953123143532209\n",
    "\n",
    "# Kernel: rbf\n",
    "# Conformance AUC: 0.6121858465608465\n",
    "# Mahalanobis AUC: 0.235896164021164\n",
    "# Conformance PR AUC: 0.9543784983385764\n",
    "# Mahalanobis PR AUC: 0.9073331474510873\n",
    "\n",
    "# Kernel: gak\n",
    "# Conformance AUC: 0.8303240740740742\n",
    "# Mahalanobis AUC: 0.056779100529100526\n",
    "# Conformance PR AUC: 0.9720553520740843\n",
    "# Mahalanobis PR AUC: 0.8271237492596946\n",
    "\n",
    "# Kernel: truncated signature\n",
    "# Conformance AUC: 0.8772486772486773\n",
    "# Mahalanobis AUC: 0.8711970899470899\n",
    "# Conformance PR AUC: 0.9884586491342986\n",
    "# Mahalanobis PR AUC: 0.9880448383038575\n",
    "\n",
    "# Kernel: signature pde\n",
    "# Conformance AUC: 0.8616071428571429\n",
    "# Mahalanobis AUC: 0.8559854497354498\n",
    "# Conformance PR AUC: 0.9860276339146576\n",
    "# Mahalanobis PR AUC: 0.9856341293618142\n",
    "\n",
    "# Kernel: signature pde RBF\n",
    "# Conformance AUC: 0.48647486772486775\n",
    "# Mahalanobis AUC: 0.5124669312169311\n",
    "# Conformance PR AUC: 0.9148945413486355\n",
    "# Mahalanobis PR AUC: 0.9149847211152167\n",
    "\n",
    "# End Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_experiment_results({\"PenDigits\": pendigits_results})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "order 1\n",
      "dot1 -1.755811675941624\n",
      "dot2 -1.7558116759416191\n",
      "dot3 -1.7558116759416258\n",
      "\n",
      "order 2\n",
      "dot1 1.4030025671165636\n",
      "dot2 1.4030025671165642\n",
      "dot3 110.89060746948743\n",
      "\n",
      "order 3\n",
      "dot1 1.061035219597274\n",
      "dot2 1.0610352195972634\n",
      "dot3 83.69762398102489\n",
      "\n",
      "order 4\n",
      "dot1 1.212371443858032\n",
      "dot2 1.2123714438579665\n",
      "dot3 1443.8494052680496\n",
      "\n",
      "order 5\n",
      "dot1 2.1171672546357194\n",
      "dot2 2.11716725463564\n",
      "dot3 4318.918584525861\n",
      "\n",
      "order 6\n",
      "dot1 1.2958898619566153\n",
      "dot2 1.2958898619565575\n",
      "dot3 6211.33595570119\n",
      "\n",
      "order 7\n",
      "dot1 1.7067618411050665\n",
      "dot2 1.706761841105013\n",
      "dot3 15519.90607393497\n",
      "\n",
      "order 8\n",
      "dot1 1.5164888736274276\n",
      "dot2 1.5164888736273308\n",
      "dot3 17961.066545503454\n",
      "\n",
      "order 9\n",
      "dot1 1.788181977228414\n",
      "dot2 1.7881819772283214\n",
      "dot3 17964.064206142768\n",
      "\n",
      "order 10\n",
      "dot1 2.140755266525466\n",
      "dot2 2.140755266525469\n",
      "dot3 21634.64009985917\n",
      "\n",
      "order 11\n",
      "dot1 2.211273722994132\n",
      "dot2 2.211273722994162\n",
      "dot3 21444.509913591995\n",
      "\n",
      "order 12\n",
      "dot1 2.2948210029800458\n",
      "dot2 2.2948210029800493\n",
      "dot3 21344.041986561395\n",
      "\n",
      "order 13\n",
      "dot1 2.2952324218406037\n",
      "dot2 2.295232421840585\n",
      "dot3 21310.340851525743\n",
      "\n",
      "order 14\n",
      "dot1 2.3100008637742793\n",
      "dot2 2.3100008637742553\n",
      "dot3 21304.374101057052\n",
      "\n",
      "order 15\n",
      "dot1 2.309267724509067\n",
      "dot2 2.30926772450902\n",
      "dot3 21304.50402411165\n",
      "\n",
      "order 16\n",
      "dot1 2.312966259671785\n",
      "dot2 2.312966259671764\n",
      "dot3 21304.576800412688\n",
      "\n",
      "order 17\n",
      "dot1 2.3124858323973534\n",
      "dot2 2.3124858323973867\n",
      "dot3 21304.57892976731\n",
      "\n",
      "order 18\n",
      "dot1 2.312919502589089\n",
      "dot2 2.312919502589118\n",
      "dot3 21304.579063619887\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kernel Gram Matrix: 100%|| 1/1 [00:00<00:00, 117.14it/s]\n",
      "Kernel Gram Matrix: 100%|| 1/1 [00:00<00:00, 44.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot_pde 2.3122997286191893\n",
      "\n",
      "comparison [1.87387387 1.38100209 0.47232704 0.59489456 0.19172842 0.15874269\n",
      " 0.20778146 0.1707777  0.19585871 0.24052829 0.42410606 0.30205229\n",
      " 0.99533344 1.72951591 2.11428968 3.44366661 6.09059463]\n",
      "\n",
      "iisig [2.47955322e-04 3.15427780e-04 1.79052353e-04 3.83377075e-04\n",
      " 1.91211700e-04 2.39610672e-04 4.78744507e-04 5.39779663e-04\n",
      " 9.58442688e-04 1.45459175e-03 3.32546234e-03 6.78992271e-03\n",
      " 2.71043777e-02 5.95753193e-02 8.72416496e-02 1.62200212e-01\n",
      " 3.45914602e-01]\n",
      "\n",
      "sigker [0.00013232 0.0002284  0.00037909 0.00064445 0.0009973  0.00150943\n",
      " 0.00230408 0.00316072 0.00489354 0.00604749 0.00784111 0.0224793\n",
      " 0.02723145 0.03444624 0.04126287 0.04710102 0.05679488]\n",
      "\n",
      "ksig [0.00180602 0.00174117 0.00211787 0.00223088 0.00233984 0.00345516\n",
      " 0.00381732 0.00341773 0.00444031 0.00453067 0.0053246  0.01027536\n",
      " 0.0112586  0.01289487 0.01192331 0.0119617  0.01337743]\n",
      "\n",
      "comparison [0.07326733 0.13117897 0.17899358 0.28887464 0.42622784 0.43686172\n",
      " 0.60358504 0.92479944 1.10207259 1.33478924 1.47261899 2.18768852\n",
      " 2.41872432 2.67131367 3.46068786 3.93765323 4.24557558]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# testing ksig  ---- clearly something wrong, since the values dont converge as order->infty for ksig\n",
    "\n",
    "import time\n",
    "\n",
    "def calc_iisig_kernel(X, Y, order):\n",
    "    sig_X, sig_Y = streams_to_sigs([X,Y], order, disable_tqdm=True)\n",
    "    dot = 1 + np.dot(sig_X, sig_Y)\n",
    "    return dot\n",
    "\n",
    "\n",
    "def calc_sigpde_kernel(X,Y):\n",
    "    dyadic_order = 5\n",
    "    static_kernel = sigkernel.LinearKernel()\n",
    "    vv, uv = case_sig_pde([X], [Y], dyadic_order, static_kernel)\n",
    "    return uv[0,0]\n",
    "\n",
    "\n",
    "def calc_ksig_kernel(X,Y, order):\n",
    "    import ksig\n",
    "    static_kernel = ksig.static.kernels.LinearKernel() \n",
    "    sig_kernel = ksig.kernels.SignatureKernel(n_levels=order, order=1, static_kernel=static_kernel, normalize=False)\n",
    "    dot = sig_kernel(np.array([X,X]), np.array([Y,Y]))[0,0]\n",
    "    return dot\n",
    "\n",
    "\n",
    "def trunc_sig_kernel(s1:np.ndarray, \n",
    "                    s2:np.ndarray, \n",
    "                    order:int, #order is truncation level of the signature\n",
    "                    static_kernel_gram:Callable = linear_kernel_gram,\n",
    "                    only_last:bool = True,\n",
    "\n",
    "                    ):\n",
    "    \"\"\"s1 and s2 are time series of shape (T_i, d)\"\"\"\n",
    "    K = static_kernel_gram(s1, s2)\n",
    "    nabla = K[1:, 1:] + K[:-1, :-1] - K[1:, :-1] - K[:-1, 1:]\n",
    "    sig_kers = jitted_trunc_sig_kernel(nabla, order)\n",
    "    if only_last:\n",
    "        return sig_kers[-1]\n",
    "    else:\n",
    "        return sig_kers\n",
    "\n",
    "\n",
    "\n",
    "@njit\n",
    "def reverse_cumsum(arr:np.ndarray, axis:int): #ndim=2\n",
    "    \"\"\"JITed reverse cumulative sum along the specified axis.\n",
    "    (np.cumsum with axis is not natively supported by Numba)\"\"\"\n",
    "    A = arr.copy()\n",
    "    if axis==0:\n",
    "        for i in np.arange(A.shape[0]-2, -1, -1):\n",
    "            A[i, :] += A[i+1, :]\n",
    "    else: #axis==1\n",
    "        for i in np.arange(A.shape[1]-2, -1, -1):\n",
    "            A[:,i] += A[:,i+1]\n",
    "    return A\n",
    "\n",
    "\n",
    "@njit\n",
    "def jitted_trunc_sig_kernel(nabla:np.ndarray, # gram matrix (T_1, T_2)\n",
    "                            order:int,\n",
    "                            ):\n",
    "    \"\"\"Given difference matrix nabla_ij = K[i+1, j+1] + K[i, j] - K[i+1, j] - K[i, j+1],\n",
    "    computes the truncated signature kernel of all orders up to 'order'.\"\"\"\n",
    "    B = np.ones((order+1, order+1, order+1, *nabla.shape))\n",
    "    for d in np.arange(order):\n",
    "        for n in np.arange(order-d):\n",
    "            for m in np.arange(order-d):\n",
    "                B[d+1,n,m] = 1 + nabla/(n+1)/(m+1)*B[d, n+1, m+1]\n",
    "                r1 = reverse_cumsum(nabla * B[d, n+1, 1] / (n+1), axis=0)\n",
    "                B[d+1,n,m, :-1, :] += r1[1:, :]\n",
    "                r2 = reverse_cumsum(nabla * B[d, 1, m+1] / (m+1), axis=1)\n",
    "                B[d+1,n,m, :, :-1] += r2[:, 1:]\n",
    "                rr = reverse_cumsum(nabla * B[d, 1, 1], axis=0)\n",
    "                rr = reverse_cumsum(rr, axis=1)\n",
    "                B[d+1,n,m, :-1, :-1] += rr[1:, 1:]\n",
    "\n",
    "    return B[:,0,0,0,0]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "d = 2\n",
    "MAX_ORDER = 18\n",
    "times_iisig = np.zeros( (MAX_ORDER) )\n",
    "times_sigker  = np.zeros( (MAX_ORDER) )\n",
    "times_ksig = np.zeros( (MAX_ORDER) )\n",
    "np.random.seed(99)\n",
    "X, Y = np.random.randn(2, 19, d)/np.sqrt(d)\n",
    "for order in range(1, MAX_ORDER+1):\n",
    "    print(\"\\norder\", order)\n",
    "    t0= time.time()\n",
    "    dot1=calc_iisig_kernel(X, Y, order)\n",
    "    t1 = time.time()\n",
    "    dot2=trunc_sig_kernel(X, Y, order)\n",
    "    t2 = time.time()\n",
    "    dot3=calc_ksig_kernel(X, Y, order)\n",
    "    t3 = time.time()\n",
    "    times_iisig[order-1] = t1-t0\n",
    "    times_sigker[order-1] = t2-t1\n",
    "    times_ksig[order-1] = t3-t2\n",
    "    print(\"dot1\", dot1)\n",
    "    print(\"dot2\", dot2)\n",
    "    print(\"dot3\", dot3)\n",
    "\n",
    "print(\"\\n\")\n",
    "dot4 = calc_sigpde_kernel(X, Y)\n",
    "print(\"dot_pde\", dot4)\n",
    "\n",
    "\n",
    "print(\"\\ncomparison\", times_iisig[1:]/times_sigker[1:])\n",
    "print(\"\\niisig\", times_iisig[1:])\n",
    "print(\"\\nsigker\", times_sigker[1:])\n",
    "print(\"\\nksig\", times_ksig[1:])\n",
    "print(\"\\ncomparison\", times_sigker[1:]/times_ksig[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tslearn\n",
    "\n",
    "_datasets = [\n",
    "            'ArticularyWordRecognition', \n",
    "            'BasicMotions', \n",
    "            'Cricket',\n",
    "            #'ERing',\n",
    "            'Libras', \n",
    "            'NATOPS', \n",
    "            'RacketSports',     \n",
    "            'FingerMovements',\n",
    "            'Heartbeat',\n",
    "            'SelfRegulationSCP1', \n",
    "            'UWaveGestureLibrary'\n",
    "            ]\n",
    "\n",
    "\n",
    "#for dataset_name in ucr_datasets.list_multivariate_datasets():\n",
    "for dataset_name in _datasets:\n",
    "    print(\"Dataset:\", dataset_name)\n",
    "    dataset = tslearn.datasets.UCR_UEA_datasets().load_dataset(dataset_name)\n",
    "    if dataset[0] is not None:\n",
    "        X_train, y_train, X_test, y_test = dataset\n",
    "        num_classes = len(np.unique(y_train))\n",
    "        N_train, T, d = X_train.shape\n",
    "        N_test, _, _  = X_test.shape\n",
    "\n",
    "        # is_irregular = dataset[\"is_irregular\"]\n",
    "        \n",
    "        print(\"Number of Classes:\", num_classes)\n",
    "        print(\"Dimension of path:\", d)\n",
    "        print(\"Length:\", T)\n",
    "        print(\"Train Size, Test Size\", N_train, N_test)\n",
    "        print()\n",
    "    else:\n",
    "        print(\"No dataset found\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: ArticularyWordRecognition\n",
    "# Number of Classes: 25\n",
    "# Dimension of path: 9\n",
    "# Length: 144\n",
    "# Train Size, Test Size 275 300\n",
    "\n",
    "# Dataset: BasicMotions\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 6\n",
    "# Length: 100\n",
    "# Train Size, Test Size 40 40\n",
    "\n",
    "# Dataset: Cricket\n",
    "# Number of Classes: 12\n",
    "# Dimension of path: 6\n",
    "# Length: 1197\n",
    "# Train Size, Test Size 108 72\n",
    "\n",
    "# Dataset: Libras\n",
    "# Number of Classes: 15\n",
    "# Dimension of path: 2\n",
    "# Length: 45\n",
    "# Train Size, Test Size 180 180\n",
    "\n",
    "# Dataset: NATOPS\n",
    "# Number of Classes: 6\n",
    "# Dimension of path: 24\n",
    "# Length: 51\n",
    "# Train Size, Test Size 180 180\n",
    "\n",
    "# Dataset: RacketSports\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 6\n",
    "# Length: 30\n",
    "# Train Size, Test Size 151 152\n",
    "\n",
    "# Dataset: FingerMovements\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 28\n",
    "# Length: 50\n",
    "# Train Size, Test Size 316 100\n",
    "\n",
    "# Dataset: Heartbeat\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 61\n",
    "# Length: 405\n",
    "# Train Size, Test Size 204 205\n",
    "\n",
    "# Dataset: SelfRegulationSCP1\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 6\n",
    "# Length: 896\n",
    "# Train Size, Test Size 268 293\n",
    "\n",
    "# Dataset: UWaveGestureLibrary\n",
    "# Number of Classes: 8\n",
    "# Dimension of path: 3\n",
    "# Length: 315\n",
    "# Train Size, Test Size 120 320"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation code (for anomaly detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sigma': array([0.00673795, 0.02351775, 0.082085  , 0.2865048 , 1.        ])}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 126\u001b[0m\n\u001b[1;32m    120\u001b[0m     unique_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_train)\n\u001b[1;32m    122\u001b[0m     cross_validation(X_train, \n\u001b[1;32m    123\u001b[0m                      y_train, \n\u001b[1;32m    124\u001b[0m                      unique_labels, \n\u001b[1;32m    125\u001b[0m                      [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 126\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[109], line 122\u001b[0m, in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m X_train, y_train, X_test, y_test \u001b[38;5;241m=\u001b[39m UCR_UEA_datasets()\u001b[38;5;241m.\u001b[39mload_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLibras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    120\u001b[0m unique_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_train)\n\u001b[0;32m--> 122\u001b[0m \u001b[43mcross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m                 \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m                 \u001b[49m\u001b[43munique_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m                 \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrbf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[109], line 109\u001b[0m, in \u001b[0;36mcross_validation\u001b[0;34m(X, y, unique_labels, kernel_names)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(param_ranges)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m#problem: different folds can be of different sizes.\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def balanced_k_folds(X:List,    #dataset\n",
    "                    y:np.array, #class labels\n",
    "                    k:int = 5):\n",
    "    \"\"\"Generates balanced k-folds for cross-validation, where each fold\n",
    "    is balanced the same as the original dataset.\"\"\"\n",
    "    #is X numpy array?\n",
    "    is_numpy=True if isinstance(X, np.ndarray) else False\n",
    "\n",
    "    #shuffle data\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    X = [X[i] for i in indices]\n",
    "    y = np.array([y[i] for i in indices])\n",
    "    unique_labels = np.unique(y)\n",
    "\n",
    "    #split into classes\n",
    "    classwise = {label:[] for label in unique_labels}\n",
    "    for x, label in zip(X, y):\n",
    "        classwise[label].append(x)\n",
    "\n",
    "    #split into k-folds\n",
    "    classwise_folds = {}\n",
    "    for label, dataclass in classwise.items():\n",
    "        classwise_folds[label] = np.array_split(dataclass, k)\n",
    "    \n",
    "    #create folds\n",
    "    folds=[]\n",
    "    for i in range(k):\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        X_val = []\n",
    "        y_val = []\n",
    "        for label, dataclass in classwise_folds.items():\n",
    "            for j in range(k):\n",
    "                if j!=i:\n",
    "                    X_train.extend(dataclass[j])\n",
    "                    y_train.extend([label]*len(dataclass[j]))\n",
    "                else:\n",
    "                    X_val.extend(dataclass[j])\n",
    "                    y_val.extend([label]*len(dataclass[j]))\n",
    "\n",
    "        #convert to numpy if possible\n",
    "        if is_numpy:\n",
    "            X_train = np.array(X_train)\n",
    "            X_val = np.array(X_val)\n",
    "        y_train = np.array(y_train)\n",
    "        y_val = np.array(y_val)\n",
    "        folds.append([X_train, y_train, X_val, y_val])\n",
    "\n",
    "    return folds\n",
    "\n",
    "\n",
    "\n",
    "def get_hyperparam_ranges(kernel_name:str, \n",
    "                          leave_out_sig_order:bool=True):\n",
    "    num_params = 5\n",
    "    sigma_grid = np.exp(np.linspace(-5, 0, num_params))\n",
    "    p_grid = np.array([1.5, 2.0, 2.5, 3.0])\n",
    "    ranges = {}\n",
    "    if \"rbf\" in kernel_name:\n",
    "        ranges[\"sigma\"] = sigma_grid\n",
    "    elif \"poly\" in kernel_name:\n",
    "        ranges[\"p\"] = p_grid\n",
    "    \n",
    "    if not leave_out_sig_order:\n",
    "        if \"truncated sig\" in kernel_name:\n",
    "            ranges[\"order\"] = np.arange(2, 15)\n",
    "    return ranges\n",
    "    \n",
    "\n",
    "\n",
    "def test_model_params_CV(kernel_name, \n",
    "                         label, \n",
    "                         X_train, \n",
    "                         y_train, \n",
    "                         X_val, \n",
    "                         y_val, \n",
    "                         ):\n",
    "    \"\"\"\"Kernel_name specifies the model. We work on a single fold, and do\n",
    "    anomaly detection using 'label' as the normal class. We then calculate\n",
    "    the AUC scores for each hyperparameter.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def cross_validation(X:List,                #Training Dataset\n",
    "                     y:np.array,            #Training class labels\n",
    "                     unique_labels:np.array, #Unique class labels\n",
    "                     kernel_names:List[str],\n",
    "                     ):\n",
    "    model_params_CV = {}\n",
    "    # anomaly detection model is specified by a pair (class label, kernel_name)\n",
    "    # model_params_CV[pair] = CV_scores\n",
    "    folds = balanced_k_folds(X, y, k=5)\n",
    "    for kernel_name in kernel_names:\n",
    "        for label in unique_labels:\n",
    "            #calc minimum size of the label class in folds\n",
    "            min_fold_size = min([len(np.where(y_train==label)[0]) \n",
    "                                 for (_, y_train, _, _) in folds])\n",
    "            \n",
    "            #loop over folds\n",
    "            for fold in folds:\n",
    "                X_train, y_train, X_val, y_val = fold\n",
    "                param_ranges = get_hyperparam_ranges(kernel_name)\n",
    "                print(param_ranges)\n",
    "                \n",
    "                #problem: different folds can be of different sizes.\n",
    "\n",
    "                assert False\n",
    "                #model_params_CV[(label, kernel_name)] = []\n",
    "                # each model has its own set of hyperparameters\n",
    "\n",
    "\n",
    "                # take max of mahal and conf in same category (PR vs PR etc.)\n",
    "                # We are after the best model after all.\n",
    "\n",
    "                for param in param_ranges:\n",
    "                    # fit model normally for most models\n",
    "                # if model is truncated sig, we only need to calculate the Gram matrices once.\n",
    "\n",
    "                # TODO modify run_single_kernel_single_label() to return\n",
    "                # multiple AUC scores for each threshold. \n",
    "                # use variable 'min_fold_size' later.\n",
    "\n",
    "def test():\n",
    "    X_train, y_train, X_test, y_test = UCR_UEA_datasets().load_dataset(\"Libras\")\n",
    "    unique_labels = np.unique(y_train)\n",
    "\n",
    "    cross_validation(X_train, \n",
    "                     y_train, \n",
    "                     unique_labels, \n",
    "                     [\"rbf\"])\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([1, 2, 2, 3, 3, 3, 4, 4, 4, 4])\n",
    "label = 3\n",
    "\n",
    "np.where(y_train==label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
