{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "from typing import List, Optional, Dict, Set, Callable, Any\n",
    "from joblib import Memory, Parallel, delayed\n",
    "import tslearn\n",
    "import tslearn.metrics\n",
    "from tslearn.datasets import UCR_UEA_datasets\n",
    "from scipy.interpolate import interp1d\n",
    "from numba import njit\n",
    "import pickle\n",
    "\n",
    "from experiment_code import print_dataset_stats, run_all_kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all tslearn datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _datasets = [\n",
    "#             'ArticularyWordRecognition', \n",
    "#             'BasicMotions', \n",
    "#             'Cricket',\n",
    "#             #'ERing',\n",
    "#             'Libras', \n",
    "#             'NATOPS', \n",
    "#             'RacketSports',     \n",
    "#             'FingerMovements',\n",
    "#             'Heartbeat',\n",
    "#             'SelfRegulationSCP1', \n",
    "#             'UWaveGestureLibrary'\n",
    "#             ]\n",
    "\n",
    "# import tslearn\n",
    "# UCR_UEA_datasets = tslearn.datasets.UCR_UEA_datasets()\n",
    "\n",
    "# for dataset_name in UCR_UEA_datasets.list_multivariate_datasets():\n",
    "# #for dataset_name in _datasets:\n",
    "#     print(\"Dataset:\", dataset_name)\n",
    "#     dataset = UCR_UEA_datasets.load_dataset(dataset_name)\n",
    "#     if dataset[0] is not None:\n",
    "#         X_train, y_train, X_test, y_test = dataset\n",
    "#         num_classes = len(np.unique(y_train))\n",
    "#         N_train, T, d = X_train.shape\n",
    "#         N_test, _, _  = X_test.shape\n",
    "        \n",
    "#         print(\"Number of Classes:\", num_classes)\n",
    "#         print(\"Dimension of path:\", d)\n",
    "#         print(\"Length:\", T)\n",
    "#         print(\"Train Size, Test Size\", N_train, N_test)\n",
    "#         print()\n",
    "#     else:\n",
    "#         print(\"No dataset found\")\n",
    "#         print()\n",
    "\n",
    "#yes\n",
    "# Dataset: ArticularyWordRecognition\n",
    "# Number of Classes: 25\n",
    "# Dimension of path: 9\n",
    "# Length: 144\n",
    "# Train Size, Test Size 275 300\n",
    "\n",
    "# Dataset: AtrialFibrillation\n",
    "# No dataset found\n",
    "\n",
    "#yes\n",
    "# Dataset: BasicMotions\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 6\n",
    "# Length: 100\n",
    "# Train Size, Test Size 40 40\n",
    "\n",
    "# Dataset: CharacterTrajectories\n",
    "# No dataset found\n",
    "\n",
    "#yes\n",
    "# Dataset: Cricket\n",
    "# Number of Classes: 12\n",
    "# Dimension of path: 6\n",
    "# Length: 1197\n",
    "# Train Size, Test Size 108 72\n",
    "\n",
    "# Dataset: DuckDuckGeese\n",
    "# No dataset found\n",
    "\n",
    "# Dataset: EigenWorms\n",
    "# Number of Classes: 5\n",
    "# Dimension of path: 6\n",
    "# Length: 17984\n",
    "# Train Size, Test Size 128 131\n",
    "\n",
    "#why not\n",
    "# Dataset: Epilepsy\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 3\n",
    "# Length: 206\n",
    "# Train Size, Test Size 137 138\n",
    "\n",
    "#longLength\n",
    "# Dataset: EthanolConcentration\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 3\n",
    "# Length: 1751\n",
    "# Train Size, Test Size 261 263\n",
    "\n",
    "# Dataset: ERing\n",
    "# No dataset found\n",
    "\n",
    "#big\n",
    "# Dataset: FaceDetection\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 144\n",
    "# Length: 62\n",
    "# Train Size, Test Size 5890 3524\n",
    "\n",
    "#yes\n",
    "# Dataset: FingerMovements\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 28\n",
    "# Length: 50\n",
    "# Train Size, Test Size 316 100\n",
    "\n",
    "#why not, maybe big length\n",
    "# Dataset: HandMovementDirection\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 10\n",
    "# Length: 400\n",
    "# Train Size, Test Size 160 74\n",
    "\n",
    "#smallTrain\n",
    "# Dataset: Handwriting\n",
    "# Number of Classes: 26\n",
    "# Dimension of path: 3\n",
    "# Length: 152\n",
    "# Train Size, Test Size 150 850\n",
    "\n",
    "#yes\n",
    "# Dataset: Heartbeat\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 61\n",
    "# Length: 405\n",
    "# Train Size, Test Size 204 205\n",
    "\n",
    "#big\n",
    "# Dataset: InsectWingbeat\n",
    "# Number of Classes: 10\n",
    "# Dimension of path: 200\n",
    "# Length: 22\n",
    "# Train Size, Test Size 25000 25000\n",
    "\n",
    "# Dataset: JapaneseVowels\n",
    "# No dataset found\n",
    "\n",
    "#yes\n",
    "# Dataset: Libras\n",
    "# Number of Classes: 15\n",
    "# Dimension of path: 2\n",
    "# Length: 45\n",
    "# Train Size, Test Size 180 180\n",
    "\n",
    "#TODO I SHOULD INCLUDE\n",
    "# Dataset: LSST\n",
    "# Number of Classes: 14\n",
    "# Dimension of path: 6\n",
    "# Length: 36\n",
    "# Train Size, Test Size 2459 2466\n",
    "\n",
    "#length\n",
    "# Dataset: MotorImagery\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 64\n",
    "# Length: 3000\n",
    "# Train Size, Test Size 278 100\n",
    "\n",
    "#yes\n",
    "# Dataset: NATOPS\n",
    "# Number of Classes: 6\n",
    "# Dimension of path: 24\n",
    "# Length: 51\n",
    "# Train Size, Test Size 180 180\n",
    "\n",
    "#TODO NOT TSLEARN. LENGTH WRONG\n",
    "# Dataset: PenDigits\n",
    "# Number of Classes: 10\n",
    "# Dimension of path: 2\n",
    "# Length: 8\n",
    "# Train Size, Test Size 7494 3498\n",
    "\n",
    "#TODO SHOULD INCLUDE highDim\n",
    "# Dataset: PEMS-SF\n",
    "# Number of Classes: 7\n",
    "# Dimension of path: 963\n",
    "# Length: 144\n",
    "# Train Size, Test Size 267 173\n",
    "\n",
    "#dim=1, big length, large num classes\n",
    "# Dataset: Phoneme\n",
    "# Number of Classes: 39\n",
    "# Dimension of path: 1\n",
    "# Length: 1024\n",
    "# Train Size, Test Size 214 1896\n",
    "\n",
    "#yes\n",
    "# Dataset: RacketSports\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 6\n",
    "# Length: 30\n",
    "# Train Size, Test Size 151 152\n",
    "\n",
    "#yes\n",
    "# Dataset: SelfRegulationSCP1\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 6\n",
    "# Length: 896\n",
    "# Train Size, Test Size 268 293\n",
    "\n",
    "# Dataset: SelfRegulationSCP2\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 7\n",
    "# Length: 1152\n",
    "# Train Size, Test Size 200 180\n",
    "\n",
    "# Dataset: SpokenArabicDigits\n",
    "# No dataset found\n",
    "\n",
    "#long, also very small set\n",
    "# Dataset: StandWalkJump\n",
    "# Number of Classes: 3\n",
    "# Dimension of path: 4\n",
    "# Length: 2500\n",
    "# Train Size, Test Size 12 15\n",
    "\n",
    "#yes\n",
    "# Dataset: UWaveGestureLibrary\n",
    "# Number of Classes: 8\n",
    "# Dimension of path: 3\n",
    "# Length: 315\n",
    "# Train Size, Test Size 120 320\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (tslearn) Cross Validation on Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.cross_validation import cv_tslearn\n",
    "\n",
    "cv_best_models = cv_tslearn(\n",
    "    dataset_names = [\n",
    "        #'ArticularyWordRecognition', \n",
    "        #'BasicMotions', \n",
    "        #'Cricket',\n",
    "         ##########'ERing', #cant find dataset\n",
    "        'Libras', \n",
    "        #'NATOPS', \n",
    "        #'RacketSports',     \n",
    "        #'FingerMovements',\n",
    "        #'Heartbeat',\n",
    "        #'SelfRegulationSCP1', \n",
    "        #'UWaveGestureLibrary'\n",
    "        ],\n",
    "    kernel_names = [\n",
    "        \"linear\",\n",
    "        \"rbf\",\n",
    "        #\"poly\",\n",
    "        #\"gak\",\n",
    "        #\"truncated sig\",\n",
    "        #\"truncated sig rbf\",\n",
    "        #\"truncated sig poly\",\n",
    "        #\"signature pde\",\n",
    "        #\"signature pde rbf\",\n",
    "        #\"signature pde poly\",\n",
    "        #\"integral linear\",\n",
    "        #\"integral rbf\",\n",
    "        #\"integral poly\",\n",
    "        ],\n",
    "        k=4,\n",
    "        n_repeats=1,\n",
    "        n_jobs_repeats=1,\n",
    "        n_jobs_gram=4,\n",
    "        verbose=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print CV results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_labels(labelwise_dict:Dict[str, Dict[str, Any]],\n",
    "                          field:str):\n",
    "    \"\"\"Averages the values of a field over the labels.\"\"\"\n",
    "    return np.mean([param_dict[field] for param_dict in labelwise_dict.values()],\n",
    "                   axis=0)\n",
    "\n",
    "\n",
    "def print_cv_tslearn_results(\n",
    "        dataset_kernel_label_paramdict : Dict[str, Dict[str, Dict[str, Any]]],\n",
    "        ):\n",
    "\n",
    "    # return experiments\n",
    "    for dataset_name, results in dataset_kernel_label_paramdict.items():\n",
    "        print(dataset_name)\n",
    "        kernelwise_dict = results[\"kernel_results\"]\n",
    "        n_classes = results['num_classes']\n",
    "        ts_length = results['ts_length']\n",
    "        n_train = results['N_train']\n",
    "        path_dim = results['path dim']\n",
    "        from experiment_code import print_dataset_stats\n",
    "        print_dataset_stats(n_classes, path_dim, ts_length, n_train, \"unknown\")\n",
    "        for kernel_name, labelwise_dict in kernelwise_dict.items():\n",
    "            final_auc_avgs = average_labels(labelwise_dict, \"CV_train_auc\")\n",
    "            params_auc_avgs = average_labels(labelwise_dict, \"auc_params\")\n",
    "            thresh_auc_avgs = average_labels(labelwise_dict, \"auc_thresh\")\n",
    "            print(f\"\\n{kernel_name}\")\n",
    "            print(\"final_auc_avgs\", final_auc_avgs)\n",
    "            print(\"params_auc_avgs\", params_auc_avgs)\n",
    "            print(\"thresh_auc_avgs\", thresh_auc_avgs)\n",
    "            if \"truncated sig\" in kernel_name:\n",
    "                trunc_auc_avgs = average_labels(labelwise_dict, \"auc_truncs\")\n",
    "                print(\"trunc_auc_avgs\", trunc_auc_avgs)\n",
    "        print(\"\\nEnd dataset \\n\\n\\n\")\n",
    "\n",
    "print_cv_tslearn_results(cv_best_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (tslearn) Validate on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_tslearn(\n",
    "        dataset_kernel_label_paramdict : Dict[str, Dict[str, Dict[str, Any]]],\n",
    "        n_jobs:int = 1, \n",
    "        verbose:bool=True,\n",
    "        ):\n",
    "    \"\"\"Validates the best models from cross validation on the\n",
    "    tslearn datasets using kernel conformance scores.\"\"\"\n",
    "    experiments = {}\n",
    "    for dataset_name, results in dataset_kernel_label_paramdict.items():\n",
    "\n",
    "        # Load dataset\n",
    "        print(dataset_name)\n",
    "        X_train, y_train, X_test, y_test = UCR_UEA_datasets().load_dataset(dataset_name)\n",
    "        unique_labels = np.unique(y_train)\n",
    "        num_classes = len(unique_labels)\n",
    "        N_train, T, d = X_train.shape\n",
    "        N_test, _, _  = X_test.shape\n",
    "        print_dataset_stats(num_classes, d, T, N_train, N_test)\n",
    "\n",
    "        #validate on test set\n",
    "        kernelwise_dict = results[\"kernel_results\"]\n",
    "        kernel_results = run_all_kernels(X_train, y_train, X_test, y_test, \n",
    "                            unique_labels, kernelwise_dict, fixed_length=True, \n",
    "                            n_jobs=n_jobs, verbose=verbose)\n",
    "        experiments[dataset_name] = {\"results\": kernel_results, \n",
    "                                     \"num_classes\": num_classes, \n",
    "                                     \"path dim\":d,\n",
    "                                     \"ts_length\":T, \n",
    "                                     \"N_train\":N_train, \n",
    "                                     \"N_test\":N_test}\n",
    "    return experiments\n",
    "\n",
    "test_results_tslearn = validate_tslearn(cv_best_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_experiment_results(experiments, round_digits=5):\n",
    "    for dataset_name, results in experiments.items():\n",
    "        #Dataset:\n",
    "        print(\"Dataset:\", dataset_name)\n",
    "        print_dataset_stats(results[\"num_classes\"], results[\"path dim\"], \n",
    "                            results[\"ts_length\"], results[\"N_train\"], \n",
    "                            results[\"N_test\"])\n",
    "\n",
    "        #Results for each kernel:\n",
    "        for kernel_name, scores in results[\"results\"].items():\n",
    "            print(\"\\nKernel:\", kernel_name)\n",
    "            print(\"Conformance AUC:\", round(scores[0, 0], round_digits))\n",
    "            print(\"Mahalanobis AUC:\", round(scores[1, 0], round_digits))\n",
    "            print(\"Conformance PR AUC:\", round(scores[0, 1], round_digits))\n",
    "            print(\"Mahalanobis PR AUC:\", round(scores[1, 1], round_digits))\n",
    "\n",
    "        print(\"\\nEnd Dataset\\n\\n\\n\")\n",
    "\n",
    "\n",
    "print_experiment_results(test_results_tslearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read CV data from file and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
