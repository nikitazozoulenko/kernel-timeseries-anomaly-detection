{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Optional, Dict, Set, Callable, Any\n",
    "from joblib import Memory, Parallel, delayed\n",
    "import tslearn\n",
    "import tslearn.metrics\n",
    "from tslearn.datasets import UCR_UEA_datasets\n",
    "\n",
    "from cross_validation import cv_tslearn, print_cv_results\n",
    "from eval_on_test import validate_tslearn, print_test_results\n",
    "from utils import load_from_pickle, save_to_pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enumerate all tslearn datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _datasets = [\n",
    "#             'ArticularyWordRecognition', \n",
    "#             'BasicMotions', \n",
    "#             'Cricket',\n",
    "#             #'ERing',\n",
    "#             'Libras', \n",
    "#             'NATOPS', \n",
    "#             'RacketSports',     \n",
    "#             'FingerMovements',\n",
    "#             'Heartbeat',\n",
    "#             'SelfRegulationSCP1', \n",
    "#             'UWaveGestureLibrary'\n",
    "#             ]\n",
    "\n",
    "# import tslearn\n",
    "# UCR_UEA_datasets = tslearn.datasets.UCR_UEA_datasets()\n",
    "\n",
    "# for dataset_name in UCR_UEA_datasets.list_multivariate_datasets():\n",
    "# #for dataset_name in _datasets:\n",
    "#     print(\"Dataset:\", dataset_name)\n",
    "#     dataset = UCR_UEA_datasets.load_dataset(dataset_name)\n",
    "#     if dataset[0] is not None:\n",
    "#         X_train, y_train, X_test, y_test = dataset\n",
    "#         num_classes = len(np.unique(y_train))\n",
    "#         N_train, T, d = X_train.shape\n",
    "#         N_test, _, _  = X_test.shape\n",
    "        \n",
    "#         print(\"Number of Classes:\", num_classes)\n",
    "#         print(\"Dimension of path:\", d)\n",
    "#         print(\"Length:\", T)\n",
    "#         print(\"Train Size, Test Size\", N_train, N_test)\n",
    "#         print()\n",
    "#     else:\n",
    "#         print(\"No dataset found\")\n",
    "#         print()\n",
    "\n",
    "#yes\n",
    "# Dataset: ArticularyWordRecognition\n",
    "# Number of Classes: 25\n",
    "# Dimension of path: 9\n",
    "# Length: 144\n",
    "# Train Size, Test Size 275 300\n",
    "\n",
    "# Dataset: AtrialFibrillation\n",
    "# No dataset found\n",
    "\n",
    "#yes\n",
    "# Dataset: BasicMotions\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 6\n",
    "# Length: 100\n",
    "# Train Size, Test Size 40 40\n",
    "\n",
    "# Dataset: CharacterTrajectories\n",
    "# No dataset found\n",
    "\n",
    "#yes\n",
    "# Dataset: Cricket\n",
    "# Number of Classes: 12\n",
    "# Dimension of path: 6\n",
    "# Length: 1197\n",
    "# Train Size, Test Size 108 72\n",
    "\n",
    "# Dataset: DuckDuckGeese\n",
    "# No dataset found\n",
    "\n",
    "# Dataset: EigenWorms\n",
    "# Number of Classes: 5\n",
    "# Dimension of path: 6\n",
    "# Length: 17984\n",
    "# Train Size, Test Size 128 131\n",
    "\n",
    "#why not\n",
    "# Dataset: Epilepsy\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 3\n",
    "# Length: 206\n",
    "# Train Size, Test Size 137 138\n",
    "\n",
    "#longLength\n",
    "# Dataset: EthanolConcentration\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 3\n",
    "# Length: 1751\n",
    "# Train Size, Test Size 261 263\n",
    "\n",
    "# Dataset: ERing\n",
    "# No dataset found\n",
    "\n",
    "#big\n",
    "# Dataset: FaceDetection\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 144\n",
    "# Length: 62\n",
    "# Train Size, Test Size 5890 3524\n",
    "\n",
    "#yes\n",
    "# Dataset: FingerMovements\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 28\n",
    "# Length: 50\n",
    "# Train Size, Test Size 316 100\n",
    "\n",
    "#why not, maybe big length\n",
    "# Dataset: HandMovementDirection\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 10\n",
    "# Length: 400\n",
    "# Train Size, Test Size 160 74\n",
    "\n",
    "#smallTrain\n",
    "# Dataset: Handwriting\n",
    "# Number of Classes: 26\n",
    "# Dimension of path: 3\n",
    "# Length: 152\n",
    "# Train Size, Test Size 150 850\n",
    "\n",
    "#yes\n",
    "# Dataset: Heartbeat\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 61\n",
    "# Length: 405\n",
    "# Train Size, Test Size 204 205\n",
    "\n",
    "#big\n",
    "# Dataset: InsectWingbeat\n",
    "# Number of Classes: 10\n",
    "# Dimension of path: 200\n",
    "# Length: 22\n",
    "# Train Size, Test Size 25000 25000\n",
    "\n",
    "# Dataset: JapaneseVowels\n",
    "# No dataset found\n",
    "\n",
    "#yes\n",
    "# Dataset: Libras\n",
    "# Number of Classes: 15\n",
    "# Dimension of path: 2\n",
    "# Length: 45\n",
    "# Train Size, Test Size 180 180\n",
    "\n",
    "#TODO I SHOULD INCLUDE\n",
    "# Dataset: LSST\n",
    "# Number of Classes: 14\n",
    "# Dimension of path: 6\n",
    "# Length: 36\n",
    "# Train Size, Test Size 2459 2466\n",
    "\n",
    "#length\n",
    "# Dataset: MotorImagery\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 64\n",
    "# Length: 3000\n",
    "# Train Size, Test Size 278 100\n",
    "\n",
    "#yes\n",
    "# Dataset: NATOPS\n",
    "# Number of Classes: 6\n",
    "# Dimension of path: 24\n",
    "# Length: 51\n",
    "# Train Size, Test Size 180 180\n",
    "\n",
    "#yes\n",
    "# Dataset: PenDigits\n",
    "# Number of Classes: 10\n",
    "# Dimension of path: 2\n",
    "# Length: 8\n",
    "# Train Size, Test Size 7494 3498\n",
    "\n",
    "#TODO SHOULD INCLUDE highDim\n",
    "# Dataset: PEMS-SF\n",
    "# Number of Classes: 7\n",
    "# Dimension of path: 963\n",
    "# Length: 144\n",
    "# Train Size, Test Size 267 173\n",
    "\n",
    "#NO, dim=1, big length, large num classes\n",
    "# Dataset: Phoneme\n",
    "# Number of Classes: 39\n",
    "# Dimension of path: 1\n",
    "# Length: 1024\n",
    "# Train Size, Test Size 214 1896\n",
    "\n",
    "#yes\n",
    "# Dataset: RacketSports\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 6\n",
    "# Length: 30\n",
    "# Train Size, Test Size 151 152\n",
    "\n",
    "#yes\n",
    "# Dataset: SelfRegulationSCP1\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 6\n",
    "# Length: 896\n",
    "# Train Size, Test Size 268 293\n",
    "\n",
    "# Dataset: SelfRegulationSCP2\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 7\n",
    "# Length: 1152\n",
    "# Train Size, Test Size 200 180\n",
    "\n",
    "# Dataset: SpokenArabicDigits\n",
    "# No dataset found\n",
    "\n",
    "#NO, long, also very small set\n",
    "# Dataset: StandWalkJump\n",
    "# Number of Classes: 3\n",
    "# Dimension of path: 4\n",
    "# Length: 2500\n",
    "# Train Size, Test Size 12 15\n",
    "\n",
    "#yes\n",
    "# Dataset: UWaveGestureLibrary\n",
    "# Number of Classes: 8\n",
    "# Dimension of path: 3\n",
    "# Length: 315\n",
    "# Train Size, Test Size 120 320\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation on Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cv_tslearn(\n",
    "    dataset_names = [\n",
    "        #'ArticularyWordRecognition', \n",
    "        #'BasicMotions', \n",
    "        #'Cricket',\n",
    "             #########'ERing', #cant find dataset\n",
    "        'Libras', \n",
    "        #'NATOPS', \n",
    "        #'RacketSports',     \n",
    "        #'FingerMovements',\n",
    "        #'Heartbeat',\n",
    "        #'SelfRegulationSCP1', \n",
    "        #'UWaveGestureLibrary',\n",
    "        #'PenDigits',\n",
    "        #'LSST',\n",
    "        #'EthanolConcentration',\n",
    "        ],\n",
    "    kernel_names = [\n",
    "        \"linear\",\n",
    "        #\"rbf\",\n",
    "        #\"poly\",\n",
    "        #\"gak\",\n",
    "        #\"truncated sig\",\n",
    "        #\"truncated sig rbf\",\n",
    "        #\"signature pde rbf\",\n",
    "        #\"integral linear\",\n",
    "        #\"integral rbf\",\n",
    "        #\"integral poly\",\n",
    "        ],\n",
    "        k=5,\n",
    "        n_repeats=1,\n",
    "        n_jobs_repeats=1,\n",
    "        n_jobs_gram=1,\n",
    "        verbose=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print CV results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cv_results(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = validate_tslearn(cv_results, n_jobs=1, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_test_results(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read CV data from file and print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dicts_from_pickle(paths:List[str]) -> Dict:\n",
    "    dicts = [load_from_pickle(path)\n",
    "             for path in paths]\n",
    "    joined_dicts = {}\n",
    "    for d in dicts:\n",
    "        joined_dicts.update(d)\n",
    "    return joined_dicts\n",
    "\n",
    "# Load the cross validation results\n",
    "cv_results = read_dicts_from_pickle(\n",
    "    [\n",
    "    \"../Data/cv_RacketSports.pkl\", \n",
    "    \"../Data/cv_NATOPS.pkl\",\n",
    "    \"../Data/cv_Libras.pkl\",\n",
    "    \"../Data/cv_FingerMovements.pkl\",\n",
    "    \"../Data/cv_UWaveGestureLibrary.pkl\",\n",
    "    \"../Data/cv_ArticularyWordRecognition.pkl\",\n",
    "    ])\n",
    "print_cv_results(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = read_dicts_from_pickle([\"TODO\"])\n",
    "print_test_results(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start validation on test sets\n",
      "Libras\n",
      "Number of Classes: 15\n",
      "Dimension of path: 2\n",
      "Length: 45\n",
      "Train: 180\n",
      "Test: 180\n",
      "Kernel: linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 145.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: rbf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 118.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: poly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 132.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: gak\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:18<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: truncated sig\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:27<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: truncated sig rbf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: signature pde rbf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]/home/nikita/Code/kernel-timeseries-anomaly-detection/.conda/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [01:21<00:00,  5.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: integral linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 106.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: integral rbf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 93.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: integral poly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 99.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time for Libras: 135.70629105700027 seconds\n",
      "\n",
      "End validation on test sets\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#experiment with AUCs\n",
    "import numpy as np\n",
    "from typing import List, Optional, Dict, Set, Callable, Any\n",
    "from joblib import Memory, Parallel, delayed\n",
    "import tslearn\n",
    "import tslearn.metrics\n",
    "from tslearn.datasets import UCR_UEA_datasets\n",
    "\n",
    "from cross_validation import cv_tslearn, print_cv_results\n",
    "from eval_on_test import validate_tslearn, print_test_results\n",
    "from utils import load_from_pickle, save_to_pickle\n",
    "\n",
    "def read_dicts_from_pickle(paths:List[str]) -> Dict:\n",
    "    dicts = [load_from_pickle(path)\n",
    "             for path in paths]\n",
    "    joined_dicts = {}\n",
    "    for d in dicts:\n",
    "        joined_dicts.update(d)\n",
    "    return joined_dicts\n",
    "\n",
    "\n",
    "cv_results = read_dicts_from_pickle(\n",
    "    [\n",
    "    \"../Data/cv_Libras.pkl\",\n",
    "    ])\n",
    "test_results = validate_tslearn(cv_results, n_jobs=4, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results\n",
      "\n",
      "Dataset: Libras\n",
      "Number of Classes: 15\n",
      "Dimension of path: 2\n",
      "Length: 45\n",
      "Train: 180\n",
      "Test: 180\n",
      "\n",
      "Kernel: linear\n",
      "Conformance AUC: 0.94\n",
      "Mahalanobis AUC: 0.872\n",
      "Conformance PR AUC: 0.685\n",
      "Mahalanobis PR AUC: 0.553\n",
      "\n",
      "Kernel: rbf\n",
      "Conformance AUC: 0.934\n",
      "Mahalanobis AUC: 0.811\n",
      "Conformance PR AUC: 0.675\n",
      "Mahalanobis PR AUC: 0.513\n",
      "\n",
      "Kernel: poly\n",
      "Conformance AUC: 0.848\n",
      "Mahalanobis AUC: 0.678\n",
      "Conformance PR AUC: 0.486\n",
      "Mahalanobis PR AUC: 0.271\n",
      "\n",
      "Kernel: gak\n",
      "Conformance AUC: 0.876\n",
      "Mahalanobis AUC: 0.556\n",
      "Conformance PR AUC: 0.627\n",
      "Mahalanobis PR AUC: 0.296\n",
      "\n",
      "Kernel: truncated sig\n",
      "Conformance AUC: 0.925\n",
      "Mahalanobis AUC: 0.922\n",
      "Conformance PR AUC: 0.608\n",
      "Mahalanobis PR AUC: 0.591\n",
      "\n",
      "Kernel: truncated sig rbf\n",
      "Conformance AUC: 0.925\n",
      "Mahalanobis AUC: 0.849\n",
      "Conformance PR AUC: 0.615\n",
      "Mahalanobis PR AUC: 0.508\n",
      "\n",
      "Kernel: signature pde rbf\n",
      "Conformance AUC: 0.846\n",
      "Mahalanobis AUC: 0.752\n",
      "Conformance PR AUC: 0.604\n",
      "Mahalanobis PR AUC: 0.444\n",
      "\n",
      "Kernel: integral linear\n",
      "Conformance AUC: 0.938\n",
      "Mahalanobis AUC: 0.871\n",
      "Conformance PR AUC: 0.672\n",
      "Mahalanobis PR AUC: 0.551\n",
      "\n",
      "Kernel: integral rbf\n",
      "Conformance AUC: 0.928\n",
      "Mahalanobis AUC: 0.783\n",
      "Conformance PR AUC: 0.671\n",
      "Mahalanobis PR AUC: 0.492\n",
      "\n",
      "Kernel: integral poly\n",
      "Conformance AUC: 0.892\n",
      "Mahalanobis AUC: 0.815\n",
      "Conformance PR AUC: 0.547\n",
      "Mahalanobis PR AUC: 0.35\n",
      "\n",
      "End Dataset\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_test_results(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Results  OVR\n",
    "\n",
    "# Dataset: Libras\n",
    "# Number of Classes: 15\n",
    "# Dimension of path: 2\n",
    "# Length: 45\n",
    "# Train: 180\n",
    "# Test: 180\n",
    "\n",
    "# Kernel: linear\n",
    "# Conformance AUC: 0.938\n",
    "# Mahalanobis AUC: 0.87\n",
    "# Conformance PR AUC: 0.969\n",
    "# Mahalanobis PR AUC: 0.935\n",
    "\n",
    "# Kernel: rbf\n",
    "# Conformance AUC: 0.931\n",
    "# Mahalanobis AUC: 0.81\n",
    "# Conformance PR AUC: 0.965\n",
    "# Mahalanobis PR AUC: 0.905\n",
    "\n",
    "# Kernel: poly\n",
    "# Conformance AUC: 0.853\n",
    "# Mahalanobis AUC: 0.69\n",
    "# Conformance PR AUC: 0.927\n",
    "# Mahalanobis PR AUC: 0.845\n",
    "\n",
    "# Kernel: gak\n",
    "# Conformance AUC: 0.881\n",
    "# Mahalanobis AUC: 0.564\n",
    "# Conformance PR AUC: 0.94\n",
    "# Mahalanobis PR AUC: 0.782\n",
    "\n",
    "# Kernel: truncated sig\n",
    "# Conformance AUC: 0.922\n",
    "# Mahalanobis AUC: 0.919\n",
    "# Conformance PR AUC: 0.961\n",
    "# Mahalanobis PR AUC: 0.96\n",
    "\n",
    "# Kernel: truncated sig rbf\n",
    "# Conformance AUC: 0.925\n",
    "# Mahalanobis AUC: 0.849\n",
    "# Conformance PR AUC: 0.962\n",
    "# Mahalanobis PR AUC: 0.924\n",
    "\n",
    "# Kernel: signature pde rbf\n",
    "# Conformance AUC: 0.847\n",
    "# Mahalanobis AUC: 0.755\n",
    "# Conformance PR AUC: 0.924\n",
    "# Mahalanobis PR AUC: 0.877\n",
    "\n",
    "# Kernel: integral linear\n",
    "# Conformance AUC: 0.936\n",
    "# Mahalanobis AUC: 0.869\n",
    "# Conformance PR AUC: 0.968\n",
    "# Mahalanobis PR AUC: 0.935\n",
    "\n",
    "# Kernel: integral rbf\n",
    "# Conformance AUC: 0.926\n",
    "# Mahalanobis AUC: 0.787\n",
    "# Conformance PR AUC: 0.963\n",
    "# Mahalanobis PR AUC: 0.893\n",
    "\n",
    "# Kernel: integral poly\n",
    "# Conformance AUC: 0.893\n",
    "# Mahalanobis AUC: 0.815\n",
    "# Conformance PR AUC: 0.947\n",
    "# Mahalanobis PR AUC: 0.907\n",
    "\n",
    "# End Dataset\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Results\n",
    "\n",
    "# Dataset: Libras\n",
    "# Number of Classes: 15\n",
    "# Dimension of path: 2\n",
    "# Length: 45\n",
    "# Train: 180\n",
    "# Test: 180\n",
    "\n",
    "# Kernel: linear\n",
    "# Conformance AUC: 0.94\n",
    "# Mahalanobis AUC: 0.872\n",
    "# Conformance PR AUC: 0.995\n",
    "# Mahalanobis PR AUC: 0.983\n",
    "\n",
    "# Kernel: rbf\n",
    "# Conformance AUC: 0.934\n",
    "# Mahalanobis AUC: 0.811\n",
    "# Conformance PR AUC: 0.993\n",
    "# Mahalanobis PR AUC: 0.973\n",
    "\n",
    "# Kernel: poly\n",
    "# Conformance AUC: 0.848\n",
    "# Mahalanobis AUC: 0.678\n",
    "# Conformance PR AUC: 0.984\n",
    "# Mahalanobis PR AUC: 0.959\n",
    "\n",
    "# Kernel: gak\n",
    "# Conformance AUC: 0.873\n",
    "# Mahalanobis AUC: 0.559\n",
    "# Conformance PR AUC: 0.981\n",
    "# Mahalanobis PR AUC: 0.925\n",
    "\n",
    "# Kernel: truncated sig\n",
    "# Conformance AUC: 0.925\n",
    "# Mahalanobis AUC: 0.922\n",
    "# Conformance PR AUC: 0.994\n",
    "# Mahalanobis PR AUC: 0.994\n",
    "\n",
    "# Kernel: truncated sig rbf\n",
    "# Conformance AUC: 0.925\n",
    "# Mahalanobis AUC: 0.849\n",
    "# Conformance PR AUC: 0.994\n",
    "# Mahalanobis PR AUC: 0.982\n",
    "\n",
    "# Kernel: signature pde rbf\n",
    "# Conformance AUC: 0.846\n",
    "# Mahalanobis AUC: 0.752\n",
    "# Conformance PR AUC: 0.976\n",
    "# Mahalanobis PR AUC: 0.96\n",
    "\n",
    "# Kernel: integral linear\n",
    "# Conformance AUC: 0.938\n",
    "# Mahalanobis AUC: 0.871\n",
    "# Conformance PR AUC: 0.994\n",
    "# Mahalanobis PR AUC: 0.983\n",
    "\n",
    "# Kernel: integral rbf\n",
    "# Conformance AUC: 0.928\n",
    "# Mahalanobis AUC: 0.783\n",
    "# Conformance PR AUC: 0.991\n",
    "# Mahalanobis PR AUC: 0.968\n",
    "\n",
    "# Kernel: integral poly\n",
    "# Conformance AUC: 0.892\n",
    "# Mahalanobis AUC: 0.815\n",
    "# Conformance PR AUC: 0.99\n",
    "# Mahalanobis PR AUC: 0.982\n",
    "\n",
    "# End Dataset\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
