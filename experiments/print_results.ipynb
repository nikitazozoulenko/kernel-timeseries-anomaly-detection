{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "from typing import List, Optional, Dict, Set, Callable, Any\n",
    "from joblib import Memory, Parallel, delayed\n",
    "import tslearn\n",
    "import tslearn.metrics\n",
    "from tslearn.datasets import UCR_UEA_datasets\n",
    "from scipy.interpolate import interp1d\n",
    "from numba import njit\n",
    "\n",
    "from cross_validation import cv_tslearn, print_cv_results\n",
    "from eval_on_test import validate_tslearn, print_test_results\n",
    "from utils import load_from_pickle, save_to_pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enumerate all tslearn datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _datasets = [\n",
    "#             'ArticularyWordRecognition', \n",
    "#             'BasicMotions', \n",
    "#             'Cricket',\n",
    "#             #'ERing',\n",
    "#             'Libras', \n",
    "#             'NATOPS', \n",
    "#             'RacketSports',     \n",
    "#             'FingerMovements',\n",
    "#             'Heartbeat',\n",
    "#             'SelfRegulationSCP1', \n",
    "#             'UWaveGestureLibrary'\n",
    "#             ]\n",
    "\n",
    "# import tslearn\n",
    "# UCR_UEA_datasets = tslearn.datasets.UCR_UEA_datasets()\n",
    "\n",
    "# for dataset_name in UCR_UEA_datasets.list_multivariate_datasets():\n",
    "# #for dataset_name in _datasets:\n",
    "#     print(\"Dataset:\", dataset_name)\n",
    "#     dataset = UCR_UEA_datasets.load_dataset(dataset_name)\n",
    "#     if dataset[0] is not None:\n",
    "#         X_train, y_train, X_test, y_test = dataset\n",
    "#         num_classes = len(np.unique(y_train))\n",
    "#         N_train, T, d = X_train.shape\n",
    "#         N_test, _, _  = X_test.shape\n",
    "        \n",
    "#         print(\"Number of Classes:\", num_classes)\n",
    "#         print(\"Dimension of path:\", d)\n",
    "#         print(\"Length:\", T)\n",
    "#         print(\"Train Size, Test Size\", N_train, N_test)\n",
    "#         print()\n",
    "#     else:\n",
    "#         print(\"No dataset found\")\n",
    "#         print()\n",
    "\n",
    "#yes\n",
    "# Dataset: ArticularyWordRecognition\n",
    "# Number of Classes: 25\n",
    "# Dimension of path: 9\n",
    "# Length: 144\n",
    "# Train Size, Test Size 275 300\n",
    "\n",
    "# Dataset: AtrialFibrillation\n",
    "# No dataset found\n",
    "\n",
    "#yes\n",
    "# Dataset: BasicMotions\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 6\n",
    "# Length: 100\n",
    "# Train Size, Test Size 40 40\n",
    "\n",
    "# Dataset: CharacterTrajectories\n",
    "# No dataset found\n",
    "\n",
    "#yes\n",
    "# Dataset: Cricket\n",
    "# Number of Classes: 12\n",
    "# Dimension of path: 6\n",
    "# Length: 1197\n",
    "# Train Size, Test Size 108 72\n",
    "\n",
    "# Dataset: DuckDuckGeese\n",
    "# No dataset found\n",
    "\n",
    "# Dataset: EigenWorms\n",
    "# Number of Classes: 5\n",
    "# Dimension of path: 6\n",
    "# Length: 17984\n",
    "# Train Size, Test Size 128 131\n",
    "\n",
    "#why not\n",
    "# Dataset: Epilepsy\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 3\n",
    "# Length: 206\n",
    "# Train Size, Test Size 137 138\n",
    "\n",
    "#longLength\n",
    "# Dataset: EthanolConcentration\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 3\n",
    "# Length: 1751\n",
    "# Train Size, Test Size 261 263\n",
    "\n",
    "# Dataset: ERing\n",
    "# No dataset found\n",
    "\n",
    "#big\n",
    "# Dataset: FaceDetection\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 144\n",
    "# Length: 62\n",
    "# Train Size, Test Size 5890 3524\n",
    "\n",
    "#yes\n",
    "# Dataset: FingerMovements\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 28\n",
    "# Length: 50\n",
    "# Train Size, Test Size 316 100\n",
    "\n",
    "#why not, maybe big length\n",
    "# Dataset: HandMovementDirection\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 10\n",
    "# Length: 400\n",
    "# Train Size, Test Size 160 74\n",
    "\n",
    "#smallTrain\n",
    "# Dataset: Handwriting\n",
    "# Number of Classes: 26\n",
    "# Dimension of path: 3\n",
    "# Length: 152\n",
    "# Train Size, Test Size 150 850\n",
    "\n",
    "#yes\n",
    "# Dataset: Heartbeat\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 61\n",
    "# Length: 405\n",
    "# Train Size, Test Size 204 205\n",
    "\n",
    "#big\n",
    "# Dataset: InsectWingbeat\n",
    "# Number of Classes: 10\n",
    "# Dimension of path: 200\n",
    "# Length: 22\n",
    "# Train Size, Test Size 25000 25000\n",
    "\n",
    "# Dataset: JapaneseVowels\n",
    "# No dataset found\n",
    "\n",
    "#yes\n",
    "# Dataset: Libras\n",
    "# Number of Classes: 15\n",
    "# Dimension of path: 2\n",
    "# Length: 45\n",
    "# Train Size, Test Size 180 180\n",
    "\n",
    "#TODO I SHOULD INCLUDE\n",
    "# Dataset: LSST\n",
    "# Number of Classes: 14\n",
    "# Dimension of path: 6\n",
    "# Length: 36\n",
    "# Train Size, Test Size 2459 2466\n",
    "\n",
    "#length\n",
    "# Dataset: MotorImagery\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 64\n",
    "# Length: 3000\n",
    "# Train Size, Test Size 278 100\n",
    "\n",
    "#yes\n",
    "# Dataset: NATOPS\n",
    "# Number of Classes: 6\n",
    "# Dimension of path: 24\n",
    "# Length: 51\n",
    "# Train Size, Test Size 180 180\n",
    "\n",
    "#yes\n",
    "# Dataset: PenDigits\n",
    "# Number of Classes: 10\n",
    "# Dimension of path: 2\n",
    "# Length: 8\n",
    "# Train Size, Test Size 7494 3498\n",
    "\n",
    "#TODO SHOULD INCLUDE highDim\n",
    "# Dataset: PEMS-SF\n",
    "# Number of Classes: 7\n",
    "# Dimension of path: 963\n",
    "# Length: 144\n",
    "# Train Size, Test Size 267 173\n",
    "\n",
    "#NO, dim=1, big length, large num classes\n",
    "# Dataset: Phoneme\n",
    "# Number of Classes: 39\n",
    "# Dimension of path: 1\n",
    "# Length: 1024\n",
    "# Train Size, Test Size 214 1896\n",
    "\n",
    "#yes\n",
    "# Dataset: RacketSports\n",
    "# Number of Classes: 4\n",
    "# Dimension of path: 6\n",
    "# Length: 30\n",
    "# Train Size, Test Size 151 152\n",
    "\n",
    "#yes\n",
    "# Dataset: SelfRegulationSCP1\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 6\n",
    "# Length: 896\n",
    "# Train Size, Test Size 268 293\n",
    "\n",
    "# Dataset: SelfRegulationSCP2\n",
    "# Number of Classes: 2\n",
    "# Dimension of path: 7\n",
    "# Length: 1152\n",
    "# Train Size, Test Size 200 180\n",
    "\n",
    "# Dataset: SpokenArabicDigits\n",
    "# No dataset found\n",
    "\n",
    "#NO, long, also very small set\n",
    "# Dataset: StandWalkJump\n",
    "# Number of Classes: 3\n",
    "# Dimension of path: 4\n",
    "# Length: 2500\n",
    "# Train Size, Test Size 12 15\n",
    "\n",
    "#yes\n",
    "# Dataset: UWaveGestureLibrary\n",
    "# Number of Classes: 8\n",
    "# Dimension of path: 3\n",
    "# Length: 315\n",
    "# Train Size, Test Size 120 320\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation on Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: PenDigits\n",
      "Number of Classes: 10\n",
      "Dimension of path: 2\n",
      "Length: 8\n",
      "Train: 7494\n",
      "Test: N/A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Label for linear:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min std: 11.470955930690362 max std: 27.137755545206648\n",
      "min std: 11.819009126053183 max std: 26.85940196098031\n",
      "min std: 11.436556767816313 max std: 26.97159376212377\n",
      "min std: 11.84980067908839 max std: 26.41815986538572\n",
      "min std: 11.838578159396002 max std: 26.702939880735283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Label for linear:  10%|█         | 1/10 [00:02<00:23,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min std: 3.101986693181394 max std: 36.60268902320962\n",
      "min std: 2.9157549685067816 max std: 36.84233336853625\n",
      "min std: 2.9463782294493064 max std: 36.02928755209803\n",
      "min std: 3.203352090567901 max std: 36.029251153367724\n",
      "min std: 3.1900655677836345 max std: 36.165512160151266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Label for linear:  20%|██        | 2/10 [00:05<00:20,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min std: 2.4762431380814514 max std: 19.38517811545415\n",
      "min std: 2.453294299833928 max std: 19.92814223540346\n",
      "min std: 2.303967792117276 max std: 19.810825552431407\n",
      "min std: 2.4096335247712424 max std: 19.68122584558303\n",
      "min std: 1.5658584878833264 max std: 20.165494763983986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Label for linear:  30%|███       | 3/10 [00:07<00:17,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min std: 1.5833863354641486 max std: 19.7081368627563\n",
      "min std: 1.3616780484940139 max std: 19.824049780921325\n",
      "min std: 1.387658265978323 max std: 19.638820857884813\n",
      "min std: 1.5894263092193213 max std: 19.57766823152702\n",
      "min std: 1.4708529648647857 max std: 19.818816356453617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Label for linear:  40%|████      | 4/10 [00:10<00:16,  2.74s/it]/home/nikita/Code/kernel-timeseries-anomaly-detection/experiments/experiment_code.py:277: RuntimeWarning: invalid value encountered in divide\n",
      "  train = (train - mean) / std\n",
      "/home/nikita/Code/kernel-timeseries-anomaly-detection/experiments/experiment_code.py:278: RuntimeWarning: divide by zero encountered in divide\n",
      "  test = (test - mean) / std\n",
      "/home/nikita/Code/kernel-timeseries-anomaly-detection/experiments/experiment_code.py:278: RuntimeWarning: invalid value encountered in divide\n",
      "  test = (test - mean) / std\n",
      "Label for linear:  40%|████      | 4/10 [00:10<00:16,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min std: 0.0 max std: 30.353891096476314\n",
      "Warning: small std\n",
      "std: [[[23.78657237  2.2577478 ]\n",
      "  [17.07604237  9.51937915]\n",
      "  [10.18737067 10.89957558]\n",
      "  [30.3538911  10.83667781]\n",
      "  [19.29319568 18.14079283]\n",
      "  [13.64034775 15.48424339]\n",
      "  [17.34263448  7.24088828]\n",
      "  [25.89026894  0.        ]]]\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "SVD did not converge",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cv_best_models \u001b[38;5;241m=\u001b[39m \u001b[43mcv_tslearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#'ArticularyWordRecognition', \u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#'BasicMotions', \u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#'Cricket',\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;66;43;03m#########'ERing', #cant find dataset\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#'Libras', \u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#'NATOPS', \u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#'RacketSports',     \u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#'FingerMovements',\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#'Heartbeat',\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#'SelfRegulationSCP1', \u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#'UWaveGestureLibrary',\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPenDigits\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#'LSST',\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#'EthanolConcentration',\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#\"rbf\",\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#\"poly\",\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#\"gak\",\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#\"truncated sig\",\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#\"truncated sig rbf\",\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#\"signature pde rbf\",\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#\"integral linear\",\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#\"integral rbf\",\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#\"integral poly\",\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs_repeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs_gram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/kernel-timeseries-anomaly-detection/experiments/cross_validation.py:340\u001b[0m, in \u001b[0;36mcv_tslearn\u001b[0;34m(dataset_names, kernel_names, k, n_repeats, n_jobs_repeats, n_jobs_gram, verbose)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# Run each kernel\u001b[39;00m\n\u001b[1;32m    339\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 340\u001b[0m kernelwise_param_dicts \u001b[38;5;241m=\u001b[39m \u001b[43mcv_given_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munique_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mkernel_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#fixed_length = True\u001b[39;49;00m\n\u001b[1;32m    342\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mn_jobs_repeats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs_gram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime taken for dataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, t1\u001b[38;5;241m-\u001b[39mt0, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseconds\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Code/kernel-timeseries-anomaly-detection/experiments/cross_validation.py:307\u001b[0m, in \u001b[0;36mcv_given_dataset\u001b[0;34m(X, y, unique_labels, kernel_names, fixed_length, k, n_repeats, n_jobs_repeats, n_jobs_gram, verbose)\u001b[0m\n\u001b[1;32m    305\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m tqdm(unique_labels, desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkernel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 307\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43meval_repeats_folds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeats_and_folds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mn_jobs_repeats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs_gram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m     final_param_dict \u001b[38;5;241m=\u001b[39m choose_best_hyperparam(scores, hyperparams)\n\u001b[1;32m    311\u001b[0m     labelwise_param_dicts[label] \u001b[38;5;241m=\u001b[39m final_param_dict\n",
      "File \u001b[0;32m~/Code/kernel-timeseries-anomaly-detection/experiments/cross_validation.py:231\u001b[0m, in \u001b[0;36meval_repeats_folds\u001b[0;34m(kernel_name, repeats_and_folds, hyperparams, class_to_test, fixed_length, n_jobs_repeats, n_jobs_gram, verbose)\u001b[0m\n\u001b[1;32m    228\u001b[0m param_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormal_class_label\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m class_to_test\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m#loop over repeats and folds\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m repeat_scores \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs_repeats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_1_paramdict_1_fold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_to_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfixed_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_fold_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs_gram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrepeats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrepeats_and_folds\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrepeats\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m#make shape consistent. Right now \u001b[39;00m\n\u001b[1;32m    238\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(repeat_scores)\n",
      "File \u001b[0;32m~/Code/kernel-timeseries-anomaly-detection/.conda/lib/python3.11/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Code/kernel-timeseries-anomaly-detection/.conda/lib/python3.11/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Code/kernel-timeseries-anomaly-detection/experiments/cross_validation.py:169\u001b[0m, in \u001b[0;36meval_1_paramdict_1_fold\u001b[0;34m(fold, class_to_test, param_dict, fixed_length, min_fold_size, n_jobs_gram, verbose)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruncated sig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m param_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# aucs shape (M, 2, 2), M <= min_fold_size\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     aucs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(min_fold_size)\n\u001b[0;32m--> 169\u001b[0m     raw_aucs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_single_kernel_single_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mclass_to_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mfixed_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_all_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mSVD_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSVD_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mSVD_max_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_fold_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs_gram\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     aucs[:\u001b[38;5;28mlen\u001b[39m(raw_aucs)] \u001b[38;5;241m=\u001b[39m aucs_to_objective(raw_aucs)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# Computationally efficient truncated signature case\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n",
      "File \u001b[0;32m~/Code/kernel-timeseries-anomaly-detection/experiments/experiment_code.py:361\u001b[0m, in \u001b[0;36mrun_single_kernel_single_label\u001b[0;34m(X_train, y_train, X_test, y_test, class_to_test, param_dict, fixed_length, SVD_threshold, SVD_max_rank, verbose, vv_gram, uv_gram, return_all_levels, n_jobs)\u001b[0m\n\u001b[1;32m    357\u001b[0m     corpus, test \u001b[38;5;241m=\u001b[39m get_corpus_and_test(X_train, y_train, X_test, \n\u001b[1;32m    358\u001b[0m                                    class_to_test, fixed_length)\n\u001b[1;32m    359\u001b[0m     vv_gram, uv_gram \u001b[38;5;241m=\u001b[39m calc_grams(corpus, test, param_dict, fixed_length, \n\u001b[1;32m    360\u001b[0m                                   n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m--> 361\u001b[0m scorer \u001b[38;5;241m=\u001b[39m \u001b[43mBaseclassConformanceScore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvv_gram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSVD_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mSVD_max_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSVD_max_rank\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# only return accs for highest allowed threshold\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_all_levels:\n",
      "File \u001b[0;32m~/Code/kernel-timeseries-anomaly-detection/models/conformance.py:60\u001b[0m, in \u001b[0;36mBaseclassConformanceScore.__init__\u001b[0;34m(self, inner_prod_Gram_matrix, SVD_threshold, verbose, SVD_max_rank)\u001b[0m\n\u001b[1;32m     53\u001b[0m A \u001b[38;5;241m=\u001b[39m ( (B \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m b) \u001b[38;5;241m/\u001b[39m N \u001b[38;5;66;03m#<f_i, f_j>\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# print(\"mean <x_i, x_j> = \", np.mean(B))\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# print(\"mean absolute <x_i, x_j> = \", np.mean(np.abs(B)))\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# print(\"mean <f_i, f_j> = \", np.mean(A))\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# print(\"mean absolute <f_i, f_j> = \", np.mean(np.abs(A)), \"\\n\")\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#SVD decomposition is equal to spectral decomposition\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m U, S, Ut \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m M \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(S \u001b[38;5;241m>\u001b[39m SVD_threshold)\n\u001b[1;32m     62\u001b[0m M \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(M, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Code/kernel-timeseries-anomaly-detection/.conda/lib/python3.11/site-packages/numpy/linalg/linalg.py:1642\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[1;32m   1639\u001b[0m         gufunc \u001b[38;5;241m=\u001b[39m _umath_linalg\u001b[38;5;241m.\u001b[39msvd_n_s\n\u001b[1;32m   1641\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->DdD\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->ddd\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1642\u001b[0m u, s, vh \u001b[38;5;241m=\u001b[39m \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1643\u001b[0m u \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1644\u001b[0m s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mastype(_realType(result_t), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Code/kernel-timeseries-anomaly-detection/.conda/lib/python3.11/site-packages/numpy/linalg/linalg.py:98\u001b[0m, in \u001b[0;36m_raise_linalgerror_svd_nonconvergence\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_svd_nonconvergence\u001b[39m(err, flag):\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVD did not converge\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: SVD did not converge"
     ]
    }
   ],
   "source": [
    "cv_best_models = cv_tslearn(\n",
    "    dataset_names = [\n",
    "        #'ArticularyWordRecognition', \n",
    "        #'BasicMotions', \n",
    "        #'Cricket',\n",
    "             #########'ERing', #cant find dataset\n",
    "        #'Libras', \n",
    "        #'NATOPS', \n",
    "        #'RacketSports',     \n",
    "        #'FingerMovements',\n",
    "        #'Heartbeat',\n",
    "        #'SelfRegulationSCP1', \n",
    "        #'UWaveGestureLibrary',\n",
    "        'PenDigits',\n",
    "        #'LSST',\n",
    "        #'EthanolConcentration',\n",
    "        ],\n",
    "    kernel_names = [\n",
    "        \"linear\",\n",
    "        #\"rbf\",\n",
    "        #\"poly\",\n",
    "        #\"gak\",\n",
    "        #\"truncated sig\",\n",
    "        #\"truncated sig rbf\",\n",
    "        #\"signature pde rbf\",\n",
    "        #\"integral linear\",\n",
    "        #\"integral rbf\",\n",
    "        #\"integral poly\",\n",
    "        ],\n",
    "        k=5,\n",
    "        n_repeats=1,\n",
    "        n_jobs_repeats=1,\n",
    "        n_jobs_gram=1,\n",
    "        verbose=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print CV results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cv_results(cv_best_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = validate_tslearn(cv_best_models, n_jobs=1, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_test_results(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read CV data from file and print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dicts_from_pickle(paths:List[str]) -> Dict:\n",
    "    dicts = [load_from_pickle(path)\n",
    "             for path in paths]\n",
    "    joined_dicts = {}\n",
    "    for d in dicts:\n",
    "        joined_dicts.update(d)\n",
    "    return joined_dicts\n",
    "\n",
    "# Load the cross validation results\n",
    "cv_results = read_dicts_from_pickle(\n",
    "    [\n",
    "    \"../Data/cv_results_Heart.pkl\", \n",
    "    \"../Data/cv_results_UWave_BM.pkl\",\n",
    "    ])\n",
    "print_cv_results(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = read_dicts_from_pickle([]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
